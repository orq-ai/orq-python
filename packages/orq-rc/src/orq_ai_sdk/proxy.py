"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from enum import Enum
from orq_ai_sdk import models, utils
from orq_ai_sdk._hooks import HookContext
from orq_ai_sdk.types import OptionalNullable, UNSET
from orq_ai_sdk.utils import eventstreaming, get_security_from_env
from orq_ai_sdk.utils.unmarshal_json_response import unmarshal_json_response
from typing import Any, Dict, List, Mapping, Optional, Union


class ChatCompletionsAcceptEnum(str, Enum):
    APPLICATION_JSON = "application/json"
    TEXT_EVENT_STREAM = "text/event-stream"


class CompletionsAcceptEnum(str, Enum):
    APPLICATION_JSON = "application/json"
    TEXT_EVENT_STREAM = "text/event-stream"


class ResponsesCreateAcceptEnum(str, Enum):
    APPLICATION_JSON = "application/json"
    TEXT_EVENT_STREAM = "text/event-stream"


class Proxy(BaseSDK):
    def chat_completions(
        self,
        *,
        messages: Union[
            List[models.CreateChatCompletionMessages],
            List[models.CreateChatCompletionMessagesTypedDict],
        ],
        model: str,
        metadata: Optional[Dict[str, str]] = None,
        audio: OptionalNullable[
            Union[
                models.CreateChatCompletionAudio,
                models.CreateChatCompletionAudioTypedDict,
            ]
        ] = UNSET,
        frequency_penalty: OptionalNullable[float] = UNSET,
        max_tokens: OptionalNullable[int] = UNSET,
        max_completion_tokens: OptionalNullable[int] = UNSET,
        logprobs: OptionalNullable[bool] = UNSET,
        top_logprobs: OptionalNullable[int] = UNSET,
        n: OptionalNullable[int] = UNSET,
        presence_penalty: OptionalNullable[float] = UNSET,
        response_format: Optional[
            Union[
                models.CreateChatCompletionResponseFormat,
                models.CreateChatCompletionResponseFormatTypedDict,
            ]
        ] = None,
        reasoning_effort: Optional[str] = None,
        verbosity: Optional[str] = None,
        seed: OptionalNullable[float] = UNSET,
        stop: OptionalNullable[Union[models.Stop, models.StopTypedDict]] = UNSET,
        stream_options: OptionalNullable[
            Union[models.StreamOptions, models.StreamOptionsTypedDict]
        ] = UNSET,
        thinking: Optional[Union[models.Thinking, models.ThinkingTypedDict]] = None,
        temperature: OptionalNullable[float] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        top_k: OptionalNullable[float] = UNSET,
        tools: Optional[Union[List[models.Tools], List[models.ToolsTypedDict]]] = None,
        tool_choice: Optional[
            Union[models.ToolChoice, models.ToolChoiceTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        modalities: OptionalNullable[List[models.Modalities]] = UNSET,
        web_search_options: Optional[
            Union[models.WebSearchOptions, models.WebSearchOptionsTypedDict]
        ] = None,
        orq: Optional[
            Union[
                models.CreateChatCompletionOrq, models.CreateChatCompletionOrqTypedDict
            ]
        ] = None,
        stream: Optional[bool] = False,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        accept_header_override: Optional[ChatCompletionsAcceptEnum] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.CreateChatCompletionResponse]:
        r"""Create chat completion

        Creates a model response for the given chat conversation with support for retries, fallbacks, prompts, and variables.

        :param messages: A list of messages comprising the conversation so far.
        :param model: ID of the model to use
        :param metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can have a maximum length of 64 characters and values can have a maximum length of 512 characters.
        :param audio: Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more.
        :param frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        :param max_tokens: `[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
        :param max_completion_tokens: An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens
        :param logprobs: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
        :param top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
        :param n: How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
        :param presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        :param response_format: An object specifying the format that the model must output
        :param reasoning_effort: Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
        :param verbosity: Adjusts response verbosity. Lower levels yield shorter answers.
        :param seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
        :param stop: Up to 4 sequences where the API will stop generating further tokens.
        :param stream_options: Options for streaming response. Only set this when you set stream: true.
        :param thinking:
        :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
        :param top_k: Limits the model to consider only the top k most likely tokens at each step.
        :param tools: A list of tools the model may call.
        :param tool_choice: Controls which (if any) tool is called by the model.
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use.
        :param modalities: Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"].
        :param web_search_options: This tool searches the web for relevant results to use in a response. Learn more about the web search tool.
        :param orq: Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution.
        :param stream:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.CreateChatCompletionRequestBody(
            messages=utils.get_pydantic_model(
                messages, List[models.CreateChatCompletionMessages]
            ),
            model=model,
            metadata=metadata,
            audio=utils.get_pydantic_model(
                audio, OptionalNullable[models.CreateChatCompletionAudio]
            ),
            frequency_penalty=frequency_penalty,
            max_tokens=max_tokens,
            max_completion_tokens=max_completion_tokens,
            logprobs=logprobs,
            top_logprobs=top_logprobs,
            n=n,
            presence_penalty=presence_penalty,
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.CreateChatCompletionResponseFormat]
            ),
            reasoning_effort=reasoning_effort,
            verbosity=verbosity,
            seed=seed,
            stop=stop,
            stream_options=utils.get_pydantic_model(
                stream_options, OptionalNullable[models.StreamOptions]
            ),
            thinking=utils.get_pydantic_model(thinking, Optional[models.Thinking]),
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            tools=utils.get_pydantic_model(tools, Optional[List[models.Tools]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ToolChoice]
            ),
            parallel_tool_calls=parallel_tool_calls,
            modalities=modalities,
            web_search_options=utils.get_pydantic_model(
                web_search_options, Optional[models.WebSearchOptions]
            ),
            orq=utils.get_pydantic_model(orq, Optional[models.CreateChatCompletionOrq]),
            stream=stream,
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/chat/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value=accept_header_override.value
            if accept_header_override is not None
            else "application/json;q=1, text/event-stream;q=0",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.CreateChatCompletionRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="CreateChatCompletion",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            return unmarshal_json_response(
                Optional[models.CreateChatCompletionResponseBody],
                http_res,
                http_res_text,
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.CreateChatCompletionProxyResponseBody
                ),
                client_ref=self,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        http_res_text = utils.stream_to_text(http_res)
        raise models.APIError("Unexpected response received", http_res, http_res_text)

    async def chat_completions_async(
        self,
        *,
        messages: Union[
            List[models.CreateChatCompletionMessages],
            List[models.CreateChatCompletionMessagesTypedDict],
        ],
        model: str,
        metadata: Optional[Dict[str, str]] = None,
        audio: OptionalNullable[
            Union[
                models.CreateChatCompletionAudio,
                models.CreateChatCompletionAudioTypedDict,
            ]
        ] = UNSET,
        frequency_penalty: OptionalNullable[float] = UNSET,
        max_tokens: OptionalNullable[int] = UNSET,
        max_completion_tokens: OptionalNullable[int] = UNSET,
        logprobs: OptionalNullable[bool] = UNSET,
        top_logprobs: OptionalNullable[int] = UNSET,
        n: OptionalNullable[int] = UNSET,
        presence_penalty: OptionalNullable[float] = UNSET,
        response_format: Optional[
            Union[
                models.CreateChatCompletionResponseFormat,
                models.CreateChatCompletionResponseFormatTypedDict,
            ]
        ] = None,
        reasoning_effort: Optional[str] = None,
        verbosity: Optional[str] = None,
        seed: OptionalNullable[float] = UNSET,
        stop: OptionalNullable[Union[models.Stop, models.StopTypedDict]] = UNSET,
        stream_options: OptionalNullable[
            Union[models.StreamOptions, models.StreamOptionsTypedDict]
        ] = UNSET,
        thinking: Optional[Union[models.Thinking, models.ThinkingTypedDict]] = None,
        temperature: OptionalNullable[float] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        top_k: OptionalNullable[float] = UNSET,
        tools: Optional[Union[List[models.Tools], List[models.ToolsTypedDict]]] = None,
        tool_choice: Optional[
            Union[models.ToolChoice, models.ToolChoiceTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        modalities: OptionalNullable[List[models.Modalities]] = UNSET,
        web_search_options: Optional[
            Union[models.WebSearchOptions, models.WebSearchOptionsTypedDict]
        ] = None,
        orq: Optional[
            Union[
                models.CreateChatCompletionOrq, models.CreateChatCompletionOrqTypedDict
            ]
        ] = None,
        stream: Optional[bool] = False,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        accept_header_override: Optional[ChatCompletionsAcceptEnum] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.CreateChatCompletionResponse]:
        r"""Create chat completion

        Creates a model response for the given chat conversation with support for retries, fallbacks, prompts, and variables.

        :param messages: A list of messages comprising the conversation so far.
        :param model: ID of the model to use
        :param metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can have a maximum length of 64 characters and values can have a maximum length of 512 characters.
        :param audio: Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more.
        :param frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        :param max_tokens: `[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
        :param max_completion_tokens: An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens
        :param logprobs: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
        :param top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
        :param n: How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
        :param presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        :param response_format: An object specifying the format that the model must output
        :param reasoning_effort: Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
        :param verbosity: Adjusts response verbosity. Lower levels yield shorter answers.
        :param seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
        :param stop: Up to 4 sequences where the API will stop generating further tokens.
        :param stream_options: Options for streaming response. Only set this when you set stream: true.
        :param thinking:
        :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
        :param top_k: Limits the model to consider only the top k most likely tokens at each step.
        :param tools: A list of tools the model may call.
        :param tool_choice: Controls which (if any) tool is called by the model.
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use.
        :param modalities: Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"].
        :param web_search_options: This tool searches the web for relevant results to use in a response. Learn more about the web search tool.
        :param orq: Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution.
        :param stream:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.CreateChatCompletionRequestBody(
            messages=utils.get_pydantic_model(
                messages, List[models.CreateChatCompletionMessages]
            ),
            model=model,
            metadata=metadata,
            audio=utils.get_pydantic_model(
                audio, OptionalNullable[models.CreateChatCompletionAudio]
            ),
            frequency_penalty=frequency_penalty,
            max_tokens=max_tokens,
            max_completion_tokens=max_completion_tokens,
            logprobs=logprobs,
            top_logprobs=top_logprobs,
            n=n,
            presence_penalty=presence_penalty,
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.CreateChatCompletionResponseFormat]
            ),
            reasoning_effort=reasoning_effort,
            verbosity=verbosity,
            seed=seed,
            stop=stop,
            stream_options=utils.get_pydantic_model(
                stream_options, OptionalNullable[models.StreamOptions]
            ),
            thinking=utils.get_pydantic_model(thinking, Optional[models.Thinking]),
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            tools=utils.get_pydantic_model(tools, Optional[List[models.Tools]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ToolChoice]
            ),
            parallel_tool_calls=parallel_tool_calls,
            modalities=modalities,
            web_search_options=utils.get_pydantic_model(
                web_search_options, Optional[models.WebSearchOptions]
            ),
            orq=utils.get_pydantic_model(orq, Optional[models.CreateChatCompletionOrq]),
            stream=stream,
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/chat/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value=accept_header_override.value
            if accept_header_override is not None
            else "application/json;q=1, text/event-stream;q=0",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.CreateChatCompletionRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="CreateChatCompletion",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            return unmarshal_json_response(
                Optional[models.CreateChatCompletionResponseBody],
                http_res,
                http_res_text,
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.CreateChatCompletionProxyResponseBody
                ),
                client_ref=self,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.APIError("Unexpected response received", http_res, http_res_text)

    def moderations(
        self,
        *,
        input_: Union[models.Input, models.InputTypedDict],
        model: Optional[str] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyModerationsResponseBody]:
        r"""
        :param input: Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models.
        :param model: The content moderation model you would like to use. Defaults to omni-moderation-latest
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyModerationsRequestBody(
            input=input_,
            model=model,
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/moderations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.PostV2ProxyModerationsRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/moderations",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyModerationsResponseBody], http_res
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = unmarshal_json_response(
                models.PostV2ProxyModerationsProxyResponseBodyData, http_res
            )
            raise models.PostV2ProxyModerationsProxyResponseBody(
                response_data, http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    async def moderations_async(
        self,
        *,
        input_: Union[models.Input, models.InputTypedDict],
        model: Optional[str] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyModerationsResponseBody]:
        r"""
        :param input: Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models.
        :param model: The content moderation model you would like to use. Defaults to omni-moderation-latest
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyModerationsRequestBody(
            input=input_,
            model=model,
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/moderations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.PostV2ProxyModerationsRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/moderations",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyModerationsResponseBody], http_res
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = unmarshal_json_response(
                models.PostV2ProxyModerationsProxyResponseBodyData, http_res
            )
            raise models.PostV2ProxyModerationsProxyResponseBody(
                response_data, http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    def embeddings(
        self,
        *,
        input_: Union[
            models.PostV2ProxyEmbeddingsInput,
            models.PostV2ProxyEmbeddingsInputTypedDict,
        ],
        model: str,
        encoding_format: Optional[models.PostV2ProxyEmbeddingsEncodingFormat] = "float",
        dimensions: Optional[float] = None,
        user: Optional[str] = None,
        orq: Optional[
            Union[
                models.PostV2ProxyEmbeddingsOrq,
                models.PostV2ProxyEmbeddingsOrqTypedDict,
            ]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyEmbeddingsResponseBody]:
        r"""Create embeddings

        Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.

        :param input: Input text to embed, encoded as a string or array of tokens.
        :param model: ID of the model to use
        :param encoding_format: Type of the document element
        :param dimensions: The number of dimensions the resulting output embeddings should have.
        :param user: A unique identifier representing your end-user
        :param orq:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyEmbeddingsRequestBody(
            input=input_,
            model=model,
            encoding_format=encoding_format,
            dimensions=dimensions,
            user=user,
            orq=utils.get_pydantic_model(
                orq, Optional[models.PostV2ProxyEmbeddingsOrq]
            ),
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/embeddings",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.PostV2ProxyEmbeddingsRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/embeddings",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyEmbeddingsResponseBody], http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    async def embeddings_async(
        self,
        *,
        input_: Union[
            models.PostV2ProxyEmbeddingsInput,
            models.PostV2ProxyEmbeddingsInputTypedDict,
        ],
        model: str,
        encoding_format: Optional[models.PostV2ProxyEmbeddingsEncodingFormat] = "float",
        dimensions: Optional[float] = None,
        user: Optional[str] = None,
        orq: Optional[
            Union[
                models.PostV2ProxyEmbeddingsOrq,
                models.PostV2ProxyEmbeddingsOrqTypedDict,
            ]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyEmbeddingsResponseBody]:
        r"""Create embeddings

        Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.

        :param input: Input text to embed, encoded as a string or array of tokens.
        :param model: ID of the model to use
        :param encoding_format: Type of the document element
        :param dimensions: The number of dimensions the resulting output embeddings should have.
        :param user: A unique identifier representing your end-user
        :param orq:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyEmbeddingsRequestBody(
            input=input_,
            model=model,
            encoding_format=encoding_format,
            dimensions=dimensions,
            user=user,
            orq=utils.get_pydantic_model(
                orq, Optional[models.PostV2ProxyEmbeddingsOrq]
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/embeddings",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.PostV2ProxyEmbeddingsRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/embeddings",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyEmbeddingsResponseBody], http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    def rerank(
        self,
        *,
        query: str,
        documents: Union[
            List[models.PostV2ProxyRerankDocuments],
            List[models.PostV2ProxyRerankDocumentsTypedDict],
        ],
        model: str,
        top_n: Optional[float] = None,
        filename: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyRerankResponseBody]:
        r"""Rerank a list of documents based on their relevance to a query.

        :param query: The search query
        :param documents: A list of document objects or strings to rerank.
        :param model: The identifier of the model to use
        :param top_n: The number of most relevant documents or indices to return, defaults to the length of the documents
        :param filename: The filename of the document to rerank
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyRerankRequestBody(
            query=query,
            documents=documents,
            model=model,
            top_n=top_n,
            filename=filename,
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/rerank",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.PostV2ProxyRerankRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/rerank",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyRerankResponseBody], http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    async def rerank_async(
        self,
        *,
        query: str,
        documents: Union[
            List[models.PostV2ProxyRerankDocuments],
            List[models.PostV2ProxyRerankDocumentsTypedDict],
        ],
        model: str,
        top_n: Optional[float] = None,
        filename: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyRerankResponseBody]:
        r"""Rerank a list of documents based on their relevance to a query.

        :param query: The search query
        :param documents: A list of document objects or strings to rerank.
        :param model: The identifier of the model to use
        :param top_n: The number of most relevant documents or indices to return, defaults to the length of the documents
        :param filename: The filename of the document to rerank
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyRerankRequestBody(
            query=query,
            documents=documents,
            model=model,
            top_n=top_n,
            filename=filename,
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/rerank",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.PostV2ProxyRerankRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/rerank",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyRerankResponseBody], http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    def audio_speech(
        self,
        *,
        input_: str,
        model: Union[models.Model, models.ModelTypedDict],
        voice: str,
        response_format: Optional[models.PostV2ProxyAudioSpeechResponseFormat] = "mp3",
        speed: Optional[float] = 1,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ):
        r"""Generates audio from the input text.

        :param input: The text to generate audio for. The maximum length is 4096 characters
        :param model: ID of the model to use
        :param voice: The voice to use.    Available voices for OpenAI    `alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer`    Available voices for ElevenLabs    `aria`, `roger`, `sarah`, `laura`, `charlie`, `george`, `callum`, `river`, `liam`, `charlotte`, `alice`, `matilda`, `will`, `jessica`, `eric`, `chris`, `brian`, `daniel`, `lily`, `bill`
        :param response_format: The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`, `wav`, and `pcm`. If a format is provided but not supported by the provider, the response will be in the default format. When the provided format is not supported by the provider, the response will be in the default format.
        :param speed: The speed of the generated audio.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyAudioSpeechRequestBody(
            input=input_,
            model=model,
            voice=voice,
            response_format=response_format,
            speed=speed,
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/audio/speech",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="*/*",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.PostV2ProxyAudioSpeechRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/audio/speech",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "*"):
            return
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    async def audio_speech_async(
        self,
        *,
        input_: str,
        model: Union[models.Model, models.ModelTypedDict],
        voice: str,
        response_format: Optional[models.PostV2ProxyAudioSpeechResponseFormat] = "mp3",
        speed: Optional[float] = 1,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ):
        r"""Generates audio from the input text.

        :param input: The text to generate audio for. The maximum length is 4096 characters
        :param model: ID of the model to use
        :param voice: The voice to use.    Available voices for OpenAI    `alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer`    Available voices for ElevenLabs    `aria`, `roger`, `sarah`, `laura`, `charlie`, `george`, `callum`, `river`, `liam`, `charlotte`, `alice`, `matilda`, `will`, `jessica`, `eric`, `chris`, `brian`, `daniel`, `lily`, `bill`
        :param response_format: The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`, `wav`, and `pcm`. If a format is provided but not supported by the provider, the response will be in the default format. When the provided format is not supported by the provider, the response will be in the default format.
        :param speed: The speed of the generated audio.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyAudioSpeechRequestBody(
            input=input_,
            model=model,
            voice=voice,
            response_format=response_format,
            speed=speed,
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/audio/speech",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="*/*",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.PostV2ProxyAudioSpeechRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/audio/speech",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "*"):
            return
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    def completions(
        self,
        *,
        model: str,
        prompt: str,
        echo: OptionalNullable[bool] = False,
        frequency_penalty: OptionalNullable[float] = 0,
        max_tokens: OptionalNullable[float] = 16,
        presence_penalty: OptionalNullable[float] = 0,
        seed: OptionalNullable[float] = UNSET,
        stop: OptionalNullable[
            Union[
                models.PostV2ProxyCompletionsStop,
                models.PostV2ProxyCompletionsStopTypedDict,
            ]
        ] = UNSET,
        temperature: OptionalNullable[float] = 1,
        top_p: OptionalNullable[float] = 1,
        user: Optional[str] = None,
        orq: Optional[
            Union[
                models.PostV2ProxyCompletionsOrq,
                models.PostV2ProxyCompletionsOrqTypedDict,
            ]
        ] = None,
        stream: Optional[bool] = False,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        accept_header_override: Optional[CompletionsAcceptEnum] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyCompletionsResponse]:
        r"""Create a completion

        For sending requests to legacy completion models

        :param model: ID of the model to use
        :param prompt: The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.
        :param echo: Echo back the prompt in addition to the completion
        :param frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        :param max_tokens: The maximum number of tokens that can be generated in the completion.
        :param presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        :param seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
        :param stop: Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
        :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        :param user: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
        :param orq: Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution.
        :param stream:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyCompletionsRequestBody(
            model=model,
            prompt=prompt,
            echo=echo,
            frequency_penalty=frequency_penalty,
            max_tokens=max_tokens,
            presence_penalty=presence_penalty,
            seed=seed,
            stop=stop,
            temperature=temperature,
            top_p=top_p,
            user=user,
            orq=utils.get_pydantic_model(
                orq, Optional[models.PostV2ProxyCompletionsOrq]
            ),
            stream=stream,
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value=accept_header_override.value
            if accept_header_override is not None
            else "application/json;q=1, text/event-stream;q=0",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.PostV2ProxyCompletionsRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/completions",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            return unmarshal_json_response(
                Optional[models.PostV2ProxyCompletionsResponseBody],
                http_res,
                http_res_text,
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.PostV2ProxyCompletionsProxyResponseBody
                ),
                client_ref=self,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        http_res_text = utils.stream_to_text(http_res)
        raise models.APIError("Unexpected response received", http_res, http_res_text)

    async def completions_async(
        self,
        *,
        model: str,
        prompt: str,
        echo: OptionalNullable[bool] = False,
        frequency_penalty: OptionalNullable[float] = 0,
        max_tokens: OptionalNullable[float] = 16,
        presence_penalty: OptionalNullable[float] = 0,
        seed: OptionalNullable[float] = UNSET,
        stop: OptionalNullable[
            Union[
                models.PostV2ProxyCompletionsStop,
                models.PostV2ProxyCompletionsStopTypedDict,
            ]
        ] = UNSET,
        temperature: OptionalNullable[float] = 1,
        top_p: OptionalNullable[float] = 1,
        user: Optional[str] = None,
        orq: Optional[
            Union[
                models.PostV2ProxyCompletionsOrq,
                models.PostV2ProxyCompletionsOrqTypedDict,
            ]
        ] = None,
        stream: Optional[bool] = False,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        accept_header_override: Optional[CompletionsAcceptEnum] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyCompletionsResponse]:
        r"""Create a completion

        For sending requests to legacy completion models

        :param model: ID of the model to use
        :param prompt: The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.
        :param echo: Echo back the prompt in addition to the completion
        :param frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        :param max_tokens: The maximum number of tokens that can be generated in the completion.
        :param presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        :param seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
        :param stop: Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
        :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        :param user: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
        :param orq: Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution.
        :param stream:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyCompletionsRequestBody(
            model=model,
            prompt=prompt,
            echo=echo,
            frequency_penalty=frequency_penalty,
            max_tokens=max_tokens,
            presence_penalty=presence_penalty,
            seed=seed,
            stop=stop,
            temperature=temperature,
            top_p=top_p,
            user=user,
            orq=utils.get_pydantic_model(
                orq, Optional[models.PostV2ProxyCompletionsOrq]
            ),
            stream=stream,
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value=accept_header_override.value
            if accept_header_override is not None
            else "application/json;q=1, text/event-stream;q=0",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.PostV2ProxyCompletionsRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/completions",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            return unmarshal_json_response(
                Optional[models.PostV2ProxyCompletionsResponseBody],
                http_res,
                http_res_text,
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.PostV2ProxyCompletionsProxyResponseBody
                ),
                client_ref=self,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.APIError("Unexpected response received", http_res, http_res_text)

    def images_generate(
        self,
        *,
        prompt: str,
        model: str,
        background: OptionalNullable[models.Background] = "auto",
        moderation: OptionalNullable[models.Moderation] = "auto",
        n: OptionalNullable[int] = 1,
        output_compression: OptionalNullable[int] = 100,
        output_format: OptionalNullable[models.OutputFormat] = "png",
        quality: OptionalNullable[models.Quality] = "auto",
        response_format: OptionalNullable[
            models.PostV2ProxyImagesGenerationsResponseFormat
        ] = "url",
        size: OptionalNullable[str] = UNSET,
        style: OptionalNullable[str] = UNSET,
        orq: Optional[
            Union[
                models.PostV2ProxyImagesGenerationsOrq,
                models.PostV2ProxyImagesGenerationsOrqTypedDict,
            ]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyImagesGenerationsResponseBody]:
        r"""Create an Image

        :param prompt: A text description of the desired image(s).
        :param model: The model to use for image generation. Defaults to dall-e-2 unless a parameter specific to gpt-image-1 is used.
        :param background: Allows to set transparency for the background of the generated image(s). This parameter is only supported for `openai/gpt-image-1`.
        :param moderation: Control the content-moderation level for images generated by `openai/gpt-image-1`.
        :param n: The number of images to generate. Must be between 1 and 10. For dall-e-3, only n=1 is supported.
        :param output_compression: The compression level (0-100%) for the generated images. This parameter is only supported for gpt-image-1 with the webp or jpeg output formats.
        :param output_format: The format in which the generated images are returned. This parameter is only supported for `openai/gpt-image-1`.
        :param quality: The quality of the image that will be generated. Auto will automatically select the best quality for the given model.
        :param response_format: The format in which generated images with are returned. This parameter isn't supported for `openai/gpt-image-1` which will always return base64-encoded images.
        :param size: The size of the generated images. Must be one of the specified sizes for each model.
        :param style: The style of the generated images. This parameter is only supported for dall-e-3.
        :param orq:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyImagesGenerationsRequestBody(
            prompt=prompt,
            background=background,
            model=model,
            moderation=moderation,
            n=n,
            output_compression=output_compression,
            output_format=output_format,
            quality=quality,
            response_format=response_format,
            size=size,
            style=style,
            orq=utils.get_pydantic_model(
                orq, Optional[models.PostV2ProxyImagesGenerationsOrq]
            ),
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/images/generations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "json",
                models.PostV2ProxyImagesGenerationsRequestBody,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/images/generations",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyImagesGenerationsResponseBody], http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    async def images_generate_async(
        self,
        *,
        prompt: str,
        model: str,
        background: OptionalNullable[models.Background] = "auto",
        moderation: OptionalNullable[models.Moderation] = "auto",
        n: OptionalNullable[int] = 1,
        output_compression: OptionalNullable[int] = 100,
        output_format: OptionalNullable[models.OutputFormat] = "png",
        quality: OptionalNullable[models.Quality] = "auto",
        response_format: OptionalNullable[
            models.PostV2ProxyImagesGenerationsResponseFormat
        ] = "url",
        size: OptionalNullable[str] = UNSET,
        style: OptionalNullable[str] = UNSET,
        orq: Optional[
            Union[
                models.PostV2ProxyImagesGenerationsOrq,
                models.PostV2ProxyImagesGenerationsOrqTypedDict,
            ]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyImagesGenerationsResponseBody]:
        r"""Create an Image

        :param prompt: A text description of the desired image(s).
        :param model: The model to use for image generation. Defaults to dall-e-2 unless a parameter specific to gpt-image-1 is used.
        :param background: Allows to set transparency for the background of the generated image(s). This parameter is only supported for `openai/gpt-image-1`.
        :param moderation: Control the content-moderation level for images generated by `openai/gpt-image-1`.
        :param n: The number of images to generate. Must be between 1 and 10. For dall-e-3, only n=1 is supported.
        :param output_compression: The compression level (0-100%) for the generated images. This parameter is only supported for gpt-image-1 with the webp or jpeg output formats.
        :param output_format: The format in which the generated images are returned. This parameter is only supported for `openai/gpt-image-1`.
        :param quality: The quality of the image that will be generated. Auto will automatically select the best quality for the given model.
        :param response_format: The format in which generated images with are returned. This parameter isn't supported for `openai/gpt-image-1` which will always return base64-encoded images.
        :param size: The size of the generated images. Must be one of the specified sizes for each model.
        :param style: The style of the generated images. This parameter is only supported for dall-e-3.
        :param orq:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyImagesGenerationsRequestBody(
            prompt=prompt,
            background=background,
            model=model,
            moderation=moderation,
            n=n,
            output_compression=output_compression,
            output_format=output_format,
            quality=quality,
            response_format=response_format,
            size=size,
            style=style,
            orq=utils.get_pydantic_model(
                orq, Optional[models.PostV2ProxyImagesGenerationsOrq]
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/images/generations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "json",
                models.PostV2ProxyImagesGenerationsRequestBody,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/images/generations",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyImagesGenerationsResponseBody], http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    def images_edit(
        self,
        *,
        model: str,
        image: Union[models.Image, models.ImageTypedDict],
        prompt: str,
        n: OptionalNullable[int] = 1,
        size: OptionalNullable[str] = UNSET,
        response_format: OptionalNullable[
            models.PostV2ProxyImagesEditsResponseFormat
        ] = "url",
        user: Optional[str] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyImagesEditsResponseBody]:
        r"""Edit an Image

        :param model: The model to use for image generation. [Check models](https://docs.orq.ai/docs/proxy#/image-models)
        :param image: The image(s) to edit. Must be a supported image file or an array of images.  Each image should be a png, webp, or jpg file less than 50MB. You can provide up to 16 images.
        :param prompt: A text description of the desired image(s).
        :param n: The number of images to generate. Must be between 1 and 10.
        :param size: The size of the generated images
        :param response_format: The format in which the generated images are returned. Some of the models only return the image in base64 format.
        :param user: A unique identifier representing your end-user, which can help to monitor and detect abuse.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyImagesEditsRequestBody(
            model=model,
            image=image,
            prompt=prompt,
            n=n,
            size=size,
            response_format=response_format,
            user=user,
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/images/edits",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "multipart",
                models.PostV2ProxyImagesEditsRequestBody,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/images/edits",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyImagesEditsResponseBody], http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    async def images_edit_async(
        self,
        *,
        model: str,
        image: Union[models.Image, models.ImageTypedDict],
        prompt: str,
        n: OptionalNullable[int] = 1,
        size: OptionalNullable[str] = UNSET,
        response_format: OptionalNullable[
            models.PostV2ProxyImagesEditsResponseFormat
        ] = "url",
        user: Optional[str] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyImagesEditsResponseBody]:
        r"""Edit an Image

        :param model: The model to use for image generation. [Check models](https://docs.orq.ai/docs/proxy#/image-models)
        :param image: The image(s) to edit. Must be a supported image file or an array of images.  Each image should be a png, webp, or jpg file less than 50MB. You can provide up to 16 images.
        :param prompt: A text description of the desired image(s).
        :param n: The number of images to generate. Must be between 1 and 10.
        :param size: The size of the generated images
        :param response_format: The format in which the generated images are returned. Some of the models only return the image in base64 format.
        :param user: A unique identifier representing your end-user, which can help to monitor and detect abuse.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyImagesEditsRequestBody(
            model=model,
            image=image,
            prompt=prompt,
            n=n,
            size=size,
            response_format=response_format,
            user=user,
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/images/edits",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "multipart",
                models.PostV2ProxyImagesEditsRequestBody,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/images/edits",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyImagesEditsResponseBody], http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    def images_variation(
        self,
        *,
        image: Any,
        model: str,
        n: OptionalNullable[int] = 1,
        response_format: Optional[
            models.PostV2ProxyImagesVariationsResponseFormat
        ] = "url",
        size: Optional[models.Size] = "1024x1024",
        user: Optional[str] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyImagesVariationsResponseBody]:
        r"""Create an Image Variation

        :param image: The image to edit. Must be a supported image file. It should be a png, webp, or jpg file less than 50MB.
        :param model: The model to use for image generation.
        :param n: The number of images to generate. Must be between 1 and 10.
        :param response_format: The format in which the generated images are returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes after the image has been generated.
        :param size: The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`.
        :param user: A unique identifier representing your end-user, which can help to monitor and detect abuse.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyImagesVariationsRequestBody(
            image=image,
            model=model,
            n=n,
            response_format=response_format,
            size=size,
            user=user,
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/images/variations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "multipart",
                models.PostV2ProxyImagesVariationsRequestBody,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/images/variations",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyImagesVariationsResponseBody], http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    async def images_variation_async(
        self,
        *,
        image: Any,
        model: str,
        n: OptionalNullable[int] = 1,
        response_format: Optional[
            models.PostV2ProxyImagesVariationsResponseFormat
        ] = "url",
        size: Optional[models.Size] = "1024x1024",
        user: Optional[str] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyImagesVariationsResponseBody]:
        r"""Create an Image Variation

        :param image: The image to edit. Must be a supported image file. It should be a png, webp, or jpg file less than 50MB.
        :param model: The model to use for image generation.
        :param n: The number of images to generate. Must be between 1 and 10.
        :param response_format: The format in which the generated images are returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes after the image has been generated.
        :param size: The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`.
        :param user: A unique identifier representing your end-user, which can help to monitor and detect abuse.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyImagesVariationsRequestBody(
            image=image,
            model=model,
            n=n,
            response_format=response_format,
            size=size,
            user=user,
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/images/variations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "multipart",
                models.PostV2ProxyImagesVariationsRequestBody,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/images/variations",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyImagesVariationsResponseBody], http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    def audio_transcriptions(
        self,
        *,
        file: Union[
            models.PostV2ProxyAudioTranscriptionsFile,
            models.PostV2ProxyAudioTranscriptionsFileTypedDict,
        ],
        model: Optional[str] = "openai/whisper-1",
        prompt: Optional[str] = None,
        enable_logging: Optional[bool] = True,
        diarize: Optional[bool] = False,
        response_format: Optional[
            models.PostV2ProxyAudioTranscriptionsResponseFormat
        ] = None,
        tag_audio_events: Optional[bool] = True,
        num_speakers: Optional[float] = None,
        timestamps_granularity: Optional[models.TimestampsGranularity] = "word",
        temperature: Optional[float] = None,
        language: Optional[str] = None,
        timestamp_granularities: Optional[List[models.TimestampGranularities]] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyAudioTranscriptionsResponseBody]:
        r"""
        :param file: The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
        :param model: ID of the model to use
        :param prompt: An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.
        :param enable_logging: When enable_logging is set to false, zero retention mode is used. This disables history features like request stitching and is only available to enterprise customers.
        :param diarize: Whether to annotate which speaker is currently talking in the uploaded file.
        :param response_format: The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt.
        :param tag_audio_events: Whether to tag audio events like (laughter), (footsteps), etc. in the transcription.
        :param num_speakers: The maximum amount of speakers talking in the uploaded file. Helps with predicting who speaks when, the maximum is 32.
        :param timestamps_granularity: The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word.
        :param temperature: The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.
        :param language: The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.
        :param timestamp_granularities: The timestamp granularities to populate for this transcription. response_format must be set to verbose_json to use timestamp granularities. Either or both of these options are supported: \"word\" or \"segment\". Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyAudioTranscriptionsRequestBody(
            model=model,
            prompt=prompt,
            enable_logging=enable_logging,
            diarize=diarize,
            response_format=response_format,
            tag_audio_events=tag_audio_events,
            num_speakers=num_speakers,
            timestamps_granularity=timestamps_granularity,
            temperature=temperature,
            language=language,
            timestamp_granularities=timestamp_granularities,
            file=utils.get_pydantic_model(
                file, models.PostV2ProxyAudioTranscriptionsFile
            ),
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/audio/transcriptions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "multipart",
                models.PostV2ProxyAudioTranscriptionsRequestBody,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/audio/transcriptions",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyAudioTranscriptionsResponseBody], http_res
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = unmarshal_json_response(
                models.PostV2ProxyAudioTranscriptionsProxyResponseBodyData, http_res
            )
            raise models.PostV2ProxyAudioTranscriptionsProxyResponseBody(
                response_data, http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    async def audio_transcriptions_async(
        self,
        *,
        file: Union[
            models.PostV2ProxyAudioTranscriptionsFile,
            models.PostV2ProxyAudioTranscriptionsFileTypedDict,
        ],
        model: Optional[str] = "openai/whisper-1",
        prompt: Optional[str] = None,
        enable_logging: Optional[bool] = True,
        diarize: Optional[bool] = False,
        response_format: Optional[
            models.PostV2ProxyAudioTranscriptionsResponseFormat
        ] = None,
        tag_audio_events: Optional[bool] = True,
        num_speakers: Optional[float] = None,
        timestamps_granularity: Optional[models.TimestampsGranularity] = "word",
        temperature: Optional[float] = None,
        language: Optional[str] = None,
        timestamp_granularities: Optional[List[models.TimestampGranularities]] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyAudioTranscriptionsResponseBody]:
        r"""
        :param file: The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
        :param model: ID of the model to use
        :param prompt: An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.
        :param enable_logging: When enable_logging is set to false, zero retention mode is used. This disables history features like request stitching and is only available to enterprise customers.
        :param diarize: Whether to annotate which speaker is currently talking in the uploaded file.
        :param response_format: The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt.
        :param tag_audio_events: Whether to tag audio events like (laughter), (footsteps), etc. in the transcription.
        :param num_speakers: The maximum amount of speakers talking in the uploaded file. Helps with predicting who speaks when, the maximum is 32.
        :param timestamps_granularity: The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word.
        :param temperature: The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.
        :param language: The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.
        :param timestamp_granularities: The timestamp granularities to populate for this transcription. response_format must be set to verbose_json to use timestamp granularities. Either or both of these options are supported: \"word\" or \"segment\". Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyAudioTranscriptionsRequestBody(
            model=model,
            prompt=prompt,
            enable_logging=enable_logging,
            diarize=diarize,
            response_format=response_format,
            tag_audio_events=tag_audio_events,
            num_speakers=num_speakers,
            timestamps_granularity=timestamps_granularity,
            temperature=temperature,
            language=language,
            timestamp_granularities=timestamp_granularities,
            file=utils.get_pydantic_model(
                file, models.PostV2ProxyAudioTranscriptionsFile
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/audio/transcriptions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "multipart",
                models.PostV2ProxyAudioTranscriptionsRequestBody,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/audio/transcriptions",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyAudioTranscriptionsResponseBody], http_res
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = unmarshal_json_response(
                models.PostV2ProxyAudioTranscriptionsProxyResponseBodyData, http_res
            )
            raise models.PostV2ProxyAudioTranscriptionsProxyResponseBody(
                response_data, http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    def audio_translations(
        self,
        *,
        file: Union[
            models.PostV2ProxyAudioTranslationsFile,
            models.PostV2ProxyAudioTranslationsFileTypedDict,
        ],
        model: Optional[str] = "openai/whisper-1",
        prompt: Optional[str] = None,
        enable_logging: Optional[bool] = True,
        diarize: Optional[bool] = False,
        response_format: Optional[
            models.PostV2ProxyAudioTranslationsResponseFormat
        ] = None,
        tag_audio_events: Optional[bool] = True,
        num_speakers: Optional[float] = None,
        timestamps_granularity: Optional[
            models.PostV2ProxyAudioTranslationsTimestampsGranularity
        ] = "word",
        temperature: Optional[float] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyAudioTranslationsResponseBody]:
        r"""
        :param file: The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
        :param model: ID of the model to use
        :param prompt: An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.
        :param enable_logging: When enable_logging is set to false, zero retention mode is used. This disables history features like request stitching and is only available to enterprise customers.
        :param diarize: Whether to annotate which speaker is currently talking in the uploaded file.
        :param response_format: The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt.
        :param tag_audio_events: Whether to tag audio events like (laughter), (footsteps), etc. in the transcription.
        :param num_speakers: The maximum amount of speakers talking in the uploaded file. Helps with predicting who speaks when, the maximum is 32.
        :param timestamps_granularity: The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word.
        :param temperature: The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyAudioTranslationsRequestBody(
            model=model,
            prompt=prompt,
            enable_logging=enable_logging,
            diarize=diarize,
            response_format=response_format,
            tag_audio_events=tag_audio_events,
            num_speakers=num_speakers,
            timestamps_granularity=timestamps_granularity,
            temperature=temperature,
            file=utils.get_pydantic_model(
                file, models.PostV2ProxyAudioTranslationsFile
            ),
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/audio/translations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "multipart",
                models.PostV2ProxyAudioTranslationsRequestBody,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/audio/translations",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyAudioTranslationsResponseBody], http_res
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = unmarshal_json_response(
                models.PostV2ProxyAudioTranslationsProxyResponseBodyData, http_res
            )
            raise models.PostV2ProxyAudioTranslationsProxyResponseBody(
                response_data, http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    async def audio_translations_async(
        self,
        *,
        file: Union[
            models.PostV2ProxyAudioTranslationsFile,
            models.PostV2ProxyAudioTranslationsFileTypedDict,
        ],
        model: Optional[str] = "openai/whisper-1",
        prompt: Optional[str] = None,
        enable_logging: Optional[bool] = True,
        diarize: Optional[bool] = False,
        response_format: Optional[
            models.PostV2ProxyAudioTranslationsResponseFormat
        ] = None,
        tag_audio_events: Optional[bool] = True,
        num_speakers: Optional[float] = None,
        timestamps_granularity: Optional[
            models.PostV2ProxyAudioTranslationsTimestampsGranularity
        ] = "word",
        temperature: Optional[float] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.PostV2ProxyAudioTranslationsResponseBody]:
        r"""
        :param file: The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
        :param model: ID of the model to use
        :param prompt: An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.
        :param enable_logging: When enable_logging is set to false, zero retention mode is used. This disables history features like request stitching and is only available to enterprise customers.
        :param diarize: Whether to annotate which speaker is currently talking in the uploaded file.
        :param response_format: The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt.
        :param tag_audio_events: Whether to tag audio events like (laughter), (footsteps), etc. in the transcription.
        :param num_speakers: The maximum amount of speakers talking in the uploaded file. Helps with predicting who speaks when, the maximum is 32.
        :param timestamps_granularity: The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word.
        :param temperature: The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PostV2ProxyAudioTranslationsRequestBody(
            model=model,
            prompt=prompt,
            enable_logging=enable_logging,
            diarize=diarize,
            response_format=response_format,
            tag_audio_events=tag_audio_events,
            num_speakers=num_speakers,
            timestamps_granularity=timestamps_granularity,
            temperature=temperature,
            file=utils.get_pydantic_model(
                file, models.PostV2ProxyAudioTranslationsFile
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/audio/translations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "multipart",
                models.PostV2ProxyAudioTranslationsRequestBody,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="post_/v2/proxy/audio/translations",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                Optional[models.PostV2ProxyAudioTranslationsResponseBody], http_res
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = unmarshal_json_response(
                models.PostV2ProxyAudioTranslationsProxyResponseBodyData, http_res
            )
            raise models.PostV2ProxyAudioTranslationsProxyResponseBody(
                response_data, http_res
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        raise models.APIError("Unexpected response received", http_res)

    def responses_create(
        self,
        *,
        model: str,
        input_: Union[models.CreateResponseInput, models.CreateResponseInputTypedDict],
        metadata: Optional[Dict[str, str]] = None,
        temperature: OptionalNullable[float] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        previous_response_id: OptionalNullable[str] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        reasoning: OptionalNullable[
            Union[models.Reasoning, models.ReasoningTypedDict]
        ] = UNSET,
        max_output_tokens: OptionalNullable[int] = UNSET,
        text: OptionalNullable[Union[models.Text, models.TextTypedDict]] = UNSET,
        include: OptionalNullable[List[models.Include]] = UNSET,
        parallel_tool_calls: OptionalNullable[bool] = UNSET,
        store: OptionalNullable[bool] = True,
        tools: Optional[
            Union[
                List[models.CreateResponseTools],
                List[models.CreateResponseToolsTypedDict],
            ]
        ] = None,
        tool_choice: Optional[
            Union[
                models.CreateResponseToolChoice,
                models.CreateResponseToolChoiceTypedDict,
            ]
        ] = None,
        stream: Optional[bool] = False,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        accept_header_override: Optional[ResponsesCreateAcceptEnum] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.CreateResponseResponse]:
        r"""Create response

        Creates a model response for the given input.

        :param model: ID of the model to use. You can use the List models API to see all of your available models.
        :param input: The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input.
        :param metadata: Developer-defined key-value pairs that will be included in response objects
        :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        :param previous_response_id: The ID of a previous response to continue the conversation from. The model will have access to the previous response context.
        :param instructions: Developer-provided instructions that the model should follow. Overwrites the default system message.
        :param reasoning: Configuration for reasoning models
        :param max_output_tokens: The maximum number of tokens that can be generated in the response
        :param text:
        :param include: Specifies which (potentially large) fields to include in the response. By default, the results of Code Interpreter and file searches are excluded. Available options: - code_interpreter_call.outputs: Include the outputs of Code Interpreter tool calls - computer_call_output.output.image_url: Include the image URLs from computer use tool calls - file_search_call.results: Include the results of file search tool calls - message.input_image.image_url: Include URLs of input images - message.output_text.logprobs: Include log probabilities for output text (when logprobs is enabled) - reasoning.encrypted_content: Include encrypted reasoning content for reasoning models
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use.
        :param store: Whether to store this response for use in distillations or evals.
        :param tools: A list of tools the model may call. Use this to provide a list of functions the model may generate JSON inputs for.
        :param tool_choice: How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool.
        :param stream:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.CreateResponseRequestBody(
            model=model,
            metadata=metadata,
            temperature=temperature,
            top_p=top_p,
            previous_response_id=previous_response_id,
            instructions=instructions,
            reasoning=utils.get_pydantic_model(
                reasoning, OptionalNullable[models.Reasoning]
            ),
            max_output_tokens=max_output_tokens,
            text=utils.get_pydantic_model(text, OptionalNullable[models.Text]),
            input=utils.get_pydantic_model(input_, models.CreateResponseInput),
            include=include,
            parallel_tool_calls=parallel_tool_calls,
            store=store,
            tools=utils.get_pydantic_model(
                tools, Optional[List[models.CreateResponseTools]]
            ),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.CreateResponseToolChoice]
            ),
            stream=stream,
        )

        req = self._build_request(
            method="POST",
            path="/v2/proxy/responses",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value=accept_header_override.value
            if accept_header_override is not None
            else "application/json;q=1, text/event-stream;q=0",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.CreateResponseRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="CreateResponse",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            return unmarshal_json_response(
                Optional[models.CreateResponseResponseBody], http_res, http_res_text
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.CreateResponseProxyResponseBody
                ),
                client_ref=self,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        http_res_text = utils.stream_to_text(http_res)
        raise models.APIError("Unexpected response received", http_res, http_res_text)

    async def responses_create_async(
        self,
        *,
        model: str,
        input_: Union[models.CreateResponseInput, models.CreateResponseInputTypedDict],
        metadata: Optional[Dict[str, str]] = None,
        temperature: OptionalNullable[float] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        previous_response_id: OptionalNullable[str] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        reasoning: OptionalNullable[
            Union[models.Reasoning, models.ReasoningTypedDict]
        ] = UNSET,
        max_output_tokens: OptionalNullable[int] = UNSET,
        text: OptionalNullable[Union[models.Text, models.TextTypedDict]] = UNSET,
        include: OptionalNullable[List[models.Include]] = UNSET,
        parallel_tool_calls: OptionalNullable[bool] = UNSET,
        store: OptionalNullable[bool] = True,
        tools: Optional[
            Union[
                List[models.CreateResponseTools],
                List[models.CreateResponseToolsTypedDict],
            ]
        ] = None,
        tool_choice: Optional[
            Union[
                models.CreateResponseToolChoice,
                models.CreateResponseToolChoiceTypedDict,
            ]
        ] = None,
        stream: Optional[bool] = False,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        accept_header_override: Optional[ResponsesCreateAcceptEnum] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.CreateResponseResponse]:
        r"""Create response

        Creates a model response for the given input.

        :param model: ID of the model to use. You can use the List models API to see all of your available models.
        :param input: The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input.
        :param metadata: Developer-defined key-value pairs that will be included in response objects
        :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        :param previous_response_id: The ID of a previous response to continue the conversation from. The model will have access to the previous response context.
        :param instructions: Developer-provided instructions that the model should follow. Overwrites the default system message.
        :param reasoning: Configuration for reasoning models
        :param max_output_tokens: The maximum number of tokens that can be generated in the response
        :param text:
        :param include: Specifies which (potentially large) fields to include in the response. By default, the results of Code Interpreter and file searches are excluded. Available options: - code_interpreter_call.outputs: Include the outputs of Code Interpreter tool calls - computer_call_output.output.image_url: Include the image URLs from computer use tool calls - file_search_call.results: Include the results of file search tool calls - message.input_image.image_url: Include URLs of input images - message.output_text.logprobs: Include log probabilities for output text (when logprobs is enabled) - reasoning.encrypted_content: Include encrypted reasoning content for reasoning models
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use.
        :param store: Whether to store this response for use in distillations or evals.
        :param tools: A list of tools the model may call. Use this to provide a list of functions the model may generate JSON inputs for.
        :param tool_choice: How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool.
        :param stream:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.CreateResponseRequestBody(
            model=model,
            metadata=metadata,
            temperature=temperature,
            top_p=top_p,
            previous_response_id=previous_response_id,
            instructions=instructions,
            reasoning=utils.get_pydantic_model(
                reasoning, OptionalNullable[models.Reasoning]
            ),
            max_output_tokens=max_output_tokens,
            text=utils.get_pydantic_model(text, OptionalNullable[models.Text]),
            input=utils.get_pydantic_model(input_, models.CreateResponseInput),
            include=include,
            parallel_tool_calls=parallel_tool_calls,
            store=store,
            tools=utils.get_pydantic_model(
                tools, Optional[List[models.CreateResponseTools]]
            ),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.CreateResponseToolChoice]
            ),
            stream=stream,
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/proxy/responses",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value=accept_header_override.value
            if accept_header_override is not None
            else "application/json;q=1, text/event-stream;q=0",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.CreateResponseRequestBody
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="CreateResponse",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            return unmarshal_json_response(
                Optional[models.CreateResponseResponseBody], http_res, http_res_text
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.CreateResponseProxyResponseBody
                ),
                client_ref=self,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.APIError("Unexpected response received", http_res, http_res_text)
