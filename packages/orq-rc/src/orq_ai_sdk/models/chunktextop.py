"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from orq_ai_sdk.types import BaseModel, Nullable, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ChunkTextChunkingRequestKnowledgeRequestRequestBodyReturnType = Literal[
    "chunks", "texts"
]
r"""Return format: chunks (with metadata) or texts (plain strings)"""

AgenticChunker = Literal["agentic"]


class AgenticChunkerStrategyTypedDict(TypedDict):
    r"""Agentic LLM-powered chunker that uses AI to determine optimal split points. Best for complex documents requiring intelligent segmentation."""

    text: str
    r"""The text content to be chunked"""
    strategy: AgenticChunker
    model: str
    r"""Chat model to use for chunking. (Available models)[https://docs.orq.ai/docs/proxy#chat-models]"""
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[
        ChunkTextChunkingRequestKnowledgeRequestRequestBodyReturnType
    ]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    candidate_size: NotRequired[int]
    r"""Size of candidate splits for LLM evaluation"""
    min_characters_per_chunk: NotRequired[int]
    r"""Minimum characters allowed per chunk"""


class AgenticChunkerStrategy(BaseModel):
    r"""Agentic LLM-powered chunker that uses AI to determine optimal split points. Best for complex documents requiring intelligent segmentation."""

    text: str
    r"""The text content to be chunked"""

    strategy: AgenticChunker

    model: str
    r"""Chat model to use for chunking. (Available models)[https://docs.orq.ai/docs/proxy#chat-models]"""

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[
        ChunkTextChunkingRequestKnowledgeRequestRequestBodyReturnType
    ] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 1024
    r"""Maximum tokens per chunk"""

    candidate_size: Optional[int] = 128
    r"""Size of candidate splits for LLM evaluation"""

    min_characters_per_chunk: Optional[int] = 24
    r"""Minimum characters allowed per chunk"""


ChunkTextChunkingRequestKnowledgeRequestReturnType = Literal["chunks", "texts"]
r"""Return format: chunks (with metadata) or texts (plain strings)"""

SDPMChunker = Literal["sdpm"]

ChunkTextThreshold2 = Literal["auto"]

ChunkingRequestThresholdTypedDict = TypeAliasType(
    "ChunkingRequestThresholdTypedDict", Union[float, ChunkTextThreshold2]
)
r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""


ChunkingRequestThreshold = TypeAliasType(
    "ChunkingRequestThreshold", Union[float, ChunkTextThreshold2]
)
r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""


ChunkingRequestMode = Literal["window", "sentence"]
r"""Chunking mode: window-based or sentence-based similarity"""


class SDPMChunkerStrategyTypedDict(TypedDict):
    r"""Sub-Document Prose Model chunker that uses skip-gram patterns to identify optimal split points. Good for technical documents with structured content."""

    text: str
    r"""The text content to be chunked"""
    strategy: SDPMChunker
    embedding_model: str
    r"""Embedding model to use for semantic similarity. (Available embedding models)[https://docs.orq.ai/docs/proxy#embedding-models]"""
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[ChunkTextChunkingRequestKnowledgeRequestReturnType]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    skip_window: NotRequired[int]
    r"""Window size for skip-gram patterns"""
    threshold: NotRequired[ChunkingRequestThresholdTypedDict]
    r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""
    mode: NotRequired[ChunkingRequestMode]
    r"""Chunking mode: window-based or sentence-based similarity"""


class SDPMChunkerStrategy(BaseModel):
    r"""Sub-Document Prose Model chunker that uses skip-gram patterns to identify optimal split points. Good for technical documents with structured content."""

    text: str
    r"""The text content to be chunked"""

    strategy: SDPMChunker

    embedding_model: str
    r"""Embedding model to use for semantic similarity. (Available embedding models)[https://docs.orq.ai/docs/proxy#embedding-models]"""

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[ChunkTextChunkingRequestKnowledgeRequestReturnType] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 512
    r"""Maximum tokens per chunk"""

    skip_window: Optional[int] = 1
    r"""Window size for skip-gram patterns"""

    threshold: Optional[ChunkingRequestThreshold] = None
    r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""

    mode: Optional[ChunkingRequestMode] = "window"
    r"""Chunking mode: window-based or sentence-based similarity"""


ChunkTextChunkingRequestKnowledgeReturnType = Literal["chunks", "texts"]
r"""Return format: chunks (with metadata) or texts (plain strings)"""

SemanticChunker = Literal["semantic"]

Threshold2 = Literal["auto"]

ThresholdTypedDict = TypeAliasType("ThresholdTypedDict", Union[float, Threshold2])
r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""


Threshold = TypeAliasType("Threshold", Union[float, Threshold2])
r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""


Mode = Literal["window", "sentence"]
r"""Chunking mode: window-based or sentence-based similarity"""


class SemanticChunkerStrategyTypedDict(TypedDict):
    r"""Groups semantically similar sentences using embeddings. Excellent for maintaining topic coherence and context within chunks."""

    text: str
    r"""The text content to be chunked"""
    strategy: SemanticChunker
    embedding_model: str
    r"""Embedding model to use for semantic similarity. (Available embedding models)[https://docs.orq.ai/docs/proxy#embedding-models]"""
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[ChunkTextChunkingRequestKnowledgeReturnType]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    threshold: NotRequired[ThresholdTypedDict]
    r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""
    mode: NotRequired[Mode]
    r"""Chunking mode: window-based or sentence-based similarity"""
    similarity_window: NotRequired[int]
    r"""Window size for similarity comparison"""


class SemanticChunkerStrategy(BaseModel):
    r"""Groups semantically similar sentences using embeddings. Excellent for maintaining topic coherence and context within chunks."""

    text: str
    r"""The text content to be chunked"""

    strategy: SemanticChunker

    embedding_model: str
    r"""Embedding model to use for semantic similarity. (Available embedding models)[https://docs.orq.ai/docs/proxy#embedding-models]"""

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[ChunkTextChunkingRequestKnowledgeReturnType] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 512
    r"""Maximum tokens per chunk"""

    threshold: Optional[Threshold] = None
    r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""

    mode: Optional[Mode] = "window"
    r"""Chunking mode: window-based or sentence-based similarity"""

    similarity_window: Optional[int] = 1
    r"""Window size for similarity comparison"""


ChunkTextChunkingRequestReturnType = Literal["chunks", "texts"]
r"""Return format: chunks (with metadata) or texts (plain strings)"""

RecursiveChunker = Literal["recursive"]


class RecursiveChunkerStrategyTypedDict(TypedDict):
    r"""Recursively splits text using a hierarchy of separators (paragraphs, sentences, words). Versatile general-purpose chunker that preserves document structure."""

    text: str
    r"""The text content to be chunked"""
    strategy: RecursiveChunker
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[ChunkTextChunkingRequestReturnType]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    separators: NotRequired[List[str]]
    r"""Hierarchy of separators to use for splitting"""
    min_characters_per_chunk: NotRequired[int]
    r"""Minimum characters allowed per chunk"""


class RecursiveChunkerStrategy(BaseModel):
    r"""Recursively splits text using a hierarchy of separators (paragraphs, sentences, words). Versatile general-purpose chunker that preserves document structure."""

    text: str
    r"""The text content to be chunked"""

    strategy: RecursiveChunker

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[ChunkTextChunkingRequestReturnType] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 512
    r"""Maximum tokens per chunk"""

    separators: Optional[List[str]] = None
    r"""Hierarchy of separators to use for splitting"""

    min_characters_per_chunk: Optional[int] = 24
    r"""Minimum characters allowed per chunk"""


ChunkingRequestReturnType = Literal["chunks", "texts"]
r"""Return format: chunks (with metadata) or texts (plain strings)"""

SentenceChunker = Literal["sentence"]


class SentenceChunkerStrategyTypedDict(TypedDict):
    r"""Splits text at sentence boundaries while respecting token limits. Ideal for maintaining semantic coherence and readability."""

    text: str
    r"""The text content to be chunked"""
    strategy: SentenceChunker
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[ChunkingRequestReturnType]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    chunk_overlap: NotRequired[int]
    r"""Number of overlapping tokens between chunks"""
    min_sentences_per_chunk: NotRequired[int]
    r"""Minimum number of sentences per chunk"""


class SentenceChunkerStrategy(BaseModel):
    r"""Splits text at sentence boundaries while respecting token limits. Ideal for maintaining semantic coherence and readability."""

    text: str
    r"""The text content to be chunked"""

    strategy: SentenceChunker

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[ChunkingRequestReturnType] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 512
    r"""Maximum tokens per chunk"""

    chunk_overlap: Optional[int] = 0
    r"""Number of overlapping tokens between chunks"""

    min_sentences_per_chunk: Optional[int] = 1
    r"""Minimum number of sentences per chunk"""


ReturnType = Literal["chunks", "texts"]
r"""Return format: chunks (with metadata) or texts (plain strings)"""

TokenChunker = Literal["token"]


class TokenChunkerStrategyTypedDict(TypedDict):
    r"""Splits text based on token count. Best for ensuring chunks fit within LLM context windows and maintaining consistent chunk sizes for embedding models."""

    text: str
    r"""The text content to be chunked"""
    strategy: TokenChunker
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[ReturnType]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    chunk_overlap: NotRequired[int]
    r"""Number of tokens to overlap between chunks"""


class TokenChunkerStrategy(BaseModel):
    r"""Splits text based on token count. Best for ensuring chunks fit within LLM context windows and maintaining consistent chunk sizes for embedding models."""

    text: str
    r"""The text content to be chunked"""

    strategy: TokenChunker

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[ReturnType] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 512
    r"""Maximum tokens per chunk"""

    chunk_overlap: Optional[int] = 0
    r"""Number of tokens to overlap between chunks"""


ChunkTextChunkingRequestTypedDict = TypeAliasType(
    "ChunkTextChunkingRequestTypedDict",
    Union[
        TokenChunkerStrategyTypedDict,
        SentenceChunkerStrategyTypedDict,
        RecursiveChunkerStrategyTypedDict,
        AgenticChunkerStrategyTypedDict,
        SemanticChunkerStrategyTypedDict,
        SDPMChunkerStrategyTypedDict,
    ],
)
r"""Request payload for text chunking with strategy-specific configuration"""


ChunkTextChunkingRequest = TypeAliasType(
    "ChunkTextChunkingRequest",
    Union[
        TokenChunkerStrategy,
        SentenceChunkerStrategy,
        RecursiveChunkerStrategy,
        AgenticChunkerStrategy,
        SemanticChunkerStrategy,
        SDPMChunkerStrategy,
    ],
)
r"""Request payload for text chunking with strategy-specific configuration"""


class ChunkTextMetadataTypedDict(TypedDict):
    start_index: Nullable[float]
    end_index: Nullable[float]
    token_count: Nullable[float]


class ChunkTextMetadata(BaseModel):
    start_index: Nullable[float]

    end_index: Nullable[float]

    token_count: Nullable[float]

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["start_index", "end_index", "token_count"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ChunksTypedDict(TypedDict):
    text: str
    r"""The text content of the chunk"""
    index: float
    r"""The position index of this chunk in the sequence"""
    metadata: NotRequired[ChunkTextMetadataTypedDict]


class Chunks(BaseModel):
    text: str
    r"""The text content of the chunk"""

    index: float
    r"""The position index of this chunk in the sequence"""

    metadata: Optional[ChunkTextMetadata] = None


class ChunkTextResponseBodyTypedDict(TypedDict):
    r"""Text successfully chunked"""

    chunks: List[ChunksTypedDict]


class ChunkTextResponseBody(BaseModel):
    r"""Text successfully chunked"""

    chunks: List[Chunks]
