"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
import dateutil.parser
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import FieldMetadata, HeaderMetadata
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class CreatePromptSnippetGlobalsTypedDict(TypedDict):
    contact_id: NotRequired[str]


class CreatePromptSnippetGlobals(BaseModel):
    contact_id: Annotated[
        Optional[str],
        pydantic.Field(alias="contactId"),
        FieldMetadata(header=HeaderMetadata(style="simple", explode=False)),
    ] = None


ModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

Format = Literal["url", "b64_json", "text", "json_object"]
r"""Only supported on `image` models."""

Quality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

CreatePromptSnippetResponseFormatType = Literal["json_object"]


class ResponseFormat2TypedDict(TypedDict):
    type: CreatePromptSnippetResponseFormatType


class ResponseFormat2(BaseModel):
    type: CreatePromptSnippetResponseFormatType


ResponseFormatType = Literal["json_schema"]


class JSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class JSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class ResponseFormat1TypedDict(TypedDict):
    type: ResponseFormatType
    json_schema: JSONSchemaTypedDict


class ResponseFormat1(BaseModel):
    type: ResponseFormatType

    json_schema: JSONSchema


ResponseFormatTypedDict = TypeAliasType(
    "ResponseFormatTypedDict", Union[ResponseFormat2TypedDict, ResponseFormat1TypedDict]
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


ResponseFormat = TypeAliasType(
    "ResponseFormat", Union[ResponseFormat2, ResponseFormat1]
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


PhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

EncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""

ReasoningEffort = Literal["low", "medium", "high"]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class ModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[Format]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[Quality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[Nullable[ResponseFormatTypedDict]]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[PhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[EncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[ReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class ModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[Optional[Format], pydantic.Field(alias="format")] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[Quality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[ResponseFormat], pydantic.Field(alias="responseFormat")
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[PhotoRealVersion], pydantic.Field(alias="photoRealVersion")
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[EncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[ReasoningEffort], pydantic.Field(alias="reasoningEffort")
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


Provider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
]

CreatePromptSnippetRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

CreatePromptSnippet2PromptSnippetsType = Literal["image_url"]


class CreatePromptSnippet2ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class CreatePromptSnippet2ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class CreatePromptSnippet22TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: CreatePromptSnippet2PromptSnippetsType
    image_url: CreatePromptSnippet2ImageURLTypedDict


class CreatePromptSnippet22(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: CreatePromptSnippet2PromptSnippetsType

    image_url: CreatePromptSnippet2ImageURL


CreatePromptSnippet2Type = Literal["text"]


class CreatePromptSnippet21TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: CreatePromptSnippet2Type
    text: str


class CreatePromptSnippet21(BaseModel):
    r"""Text content part of a prompt message"""

    type: CreatePromptSnippet2Type

    text: str


CreatePromptSnippetContent2TypedDict = TypeAliasType(
    "CreatePromptSnippetContent2TypedDict",
    Union[CreatePromptSnippet21TypedDict, CreatePromptSnippet22TypedDict],
)


CreatePromptSnippetContent2 = TypeAliasType(
    "CreatePromptSnippetContent2", Union[CreatePromptSnippet21, CreatePromptSnippet22]
)


CreatePromptSnippetContentTypedDict = TypeAliasType(
    "CreatePromptSnippetContentTypedDict",
    Union[str, List[CreatePromptSnippetContent2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


CreatePromptSnippetContent = TypeAliasType(
    "CreatePromptSnippetContent", Union[str, List[CreatePromptSnippetContent2]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


CreatePromptSnippetType = Literal["function"]


class CreatePromptSnippetFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class CreatePromptSnippetFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class CreatePromptSnippetToolCallsTypedDict(TypedDict):
    type: CreatePromptSnippetType
    function: CreatePromptSnippetFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class CreatePromptSnippetToolCalls(BaseModel):
    type: CreatePromptSnippetType

    function: CreatePromptSnippetFunction

    id: Optional[str] = None

    index: Optional[float] = None


class CreatePromptSnippetMessagesTypedDict(TypedDict):
    role: CreatePromptSnippetRole
    r"""The role of the prompt message"""
    content: CreatePromptSnippetContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[CreatePromptSnippetToolCallsTypedDict]]


class CreatePromptSnippetMessages(BaseModel):
    role: CreatePromptSnippetRole
    r"""The role of the prompt message"""

    content: CreatePromptSnippetContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[CreatePromptSnippetToolCalls]] = None


class PromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[CreatePromptSnippetMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_type: NotRequired[ModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[ModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[Provider]
    version: NotRequired[str]


class PromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[CreatePromptSnippetMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_type: Optional[ModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[ModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[Provider] = None

    version: Optional[str] = None


UseCases = Literal[
    "Agents",
    "Agents simulations",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Documents QA",
    "Conversation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "SQL",
    "Summarization",
    "Tagging",
]

Language = Literal[
    "Chinese", "Dutch", "English", "French", "German", "Russian", "Spanish"
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptSnippetMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[UseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[Language]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptSnippetMetadata(BaseModel):
    use_cases: Optional[List[UseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: Optional[Language] = None
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptSnippetRequestBodyTypedDict(TypedDict):
    key: str
    prompt_config: PromptConfigTypedDict
    r"""A list of messages compatible with the openAI schema"""
    path: str
    r"""The path where the entity is stored in the project structure. The first element of the path always represents the project name. Any subsequent path element after the project will be created as a folder in the project if it does not exists."""
    description: NotRequired[Nullable[str]]
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""
    metadata: NotRequired[CreatePromptSnippetMetadataTypedDict]


class CreatePromptSnippetRequestBody(BaseModel):
    key: str

    prompt_config: PromptConfig
    r"""A list of messages compatible with the openAI schema"""

    path: str
    r"""The path where the entity is stored in the project structure. The first element of the path always represents the project name. Any subsequent path element after the project will be created as a folder in the project if it does not exists."""

    description: OptionalNullable[str] = UNSET
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    metadata: Optional[CreatePromptSnippetMetadata] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "metadata"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


Owner2 = Literal["vendor"]

OwnerTypedDict = TypeAliasType("OwnerTypedDict", Union[str, Owner2])


Owner = TypeAliasType("Owner", Union[str, Owner2])


CreatePromptSnippetModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

CreatePromptSnippetFormat = Literal["url", "b64_json", "text", "json_object"]
r"""Only supported on `image` models."""

CreatePromptSnippetQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

CreatePromptSnippetResponseFormatPromptSnippetsResponseType = Literal["json_object"]


class CreatePromptSnippetResponseFormat2TypedDict(TypedDict):
    type: CreatePromptSnippetResponseFormatPromptSnippetsResponseType


class CreatePromptSnippetResponseFormat2(BaseModel):
    type: CreatePromptSnippetResponseFormatPromptSnippetsResponseType


CreatePromptSnippetResponseFormatPromptSnippetsType = Literal["json_schema"]


class CreatePromptSnippetResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class CreatePromptSnippetResponseFormatJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class CreatePromptSnippetResponseFormat1TypedDict(TypedDict):
    type: CreatePromptSnippetResponseFormatPromptSnippetsType
    json_schema: CreatePromptSnippetResponseFormatJSONSchemaTypedDict


class CreatePromptSnippetResponseFormat1(BaseModel):
    type: CreatePromptSnippetResponseFormatPromptSnippetsType

    json_schema: CreatePromptSnippetResponseFormatJSONSchema


CreatePromptSnippetResponseFormatTypedDict = TypeAliasType(
    "CreatePromptSnippetResponseFormatTypedDict",
    Union[
        CreatePromptSnippetResponseFormat2TypedDict,
        CreatePromptSnippetResponseFormat1TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


CreatePromptSnippetResponseFormat = TypeAliasType(
    "CreatePromptSnippetResponseFormat",
    Union[CreatePromptSnippetResponseFormat2, CreatePromptSnippetResponseFormat1],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


CreatePromptSnippetPhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

CreatePromptSnippetEncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""

CreatePromptSnippetReasoningEffort = Literal["low", "medium", "high"]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class CreatePromptSnippetModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[CreatePromptSnippetFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[CreatePromptSnippetQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[Nullable[CreatePromptSnippetResponseFormatTypedDict]]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[CreatePromptSnippetPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[CreatePromptSnippetEncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[CreatePromptSnippetReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class CreatePromptSnippetModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[
        Optional[CreatePromptSnippetFormat], pydantic.Field(alias="format")
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[CreatePromptSnippetQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[CreatePromptSnippetResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[CreatePromptSnippetPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[CreatePromptSnippetEncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[CreatePromptSnippetReasoningEffort],
        pydantic.Field(alias="reasoningEffort"),
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreatePromptSnippetProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
]

CreatePromptSnippetPromptSnippetsRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

CreatePromptSnippet2PromptSnippetsResponse200Type = Literal["image_url"]


class CreatePromptSnippet2PromptSnippetsImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class CreatePromptSnippet2PromptSnippetsImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class CreatePromptSnippet2PromptSnippets2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: CreatePromptSnippet2PromptSnippetsResponse200Type
    image_url: CreatePromptSnippet2PromptSnippetsImageURLTypedDict


class CreatePromptSnippet2PromptSnippets2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: CreatePromptSnippet2PromptSnippetsResponse200Type

    image_url: CreatePromptSnippet2PromptSnippetsImageURL


CreatePromptSnippet2PromptSnippetsResponseType = Literal["text"]


class CreatePromptSnippet2PromptSnippets1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: CreatePromptSnippet2PromptSnippetsResponseType
    text: str


class CreatePromptSnippet2PromptSnippets1(BaseModel):
    r"""Text content part of a prompt message"""

    type: CreatePromptSnippet2PromptSnippetsResponseType

    text: str


CreatePromptSnippetContentPromptSnippets2TypedDict = TypeAliasType(
    "CreatePromptSnippetContentPromptSnippets2TypedDict",
    Union[
        CreatePromptSnippet2PromptSnippets1TypedDict,
        CreatePromptSnippet2PromptSnippets2TypedDict,
    ],
)


CreatePromptSnippetContentPromptSnippets2 = TypeAliasType(
    "CreatePromptSnippetContentPromptSnippets2",
    Union[CreatePromptSnippet2PromptSnippets1, CreatePromptSnippet2PromptSnippets2],
)


CreatePromptSnippetPromptSnippetsContentTypedDict = TypeAliasType(
    "CreatePromptSnippetPromptSnippetsContentTypedDict",
    Union[str, List[CreatePromptSnippetContentPromptSnippets2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


CreatePromptSnippetPromptSnippetsContent = TypeAliasType(
    "CreatePromptSnippetPromptSnippetsContent",
    Union[str, List[CreatePromptSnippetContentPromptSnippets2]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


CreatePromptSnippetPromptSnippetsResponseType = Literal["function"]


class CreatePromptSnippetPromptSnippetsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class CreatePromptSnippetPromptSnippetsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class CreatePromptSnippetPromptSnippetsToolCallsTypedDict(TypedDict):
    type: CreatePromptSnippetPromptSnippetsResponseType
    function: CreatePromptSnippetPromptSnippetsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class CreatePromptSnippetPromptSnippetsToolCalls(BaseModel):
    type: CreatePromptSnippetPromptSnippetsResponseType

    function: CreatePromptSnippetPromptSnippetsFunction

    id: Optional[str] = None

    index: Optional[float] = None


class CreatePromptSnippetPromptSnippetsMessagesTypedDict(TypedDict):
    role: CreatePromptSnippetPromptSnippetsRole
    r"""The role of the prompt message"""
    content: CreatePromptSnippetPromptSnippetsContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[CreatePromptSnippetPromptSnippetsToolCallsTypedDict]]


class CreatePromptSnippetPromptSnippetsMessages(BaseModel):
    role: CreatePromptSnippetPromptSnippetsRole
    r"""The role of the prompt message"""

    content: CreatePromptSnippetPromptSnippetsContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[CreatePromptSnippetPromptSnippetsToolCalls]] = None


class CreatePromptSnippetPromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[CreatePromptSnippetPromptSnippetsMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[CreatePromptSnippetModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[CreatePromptSnippetModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[CreatePromptSnippetProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class CreatePromptSnippetPromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[CreatePromptSnippetPromptSnippetsMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[CreatePromptSnippetModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[CreatePromptSnippetModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[CreatePromptSnippetProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreatePromptSnippetUseCases = Literal[
    "Agents",
    "Agents simulations",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Documents QA",
    "Conversation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "SQL",
    "Summarization",
    "Tagging",
]

CreatePromptSnippetLanguage = Literal[
    "Chinese", "Dutch", "English", "French", "German", "Russian", "Spanish"
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptSnippetPromptSnippetsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[CreatePromptSnippetUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[CreatePromptSnippetLanguage]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptSnippetPromptSnippetsMetadata(BaseModel):
    use_cases: Optional[List[CreatePromptSnippetUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: Optional[CreatePromptSnippetLanguage] = None
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


CreatePromptSnippetPromptSnippetsType = Literal["snippet"]

CreatePromptSnippetPromptSnippetsModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

CreatePromptSnippetPromptSnippetsFormat = Literal[
    "url", "b64_json", "text", "json_object"
]
r"""Only supported on `image` models."""

CreatePromptSnippetPromptSnippetsQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

CreatePromptSnippetResponseFormatPromptSnippetsResponse200ApplicationJSONType = Literal[
    "json_object"
]


class CreatePromptSnippetResponseFormatPromptSnippets2TypedDict(TypedDict):
    type: CreatePromptSnippetResponseFormatPromptSnippetsResponse200ApplicationJSONType


class CreatePromptSnippetResponseFormatPromptSnippets2(BaseModel):
    type: CreatePromptSnippetResponseFormatPromptSnippetsResponse200ApplicationJSONType


CreatePromptSnippetResponseFormatPromptSnippetsResponse200Type = Literal["json_schema"]


class CreatePromptSnippetResponseFormatPromptSnippetsJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class CreatePromptSnippetResponseFormatPromptSnippetsJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class CreatePromptSnippetResponseFormatPromptSnippets1TypedDict(TypedDict):
    type: CreatePromptSnippetResponseFormatPromptSnippetsResponse200Type
    json_schema: CreatePromptSnippetResponseFormatPromptSnippetsJSONSchemaTypedDict


class CreatePromptSnippetResponseFormatPromptSnippets1(BaseModel):
    type: CreatePromptSnippetResponseFormatPromptSnippetsResponse200Type

    json_schema: CreatePromptSnippetResponseFormatPromptSnippetsJSONSchema


CreatePromptSnippetPromptSnippetsResponseFormatTypedDict = TypeAliasType(
    "CreatePromptSnippetPromptSnippetsResponseFormatTypedDict",
    Union[
        CreatePromptSnippetResponseFormatPromptSnippets2TypedDict,
        CreatePromptSnippetResponseFormatPromptSnippets1TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


CreatePromptSnippetPromptSnippetsResponseFormat = TypeAliasType(
    "CreatePromptSnippetPromptSnippetsResponseFormat",
    Union[
        CreatePromptSnippetResponseFormatPromptSnippets2,
        CreatePromptSnippetResponseFormatPromptSnippets1,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


CreatePromptSnippetPromptSnippetsPhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

CreatePromptSnippetPromptSnippetsEncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""

CreatePromptSnippetPromptSnippetsReasoningEffort = Literal["low", "medium", "high"]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class CreatePromptSnippetPromptSnippetsModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[CreatePromptSnippetPromptSnippetsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[CreatePromptSnippetPromptSnippetsQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[CreatePromptSnippetPromptSnippetsResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[CreatePromptSnippetPromptSnippetsPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[CreatePromptSnippetPromptSnippetsEncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[CreatePromptSnippetPromptSnippetsReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class CreatePromptSnippetPromptSnippetsModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[
        Optional[CreatePromptSnippetPromptSnippetsFormat],
        pydantic.Field(alias="format"),
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[CreatePromptSnippetPromptSnippetsQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[CreatePromptSnippetPromptSnippetsResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[CreatePromptSnippetPromptSnippetsPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[CreatePromptSnippetPromptSnippetsEncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[CreatePromptSnippetPromptSnippetsReasoningEffort],
        pydantic.Field(alias="reasoningEffort"),
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreatePromptSnippetPromptSnippetsProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
]

CreatePromptSnippetPromptSnippetsResponseRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

CreatePromptSnippet2PromptSnippetsResponse200ApplicationJSONResponseBodyType = Literal[
    "image_url"
]


class CreatePromptSnippet2PromptSnippetsResponseImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class CreatePromptSnippet2PromptSnippetsResponseImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class CreatePromptSnippet2PromptSnippetsResponse2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: CreatePromptSnippet2PromptSnippetsResponse200ApplicationJSONResponseBodyType
    image_url: CreatePromptSnippet2PromptSnippetsResponseImageURLTypedDict


class CreatePromptSnippet2PromptSnippetsResponse2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: CreatePromptSnippet2PromptSnippetsResponse200ApplicationJSONResponseBodyType

    image_url: CreatePromptSnippet2PromptSnippetsResponseImageURL


CreatePromptSnippet2PromptSnippetsResponse200ApplicationJSONType = Literal["text"]


class CreatePromptSnippet2PromptSnippetsResponse1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: CreatePromptSnippet2PromptSnippetsResponse200ApplicationJSONType
    text: str


class CreatePromptSnippet2PromptSnippetsResponse1(BaseModel):
    r"""Text content part of a prompt message"""

    type: CreatePromptSnippet2PromptSnippetsResponse200ApplicationJSONType

    text: str


CreatePromptSnippetContentPromptSnippetsResponse2TypedDict = TypeAliasType(
    "CreatePromptSnippetContentPromptSnippetsResponse2TypedDict",
    Union[
        CreatePromptSnippet2PromptSnippetsResponse1TypedDict,
        CreatePromptSnippet2PromptSnippetsResponse2TypedDict,
    ],
)


CreatePromptSnippetContentPromptSnippetsResponse2 = TypeAliasType(
    "CreatePromptSnippetContentPromptSnippetsResponse2",
    Union[
        CreatePromptSnippet2PromptSnippetsResponse1,
        CreatePromptSnippet2PromptSnippetsResponse2,
    ],
)


CreatePromptSnippetPromptSnippetsResponseContentTypedDict = TypeAliasType(
    "CreatePromptSnippetPromptSnippetsResponseContentTypedDict",
    Union[str, List[CreatePromptSnippetContentPromptSnippetsResponse2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


CreatePromptSnippetPromptSnippetsResponseContent = TypeAliasType(
    "CreatePromptSnippetPromptSnippetsResponseContent",
    Union[str, List[CreatePromptSnippetContentPromptSnippetsResponse2]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


CreatePromptSnippetPromptSnippetsResponse200Type = Literal["function"]


class CreatePromptSnippetPromptSnippetsResponseFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class CreatePromptSnippetPromptSnippetsResponseFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class CreatePromptSnippetPromptSnippetsResponseToolCallsTypedDict(TypedDict):
    type: CreatePromptSnippetPromptSnippetsResponse200Type
    function: CreatePromptSnippetPromptSnippetsResponseFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class CreatePromptSnippetPromptSnippetsResponseToolCalls(BaseModel):
    type: CreatePromptSnippetPromptSnippetsResponse200Type

    function: CreatePromptSnippetPromptSnippetsResponseFunction

    id: Optional[str] = None

    index: Optional[float] = None


class CreatePromptSnippetPromptSnippetsResponseMessagesTypedDict(TypedDict):
    role: CreatePromptSnippetPromptSnippetsResponseRole
    r"""The role of the prompt message"""
    content: CreatePromptSnippetPromptSnippetsResponseContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[
        List[CreatePromptSnippetPromptSnippetsResponseToolCallsTypedDict]
    ]


class CreatePromptSnippetPromptSnippetsResponseMessages(BaseModel):
    role: CreatePromptSnippetPromptSnippetsResponseRole
    r"""The role of the prompt message"""

    content: CreatePromptSnippetPromptSnippetsResponseContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[CreatePromptSnippetPromptSnippetsResponseToolCalls]] = (
        None
    )


class CreatePromptSnippetPromptSnippetsPromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[CreatePromptSnippetPromptSnippetsResponseMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[CreatePromptSnippetPromptSnippetsModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[
        CreatePromptSnippetPromptSnippetsModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[CreatePromptSnippetPromptSnippetsProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class CreatePromptSnippetPromptSnippetsPromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[CreatePromptSnippetPromptSnippetsResponseMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[CreatePromptSnippetPromptSnippetsModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[CreatePromptSnippetPromptSnippetsModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[CreatePromptSnippetPromptSnippetsProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreatePromptSnippetPromptSnippetsUseCases = Literal[
    "Agents",
    "Agents simulations",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Documents QA",
    "Conversation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "SQL",
    "Summarization",
    "Tagging",
]

CreatePromptSnippetPromptSnippetsLanguage = Literal[
    "Chinese", "Dutch", "English", "French", "German", "Russian", "Spanish"
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptSnippetPromptSnippetsResponseMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[CreatePromptSnippetPromptSnippetsUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[CreatePromptSnippetPromptSnippetsLanguage]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptSnippetPromptSnippetsResponseMetadata(BaseModel):
    use_cases: Optional[List[CreatePromptSnippetPromptSnippetsUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: Optional[CreatePromptSnippetPromptSnippetsLanguage] = None
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class VersionsTypedDict(TypedDict):
    r"""Prompt version model returned from the API"""

    id: str
    prompt_config: CreatePromptSnippetPromptSnippetsPromptConfigTypedDict
    r"""A list of messages compatible with the openAI schema"""
    metadata: CreatePromptSnippetPromptSnippetsResponseMetadataTypedDict
    created_by_id: str
    updated_by_id: str
    timestamp: str
    description: NotRequired[Nullable[str]]
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""


class Versions(BaseModel):
    r"""Prompt version model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    prompt_config: CreatePromptSnippetPromptSnippetsPromptConfig
    r"""A list of messages compatible with the openAI schema"""

    metadata: CreatePromptSnippetPromptSnippetsResponseMetadata

    created_by_id: str

    updated_by_id: str

    timestamp: str

    description: OptionalNullable[str] = UNSET
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreatePromptSnippetResponseBodyTypedDict(TypedDict):
    r"""Prompt snippet model returned from the API"""

    id: str
    owner: OwnerTypedDict
    domain_id: str
    key: str
    prompt_config: CreatePromptSnippetPromptConfigTypedDict
    r"""A list of messages compatible with the openAI schema"""
    metadata: CreatePromptSnippetPromptSnippetsMetadataTypedDict
    created_by_id: str
    updated_by_id: str
    type: CreatePromptSnippetPromptSnippetsType
    versions: List[VersionsTypedDict]
    description: NotRequired[Nullable[str]]
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""
    created: NotRequired[datetime]
    r"""The date and time the resource was created"""
    updated: NotRequired[datetime]
    r"""The date and time the resource was last updated"""


class CreatePromptSnippetResponseBody(BaseModel):
    r"""Prompt snippet model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    owner: Owner

    domain_id: str

    key: str

    prompt_config: CreatePromptSnippetPromptConfig
    r"""A list of messages compatible with the openAI schema"""

    metadata: CreatePromptSnippetPromptSnippetsMetadata

    created_by_id: str

    updated_by_id: str

    type: CreatePromptSnippetPromptSnippetsType

    versions: List[Versions]

    description: OptionalNullable[str] = UNSET
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    created: Optional[datetime] = None
    r"""The date and time the resource was created"""

    updated: Optional[datetime] = dateutil.parser.isoparse("2025-02-05T19:04:45.364Z")
    r"""The date and time the resource was last updated"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "created", "updated"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
