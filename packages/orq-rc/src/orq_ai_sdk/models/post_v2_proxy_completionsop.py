"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .publiccontact import PublicContact, PublicContactTypedDict
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import eventstreaming
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


PostV2ProxyCompletionsStopTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""


PostV2ProxyCompletionsStop = TypeAliasType(
    "PostV2ProxyCompletionsStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""


class PostV2ProxyCompletionsRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class PostV2ProxyCompletionsRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""


class PostV2ProxyCompletionsFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class PostV2ProxyCompletionsFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


PostV2ProxyCompletionsVersion = Literal["latest"]
r"""Version of the prompt to use (currently only \"latest\" supported)"""


class PostV2ProxyCompletionsPromptTypedDict(TypedDict):
    r"""Prompt configuration for the request"""

    id: str
    r"""Unique identifier of the prompt to use"""
    version: PostV2ProxyCompletionsVersion
    r"""Version of the prompt to use (currently only \"latest\" supported)"""


class PostV2ProxyCompletionsPrompt(BaseModel):
    r"""Prompt configuration for the request"""

    id: str
    r"""Unique identifier of the prompt to use"""

    version: PostV2ProxyCompletionsVersion
    r"""Version of the prompt to use (currently only \"latest\" supported)"""


class PostV2ProxyCompletionsThreadTypedDict(TypedDict):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""
    tags: NotRequired[List[str]]
    r"""Optional tags to differentiate or categorize threads"""


class PostV2ProxyCompletionsThread(BaseModel):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""

    tags: Optional[List[str]] = None
    r"""Optional tags to differentiate or categorize threads"""


PostV2ProxyCompletionsType = Literal["exact_match"]


class PostV2ProxyCompletionsCacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: PostV2ProxyCompletionsType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class PostV2ProxyCompletionsCache(BaseModel):
    r"""Cache configuration for the request."""

    type: PostV2ProxyCompletionsType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


PostV2ProxyCompletionsSearchType = Literal[
    "vector_search", "keyword_search", "hybrid_search"
]
r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""


class PostV2ProxyCompletionsOrExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class PostV2ProxyCompletionsOrExists(BaseModel):
    r"""Exists"""

    exists: bool


PostV2ProxyCompletionsOrProxyNinTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsOrProxyNinTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletionsOrProxyNin = TypeAliasType(
    "PostV2ProxyCompletionsOrProxyNin", Union[str, float, bool]
)


class PostV2ProxyCompletionsOrNinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[PostV2ProxyCompletionsOrProxyNinTypedDict]


class PostV2ProxyCompletionsOrNin(BaseModel):
    r"""Not in"""

    nin: List[PostV2ProxyCompletionsOrProxyNin]


PostV2ProxyCompletionsOrProxyInTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsOrProxyInTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletionsOrProxyIn = TypeAliasType(
    "PostV2ProxyCompletionsOrProxyIn", Union[str, float, bool]
)


class PostV2ProxyCompletionsOrInTypedDict(TypedDict):
    r"""In"""

    in_: List[PostV2ProxyCompletionsOrProxyInTypedDict]


class PostV2ProxyCompletionsOrIn(BaseModel):
    r"""In"""

    in_: Annotated[List[PostV2ProxyCompletionsOrProxyIn], pydantic.Field(alias="in")]


class PostV2ProxyCompletionsOrLteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class PostV2ProxyCompletionsOrLte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class PostV2ProxyCompletionsOrLtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class PostV2ProxyCompletionsOrLt(BaseModel):
    r"""Less than"""

    lt: float


class PostV2ProxyCompletionsOrGteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class PostV2ProxyCompletionsOrGte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class PostV2ProxyCompletionsOr3TypedDict(TypedDict):
    gt: float


class PostV2ProxyCompletionsOr3(BaseModel):
    gt: float


PostV2ProxyCompletionsOrProxyNeTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsOrProxyNeTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletionsOrProxyNe = TypeAliasType(
    "PostV2ProxyCompletionsOrProxyNe", Union[str, float, bool]
)


class PostV2ProxyCompletionsOrNeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: PostV2ProxyCompletionsOrProxyNeTypedDict


class PostV2ProxyCompletionsOrNe(BaseModel):
    r"""Not equal to"""

    ne: PostV2ProxyCompletionsOrProxyNe


PostV2ProxyCompletionsOrProxyEqTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsOrProxyEqTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletionsOrProxyEq = TypeAliasType(
    "PostV2ProxyCompletionsOrProxyEq", Union[str, float, bool]
)


class PostV2ProxyCompletionsOrEqTypedDict(TypedDict):
    r"""Equal to"""

    eq: PostV2ProxyCompletionsOrProxyEqTypedDict


class PostV2ProxyCompletionsOrEq(BaseModel):
    r"""Equal to"""

    eq: PostV2ProxyCompletionsOrProxyEq


PostV2ProxyCompletionsFilterByProxyOrTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsFilterByProxyOrTypedDict",
    Union[
        PostV2ProxyCompletionsOrEqTypedDict,
        PostV2ProxyCompletionsOrNeTypedDict,
        PostV2ProxyCompletionsOr3TypedDict,
        PostV2ProxyCompletionsOrGteTypedDict,
        PostV2ProxyCompletionsOrLtTypedDict,
        PostV2ProxyCompletionsOrLteTypedDict,
        PostV2ProxyCompletionsOrInTypedDict,
        PostV2ProxyCompletionsOrNinTypedDict,
        PostV2ProxyCompletionsOrExistsTypedDict,
    ],
)


PostV2ProxyCompletionsFilterByProxyOr = TypeAliasType(
    "PostV2ProxyCompletionsFilterByProxyOr",
    Union[
        PostV2ProxyCompletionsOrEq,
        PostV2ProxyCompletionsOrNe,
        PostV2ProxyCompletionsOr3,
        PostV2ProxyCompletionsOrGte,
        PostV2ProxyCompletionsOrLt,
        PostV2ProxyCompletionsOrLte,
        PostV2ProxyCompletionsOrIn,
        PostV2ProxyCompletionsOrNin,
        PostV2ProxyCompletionsOrExists,
    ],
)


class PostV2ProxyCompletionsFilterByOrTypedDict(TypedDict):
    r"""Or"""

    or_: List[Dict[str, PostV2ProxyCompletionsFilterByProxyOrTypedDict]]


class PostV2ProxyCompletionsFilterByOr(BaseModel):
    r"""Or"""

    or_: Annotated[
        List[Dict[str, PostV2ProxyCompletionsFilterByProxyOr]],
        pydantic.Field(alias="or"),
    ]


class PostV2ProxyCompletionsAndExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class PostV2ProxyCompletionsAndExists(BaseModel):
    r"""Exists"""

    exists: bool


PostV2ProxyCompletionsAndProxyNinTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsAndProxyNinTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletionsAndProxyNin = TypeAliasType(
    "PostV2ProxyCompletionsAndProxyNin", Union[str, float, bool]
)


class PostV2ProxyCompletionsAndNinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[PostV2ProxyCompletionsAndProxyNinTypedDict]


class PostV2ProxyCompletionsAndNin(BaseModel):
    r"""Not in"""

    nin: List[PostV2ProxyCompletionsAndProxyNin]


PostV2ProxyCompletionsAndProxyInTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsAndProxyInTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletionsAndProxyIn = TypeAliasType(
    "PostV2ProxyCompletionsAndProxyIn", Union[str, float, bool]
)


class PostV2ProxyCompletionsAndInTypedDict(TypedDict):
    r"""In"""

    in_: List[PostV2ProxyCompletionsAndProxyInTypedDict]


class PostV2ProxyCompletionsAndIn(BaseModel):
    r"""In"""

    in_: Annotated[List[PostV2ProxyCompletionsAndProxyIn], pydantic.Field(alias="in")]


class PostV2ProxyCompletionsAndLteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class PostV2ProxyCompletionsAndLte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class PostV2ProxyCompletionsAndLtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class PostV2ProxyCompletionsAndLt(BaseModel):
    r"""Less than"""

    lt: float


class PostV2ProxyCompletionsAndGteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class PostV2ProxyCompletionsAndGte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class PostV2ProxyCompletionsAnd3TypedDict(TypedDict):
    gt: float


class PostV2ProxyCompletionsAnd3(BaseModel):
    gt: float


PostV2ProxyCompletionsAndProxyNeTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsAndProxyNeTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletionsAndProxyNe = TypeAliasType(
    "PostV2ProxyCompletionsAndProxyNe", Union[str, float, bool]
)


class PostV2ProxyCompletionsAndNeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: PostV2ProxyCompletionsAndProxyNeTypedDict


class PostV2ProxyCompletionsAndNe(BaseModel):
    r"""Not equal to"""

    ne: PostV2ProxyCompletionsAndProxyNe


PostV2ProxyCompletionsAndProxyEqTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsAndProxyEqTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletionsAndProxyEq = TypeAliasType(
    "PostV2ProxyCompletionsAndProxyEq", Union[str, float, bool]
)


class PostV2ProxyCompletionsAndEqTypedDict(TypedDict):
    r"""Equal to"""

    eq: PostV2ProxyCompletionsAndProxyEqTypedDict


class PostV2ProxyCompletionsAndEq(BaseModel):
    r"""Equal to"""

    eq: PostV2ProxyCompletionsAndProxyEq


PostV2ProxyCompletionsFilterByProxyAndTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsFilterByProxyAndTypedDict",
    Union[
        PostV2ProxyCompletionsAndEqTypedDict,
        PostV2ProxyCompletionsAndNeTypedDict,
        PostV2ProxyCompletionsAnd3TypedDict,
        PostV2ProxyCompletionsAndGteTypedDict,
        PostV2ProxyCompletionsAndLtTypedDict,
        PostV2ProxyCompletionsAndLteTypedDict,
        PostV2ProxyCompletionsAndInTypedDict,
        PostV2ProxyCompletionsAndNinTypedDict,
        PostV2ProxyCompletionsAndExistsTypedDict,
    ],
)


PostV2ProxyCompletionsFilterByProxyAnd = TypeAliasType(
    "PostV2ProxyCompletionsFilterByProxyAnd",
    Union[
        PostV2ProxyCompletionsAndEq,
        PostV2ProxyCompletionsAndNe,
        PostV2ProxyCompletionsAnd3,
        PostV2ProxyCompletionsAndGte,
        PostV2ProxyCompletionsAndLt,
        PostV2ProxyCompletionsAndLte,
        PostV2ProxyCompletionsAndIn,
        PostV2ProxyCompletionsAndNin,
        PostV2ProxyCompletionsAndExists,
    ],
)


class PostV2ProxyCompletionsFilterByAndTypedDict(TypedDict):
    r"""And"""

    and_: List[Dict[str, PostV2ProxyCompletionsFilterByProxyAndTypedDict]]


class PostV2ProxyCompletionsFilterByAnd(BaseModel):
    r"""And"""

    and_: Annotated[
        List[Dict[str, PostV2ProxyCompletionsFilterByProxyAnd]],
        pydantic.Field(alias="and"),
    ]


class PostV2ProxyCompletions1ExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class PostV2ProxyCompletions1Exists(BaseModel):
    r"""Exists"""

    exists: bool


PostV2ProxyCompletions1ProxyNinTypedDict = TypeAliasType(
    "PostV2ProxyCompletions1ProxyNinTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletions1ProxyNin = TypeAliasType(
    "PostV2ProxyCompletions1ProxyNin", Union[str, float, bool]
)


class PostV2ProxyCompletions1NinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[PostV2ProxyCompletions1ProxyNinTypedDict]


class PostV2ProxyCompletions1Nin(BaseModel):
    r"""Not in"""

    nin: List[PostV2ProxyCompletions1ProxyNin]


PostV2ProxyCompletions1ProxyInTypedDict = TypeAliasType(
    "PostV2ProxyCompletions1ProxyInTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletions1ProxyIn = TypeAliasType(
    "PostV2ProxyCompletions1ProxyIn", Union[str, float, bool]
)


class PostV2ProxyCompletions1InTypedDict(TypedDict):
    r"""In"""

    in_: List[PostV2ProxyCompletions1ProxyInTypedDict]


class PostV2ProxyCompletions1In(BaseModel):
    r"""In"""

    in_: Annotated[List[PostV2ProxyCompletions1ProxyIn], pydantic.Field(alias="in")]


class PostV2ProxyCompletions1LteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class PostV2ProxyCompletions1Lte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class PostV2ProxyCompletions1LtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class PostV2ProxyCompletions1Lt(BaseModel):
    r"""Less than"""

    lt: float


class PostV2ProxyCompletions1GteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class PostV2ProxyCompletions1Gte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class PostV2ProxyCompletions13TypedDict(TypedDict):
    gt: float


class PostV2ProxyCompletions13(BaseModel):
    gt: float


PostV2ProxyCompletions1ProxyNeTypedDict = TypeAliasType(
    "PostV2ProxyCompletions1ProxyNeTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletions1ProxyNe = TypeAliasType(
    "PostV2ProxyCompletions1ProxyNe", Union[str, float, bool]
)


class PostV2ProxyCompletions1NeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: PostV2ProxyCompletions1ProxyNeTypedDict


class PostV2ProxyCompletions1Ne(BaseModel):
    r"""Not equal to"""

    ne: PostV2ProxyCompletions1ProxyNe


PostV2ProxyCompletions1ProxyEqTypedDict = TypeAliasType(
    "PostV2ProxyCompletions1ProxyEqTypedDict", Union[str, float, bool]
)


PostV2ProxyCompletions1ProxyEq = TypeAliasType(
    "PostV2ProxyCompletions1ProxyEq", Union[str, float, bool]
)


class PostV2ProxyCompletions1EqTypedDict(TypedDict):
    r"""Equal to"""

    eq: PostV2ProxyCompletions1ProxyEqTypedDict


class PostV2ProxyCompletions1Eq(BaseModel):
    r"""Equal to"""

    eq: PostV2ProxyCompletions1ProxyEq


PostV2ProxyCompletionsFilterBy1TypedDict = TypeAliasType(
    "PostV2ProxyCompletionsFilterBy1TypedDict",
    Union[
        PostV2ProxyCompletions1EqTypedDict,
        PostV2ProxyCompletions1NeTypedDict,
        PostV2ProxyCompletions13TypedDict,
        PostV2ProxyCompletions1GteTypedDict,
        PostV2ProxyCompletions1LtTypedDict,
        PostV2ProxyCompletions1LteTypedDict,
        PostV2ProxyCompletions1InTypedDict,
        PostV2ProxyCompletions1NinTypedDict,
        PostV2ProxyCompletions1ExistsTypedDict,
    ],
)


PostV2ProxyCompletionsFilterBy1 = TypeAliasType(
    "PostV2ProxyCompletionsFilterBy1",
    Union[
        PostV2ProxyCompletions1Eq,
        PostV2ProxyCompletions1Ne,
        PostV2ProxyCompletions13,
        PostV2ProxyCompletions1Gte,
        PostV2ProxyCompletions1Lt,
        PostV2ProxyCompletions1Lte,
        PostV2ProxyCompletions1In,
        PostV2ProxyCompletions1Nin,
        PostV2ProxyCompletions1Exists,
    ],
)


PostV2ProxyCompletionsFilterByTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsFilterByTypedDict",
    Union[
        PostV2ProxyCompletionsFilterByAndTypedDict,
        PostV2ProxyCompletionsFilterByOrTypedDict,
        Dict[str, PostV2ProxyCompletionsFilterBy1TypedDict],
    ],
)
r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""


PostV2ProxyCompletionsFilterBy = TypeAliasType(
    "PostV2ProxyCompletionsFilterBy",
    Union[
        PostV2ProxyCompletionsFilterByAnd,
        PostV2ProxyCompletionsFilterByOr,
        Dict[str, PostV2ProxyCompletionsFilterBy1],
    ],
)
r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""


class PostV2ProxyCompletionsSearchOptionsTypedDict(TypedDict):
    r"""Additional search options"""

    include_vectors: NotRequired[bool]
    r"""Whether to include the vector in the chunk"""
    include_metadata: NotRequired[bool]
    r"""Whether to include the metadata in the chunk"""
    include_scores: NotRequired[bool]
    r"""Whether to include the scores in the chunk"""


class PostV2ProxyCompletionsSearchOptions(BaseModel):
    r"""Additional search options"""

    include_vectors: Optional[bool] = None
    r"""Whether to include the vector in the chunk"""

    include_metadata: Optional[bool] = None
    r"""Whether to include the metadata in the chunk"""

    include_scores: Optional[bool] = None
    r"""Whether to include the scores in the chunk"""


PostV2ProxyCompletionsProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
    "litellm",
    "openailike",
    "cerebras",
    "bytedance",
]

PostV2ProxyCompletionsModelType = Literal["rerank"]


class PostV2ProxyCompletionsModelParametersTypedDict(TypedDict):
    threshold: NotRequired[float]
    r"""The threshold value used to filter the rerank results, only documents with a relevance score greater than the threshold will be returned"""


class PostV2ProxyCompletionsModelParameters(BaseModel):
    threshold: Optional[float] = None
    r"""The threshold value used to filter the rerank results, only documents with a relevance score greater than the threshold will be returned"""


class PostV2ProxyCompletionsRerankConfigTypedDict(TypedDict):
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    enabled: NotRequired[bool]
    provider: NotRequired[PostV2ProxyCompletionsProvider]
    top_k: NotRequired[int]
    r"""The number of results to return by the reranking model"""
    model: NotRequired[str]
    r"""The name of the model to use"""
    model_db_id: NotRequired[str]
    r"""The ID of the model in the database"""
    model_type: NotRequired[PostV2ProxyCompletionsModelType]
    model_parameters: NotRequired[PostV2ProxyCompletionsModelParametersTypedDict]


class PostV2ProxyCompletionsRerankConfig(BaseModel):
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    enabled: Optional[bool] = None

    provider: Optional[PostV2ProxyCompletionsProvider] = None

    top_k: Optional[int] = None
    r"""The number of results to return by the reranking model"""

    model: Optional[str] = None
    r"""The name of the model to use"""

    model_db_id: Optional[str] = None
    r"""The ID of the model in the database"""

    model_type: Optional[PostV2ProxyCompletionsModelType] = None

    model_parameters: Optional[PostV2ProxyCompletionsModelParameters] = None


PostV2ProxyCompletionsProxyProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
    "litellm",
    "openailike",
    "cerebras",
    "bytedance",
]


class PostV2ProxyCompletionsAgenticRagConfigTypedDict(TypedDict):
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    model_db_id: str
    provider: PostV2ProxyCompletionsProxyProvider
    integration_id: NotRequired[Nullable[str]]


class PostV2ProxyCompletionsAgenticRagConfig(BaseModel):
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    model_db_id: str

    provider: PostV2ProxyCompletionsProxyProvider

    integration_id: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["integration_id"]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsKnowledgeBasesTypedDict(TypedDict):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""
    top_k: NotRequired[int]
    r"""The number of results to return. If not provided, will default to the knowledge base configured `top_k`."""
    threshold: NotRequired[float]
    r"""The threshold to apply to the search. If not provided, will default to the knowledge base configured `threshold`"""
    search_type: NotRequired[PostV2ProxyCompletionsSearchType]
    r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""
    filter_by: NotRequired[PostV2ProxyCompletionsFilterByTypedDict]
    r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""
    search_options: NotRequired[PostV2ProxyCompletionsSearchOptionsTypedDict]
    r"""Additional search options"""
    rerank_config: NotRequired[Nullable[PostV2ProxyCompletionsRerankConfigTypedDict]]
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""
    agentic_rag_config: NotRequired[PostV2ProxyCompletionsAgenticRagConfigTypedDict]
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""
    query: NotRequired[str]
    r"""The query to use to search the knowledge base. If not provided we will use the last user message from the messages of the requests"""


class PostV2ProxyCompletionsKnowledgeBases(BaseModel):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""

    top_k: Optional[int] = None
    r"""The number of results to return. If not provided, will default to the knowledge base configured `top_k`."""

    threshold: Optional[float] = None
    r"""The threshold to apply to the search. If not provided, will default to the knowledge base configured `threshold`"""

    search_type: Optional[PostV2ProxyCompletionsSearchType] = "hybrid_search"
    r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""

    filter_by: Optional[PostV2ProxyCompletionsFilterBy] = None
    r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""

    search_options: Optional[PostV2ProxyCompletionsSearchOptions] = None
    r"""Additional search options"""

    rerank_config: OptionalNullable[PostV2ProxyCompletionsRerankConfig] = UNSET
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    agentic_rag_config: Optional[PostV2ProxyCompletionsAgenticRagConfig] = None
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    query: Optional[str] = None
    r"""The query to use to search the knowledge base. If not provided we will use the last user message from the messages of the requests"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "top_k",
            "threshold",
            "search_type",
            "filter_by",
            "search_options",
            "rerank_config",
            "agentic_rag_config",
            "query",
        ]
        nullable_fields = ["rerank_config"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsOrqTypedDict(TypedDict):
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    retry: NotRequired[PostV2ProxyCompletionsRetryTypedDict]
    r"""Retry configuration for the request"""
    fallbacks: NotRequired[List[PostV2ProxyCompletionsFallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    prompt: NotRequired[PostV2ProxyCompletionsPromptTypedDict]
    r"""Prompt configuration for the request"""
    contact: NotRequired[PublicContactTypedDict]
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""
    thread: NotRequired[PostV2ProxyCompletionsThreadTypedDict]
    r"""Thread information to group related requests"""
    inputs: NotRequired[Dict[str, Any]]
    r"""Values to replace in the prompt messages using {{variableName}} syntax"""
    cache: NotRequired[PostV2ProxyCompletionsCacheTypedDict]
    r"""Cache configuration for the request."""
    knowledge_bases: NotRequired[List[PostV2ProxyCompletionsKnowledgeBasesTypedDict]]


class PostV2ProxyCompletionsOrq(BaseModel):
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    retry: Optional[PostV2ProxyCompletionsRetry] = None
    r"""Retry configuration for the request"""

    fallbacks: Optional[List[PostV2ProxyCompletionsFallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    prompt: Optional[PostV2ProxyCompletionsPrompt] = None
    r"""Prompt configuration for the request"""

    contact: Optional[PublicContact] = None
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""

    thread: Optional[PostV2ProxyCompletionsThread] = None
    r"""Thread information to group related requests"""

    inputs: Optional[Dict[str, Any]] = None
    r"""Values to replace in the prompt messages using {{variableName}} syntax"""

    cache: Optional[PostV2ProxyCompletionsCache] = None
    r"""Cache configuration for the request."""

    knowledge_bases: Optional[List[PostV2ProxyCompletionsKnowledgeBases]] = None


class PostV2ProxyCompletionsRequestBodyTypedDict(TypedDict):
    model: str
    r"""ID of the model to use"""
    prompt: str
    r"""The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays."""
    echo: NotRequired[Nullable[bool]]
    r"""Echo back the prompt in addition to the completion"""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[float]]
    r"""The maximum number of tokens that can be generated in the completion."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[PostV2ProxyCompletionsStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."""
    user: NotRequired[str]
    r"""A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse."""
    orq: NotRequired[PostV2ProxyCompletionsOrqTypedDict]
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""
    stream: NotRequired[bool]


class PostV2ProxyCompletionsRequestBody(BaseModel):
    model: str
    r"""ID of the model to use"""

    prompt: str
    r"""The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays."""

    echo: OptionalNullable[bool] = False
    r"""Echo back the prompt in addition to the completion"""

    frequency_penalty: OptionalNullable[float] = 0
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[float] = 16
    r"""The maximum number of tokens that can be generated in the completion."""

    presence_penalty: OptionalNullable[float] = 0
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[PostV2ProxyCompletionsStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""

    temperature: OptionalNullable[float] = 1
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = 1
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."""

    user: Optional[str] = None
    r"""A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse."""

    orq: Optional[PostV2ProxyCompletionsOrq] = None
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    stream: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "echo",
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "seed",
            "stop",
            "temperature",
            "top_p",
            "user",
            "orq",
            "stream",
        ]
        nullable_fields = [
            "echo",
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "seed",
            "stop",
            "temperature",
            "top_p",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


PostV2ProxyCompletionsProxyFinishReason = Literal[
    "stop", "length", "content_filter", "tool_calls"
]
r"""The reason the model stopped generating tokens."""


class PostV2ProxyCompletionsProxyChoicesTypedDict(TypedDict):
    finish_reason: PostV2ProxyCompletionsProxyFinishReason
    r"""The reason the model stopped generating tokens."""
    index: float
    r"""The index of the choice in the list of choices."""
    text: str


class PostV2ProxyCompletionsProxyChoices(BaseModel):
    finish_reason: PostV2ProxyCompletionsProxyFinishReason
    r"""The reason the model stopped generating tokens."""

    index: float
    r"""The index of the choice in the list of choices."""

    text: str


PostV2ProxyCompletionsCreatedTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsCreatedTypedDict", Union[str, float]
)
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


PostV2ProxyCompletionsCreated = TypeAliasType(
    "PostV2ProxyCompletionsCreated", Union[str, float]
)
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


class PostV2ProxyCompletionsProxyPromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[int]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio input tokens consumed by the request."""


class PostV2ProxyCompletionsProxyPromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[int] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio input tokens consumed by the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["cached_tokens", "audio_tokens"]
        nullable_fields = ["cached_tokens", "audio_tokens"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsProxyCompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]
    accepted_prediction_tokens: NotRequired[Nullable[float]]
    rejected_prediction_tokens: NotRequired[Nullable[float]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio output tokens produced by the response."""


class PostV2ProxyCompletionsProxyCompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    accepted_prediction_tokens: OptionalNullable[float] = UNSET

    rejected_prediction_tokens: OptionalNullable[float] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio output tokens produced by the response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        nullable_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsProxyUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""
    prompt_tokens_details: NotRequired[
        Nullable[PostV2ProxyCompletionsProxyPromptTokensDetailsTypedDict]
    ]
    completion_tokens_details: NotRequired[
        Nullable[PostV2ProxyCompletionsProxyCompletionTokensDetailsTypedDict]
    ]


class PostV2ProxyCompletionsProxyUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""

    prompt_tokens_details: OptionalNullable[
        PostV2ProxyCompletionsProxyPromptTokensDetails
    ] = UNSET

    completion_tokens_details: OptionalNullable[
        PostV2ProxyCompletionsProxyCompletionTokensDetails
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "completion_tokens",
            "prompt_tokens",
            "total_tokens",
            "prompt_tokens_details",
            "completion_tokens_details",
        ]
        nullable_fields = ["prompt_tokens_details", "completion_tokens_details"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsDataTypedDict(TypedDict):
    id: str
    r"""A unique identifier for the completion."""
    choices: List[PostV2ProxyCompletionsProxyChoicesTypedDict]
    r"""The list of completion choices the model generated for the input prompt."""
    model: str
    r"""The model used for the chat completion."""
    object: str
    r"""The object type"""
    created: NotRequired[PostV2ProxyCompletionsCreatedTypedDict]
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    system_fingerprint: NotRequired[str]
    r"""This fingerprint represents the backend configuration that the model runs with."""
    usage: NotRequired[PostV2ProxyCompletionsProxyUsageTypedDict]
    r"""Usage statistics for the completion request."""


class PostV2ProxyCompletionsData(BaseModel):
    id: str
    r"""A unique identifier for the completion."""

    choices: List[PostV2ProxyCompletionsProxyChoices]
    r"""The list of completion choices the model generated for the input prompt."""

    model: str
    r"""The model used for the chat completion."""

    object: str
    r"""The object type"""

    created: Optional[PostV2ProxyCompletionsCreated] = None
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    system_fingerprint: Optional[str] = None
    r"""This fingerprint represents the backend configuration that the model runs with."""

    usage: Optional[PostV2ProxyCompletionsProxyUsage] = None
    r"""Usage statistics for the completion request."""


class PostV2ProxyCompletionsProxyResponseBodyTypedDict(TypedDict):
    r"""Represents a completion response from the API."""

    data: NotRequired[PostV2ProxyCompletionsDataTypedDict]


class PostV2ProxyCompletionsProxyResponseBody(BaseModel):
    r"""Represents a completion response from the API."""

    data: Optional[PostV2ProxyCompletionsData] = None


PostV2ProxyCompletionsFinishReason = Literal[
    "stop", "length", "content_filter", "tool_calls"
]
r"""The reason the model stopped generating tokens."""


class PostV2ProxyCompletionsChoicesTypedDict(TypedDict):
    finish_reason: PostV2ProxyCompletionsFinishReason
    r"""The reason the model stopped generating tokens."""
    index: float
    r"""The index of the choice in the list of choices."""
    text: str


class PostV2ProxyCompletionsChoices(BaseModel):
    finish_reason: PostV2ProxyCompletionsFinishReason
    r"""The reason the model stopped generating tokens."""

    index: float
    r"""The index of the choice in the list of choices."""

    text: str


CreatedTypedDict = TypeAliasType("CreatedTypedDict", Union[str, float])
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


Created = TypeAliasType("Created", Union[str, float])
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


class PostV2ProxyCompletionsPromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[int]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio input tokens consumed by the request."""


class PostV2ProxyCompletionsPromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[int] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio input tokens consumed by the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["cached_tokens", "audio_tokens"]
        nullable_fields = ["cached_tokens", "audio_tokens"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsCompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]
    accepted_prediction_tokens: NotRequired[Nullable[float]]
    rejected_prediction_tokens: NotRequired[Nullable[float]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio output tokens produced by the response."""


class PostV2ProxyCompletionsCompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    accepted_prediction_tokens: OptionalNullable[float] = UNSET

    rejected_prediction_tokens: OptionalNullable[float] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio output tokens produced by the response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        nullable_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""
    prompt_tokens_details: NotRequired[
        Nullable[PostV2ProxyCompletionsPromptTokensDetailsTypedDict]
    ]
    completion_tokens_details: NotRequired[
        Nullable[PostV2ProxyCompletionsCompletionTokensDetailsTypedDict]
    ]


class PostV2ProxyCompletionsUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""

    prompt_tokens_details: OptionalNullable[
        PostV2ProxyCompletionsPromptTokensDetails
    ] = UNSET

    completion_tokens_details: OptionalNullable[
        PostV2ProxyCompletionsCompletionTokensDetails
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "completion_tokens",
            "prompt_tokens",
            "total_tokens",
            "prompt_tokens_details",
            "completion_tokens_details",
        ]
        nullable_fields = ["prompt_tokens_details", "completion_tokens_details"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsResponseBodyTypedDict(TypedDict):
    r"""Represents a completion response from the API."""

    id: str
    r"""A unique identifier for the completion."""
    choices: List[PostV2ProxyCompletionsChoicesTypedDict]
    r"""The list of completion choices the model generated for the input prompt."""
    model: str
    r"""The model used for the chat completion."""
    object: str
    r"""The object type"""
    created: NotRequired[CreatedTypedDict]
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    system_fingerprint: NotRequired[str]
    r"""This fingerprint represents the backend configuration that the model runs with."""
    usage: NotRequired[PostV2ProxyCompletionsUsageTypedDict]
    r"""Usage statistics for the completion request."""


class PostV2ProxyCompletionsResponseBody(BaseModel):
    r"""Represents a completion response from the API."""

    id: str
    r"""A unique identifier for the completion."""

    choices: List[PostV2ProxyCompletionsChoices]
    r"""The list of completion choices the model generated for the input prompt."""

    model: str
    r"""The model used for the chat completion."""

    object: str
    r"""The object type"""

    created: Optional[Created] = None
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    system_fingerprint: Optional[str] = None
    r"""This fingerprint represents the backend configuration that the model runs with."""

    usage: Optional[PostV2ProxyCompletionsUsage] = None
    r"""Usage statistics for the completion request."""


PostV2ProxyCompletionsResponseTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsResponseTypedDict",
    Union[
        PostV2ProxyCompletionsResponseBodyTypedDict,
        Union[
            eventstreaming.EventStream[
                PostV2ProxyCompletionsProxyResponseBodyTypedDict
            ],
            eventstreaming.EventStreamAsync[
                PostV2ProxyCompletionsProxyResponseBodyTypedDict
            ],
        ],
    ],
)


PostV2ProxyCompletionsResponse = TypeAliasType(
    "PostV2ProxyCompletionsResponse",
    Union[
        PostV2ProxyCompletionsResponseBody,
        Union[
            eventstreaming.EventStream[PostV2ProxyCompletionsProxyResponseBody],
            eventstreaming.EventStreamAsync[PostV2ProxyCompletionsProxyResponseBody],
        ],
    ],
)
