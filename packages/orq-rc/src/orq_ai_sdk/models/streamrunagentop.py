"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from dataclasses import dataclass, field
import httpx
from orq_ai_sdk.models import OrqError
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
import pydantic
from pydantic import ConfigDict, model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


StreamRunAgentModelConfigurationVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


StreamRunAgentModelConfigurationFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class StreamRunAgentModelConfigurationAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: StreamRunAgentModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: StreamRunAgentModelConfigurationFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class StreamRunAgentModelConfigurationAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: StreamRunAgentModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[
        StreamRunAgentModelConfigurationFormat, pydantic.Field(alias="format")
    ]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


StreamRunAgentResponseFormatAgentsRequestType = Literal["json_schema",]


class StreamRunAgentResponseFormatAgentsJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[Nullable[bool]]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class StreamRunAgentResponseFormatAgentsJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: OptionalNullable[bool] = UNSET
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "schema", "strict"]
        nullable_fields = ["strict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class StreamRunAgentResponseFormatJSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: StreamRunAgentResponseFormatAgentsRequestType
    json_schema: StreamRunAgentResponseFormatAgentsJSONSchemaTypedDict


class StreamRunAgentResponseFormatJSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: StreamRunAgentResponseFormatAgentsRequestType

    json_schema: StreamRunAgentResponseFormatAgentsJSONSchema


StreamRunAgentResponseFormatAgentsType = Literal["json_object",]


class StreamRunAgentResponseFormatJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: StreamRunAgentResponseFormatAgentsType


class StreamRunAgentResponseFormatJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: StreamRunAgentResponseFormatAgentsType


StreamRunAgentResponseFormatType = Literal["text",]


class StreamRunAgentResponseFormatTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: StreamRunAgentResponseFormatType


class StreamRunAgentResponseFormatText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: StreamRunAgentResponseFormatType


StreamRunAgentModelConfigurationResponseFormatTypedDict = TypeAliasType(
    "StreamRunAgentModelConfigurationResponseFormatTypedDict",
    Union[
        StreamRunAgentResponseFormatTextTypedDict,
        StreamRunAgentResponseFormatJSONObjectTypedDict,
        StreamRunAgentResponseFormatJSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


StreamRunAgentModelConfigurationResponseFormat = TypeAliasType(
    "StreamRunAgentModelConfigurationResponseFormat",
    Union[
        StreamRunAgentResponseFormatText,
        StreamRunAgentResponseFormatJSONObject,
        StreamRunAgentResponseFormatJSONSchema,
    ],
)
r"""An object specifying the format that the model must output"""


StreamRunAgentModelConfigurationStopTypedDict = TypeAliasType(
    "StreamRunAgentModelConfigurationStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


StreamRunAgentModelConfigurationStop = TypeAliasType(
    "StreamRunAgentModelConfigurationStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class StreamRunAgentModelConfigurationStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class StreamRunAgentModelConfigurationStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


StreamRunAgentModelConfigurationType = Literal[
    "enabled",
    "disabled",
]
r"""Enables or disables the thinking mode capability"""


StreamRunAgentModelConfigurationThinkingLevel = Literal[
    "low",
    "high",
]
r"""The level of reasoning the model should use. This setting is supported only by `gemini-3` models. If budget_tokens is specified and `thinking_level` is available, `budget_tokens` will be ignored."""


class StreamRunAgentModelConfigurationThinkingTypedDict(TypedDict):
    type: StreamRunAgentModelConfigurationType
    r"""Enables or disables the thinking mode capability"""
    budget_tokens: float
    r"""Determines how many tokens the model can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than `max_tokens`."""
    thinking_level: NotRequired[StreamRunAgentModelConfigurationThinkingLevel]
    r"""The level of reasoning the model should use. This setting is supported only by `gemini-3` models. If budget_tokens is specified and `thinking_level` is available, `budget_tokens` will be ignored."""


class StreamRunAgentModelConfigurationThinking(BaseModel):
    type: StreamRunAgentModelConfigurationType
    r"""Enables or disables the thinking mode capability"""

    budget_tokens: float
    r"""Determines how many tokens the model can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than `max_tokens`."""

    thinking_level: Optional[StreamRunAgentModelConfigurationThinkingLevel] = None
    r"""The level of reasoning the model should use. This setting is supported only by `gemini-3` models. If budget_tokens is specified and `thinking_level` is available, `budget_tokens` will be ignored."""


StreamRunAgentToolChoiceType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class StreamRunAgentToolChoiceFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""


class StreamRunAgentToolChoiceFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""


class StreamRunAgentToolChoice2TypedDict(TypedDict):
    function: StreamRunAgentToolChoiceFunctionTypedDict
    type: NotRequired[StreamRunAgentToolChoiceType]
    r"""The type of the tool. Currently, only function is supported."""


class StreamRunAgentToolChoice2(BaseModel):
    function: StreamRunAgentToolChoiceFunction

    type: Optional[StreamRunAgentToolChoiceType] = None
    r"""The type of the tool. Currently, only function is supported."""


StreamRunAgentToolChoice1 = Literal[
    "none",
    "auto",
    "required",
]


StreamRunAgentModelConfigurationToolChoiceTypedDict = TypeAliasType(
    "StreamRunAgentModelConfigurationToolChoiceTypedDict",
    Union[StreamRunAgentToolChoice2TypedDict, StreamRunAgentToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


StreamRunAgentModelConfigurationToolChoice = TypeAliasType(
    "StreamRunAgentModelConfigurationToolChoice",
    Union[StreamRunAgentToolChoice2, StreamRunAgentToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


StreamRunAgentModelConfigurationModalities = Literal[
    "text",
    "audio",
]


class StreamRunAgentModelConfigurationParametersTypedDict(TypedDict):
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""

    audio: NotRequired[Nullable[StreamRunAgentModelConfigurationAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[
        StreamRunAgentModelConfigurationResponseFormatTypedDict
    ]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[str]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[StreamRunAgentModelConfigurationStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[
        Nullable[StreamRunAgentModelConfigurationStreamOptionsTypedDict]
    ]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[StreamRunAgentModelConfigurationThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[StreamRunAgentModelConfigurationToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[Nullable[List[StreamRunAgentModelConfigurationModalities]]]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""


class StreamRunAgentModelConfigurationParameters(BaseModel):
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""

    audio: OptionalNullable[StreamRunAgentModelConfigurationAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[StreamRunAgentModelConfigurationResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[str] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[StreamRunAgentModelConfigurationStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[StreamRunAgentModelConfigurationStreamOptions] = (
        UNSET
    )
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[StreamRunAgentModelConfigurationThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[StreamRunAgentModelConfigurationToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[StreamRunAgentModelConfigurationModalities]] = (
        UNSET
    )
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "audio",
            "frequency_penalty",
            "max_tokens",
            "max_completion_tokens",
            "logprobs",
            "top_logprobs",
            "n",
            "presence_penalty",
            "response_format",
            "reasoning_effort",
            "verbosity",
            "seed",
            "stop",
            "stream_options",
            "thinking",
            "temperature",
            "top_p",
            "top_k",
            "tool_choice",
            "parallel_tool_calls",
            "modalities",
        ]
        nullable_fields = [
            "audio",
            "frequency_penalty",
            "max_tokens",
            "max_completion_tokens",
            "logprobs",
            "top_logprobs",
            "n",
            "presence_penalty",
            "seed",
            "stop",
            "stream_options",
            "temperature",
            "top_p",
            "top_k",
            "modalities",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class StreamRunAgentModelConfigurationRetryTypedDict(TypedDict):
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class StreamRunAgentModelConfigurationRetry(BaseModel):
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""


class StreamRunAgentModelConfiguration2TypedDict(TypedDict):
    r"""

    Model configuration with parameters and retry settings.
    """

    id: str
    r"""A model ID string (e.g., `openai/gpt-4o` or `anthropic/claude-haiku-4-5-20251001`). Only models that support tool calling can be used with agents."""
    parameters: NotRequired[StreamRunAgentModelConfigurationParametersTypedDict]
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""
    retry: NotRequired[StreamRunAgentModelConfigurationRetryTypedDict]
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""


class StreamRunAgentModelConfiguration2(BaseModel):
    r"""

    Model configuration with parameters and retry settings.
    """

    id: str
    r"""A model ID string (e.g., `openai/gpt-4o` or `anthropic/claude-haiku-4-5-20251001`). Only models that support tool calling can be used with agents."""

    parameters: Optional[StreamRunAgentModelConfigurationParameters] = None
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""

    retry: Optional[StreamRunAgentModelConfigurationRetry] = None
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""


StreamRunAgentModelConfigurationTypedDict = TypeAliasType(
    "StreamRunAgentModelConfigurationTypedDict",
    Union[StreamRunAgentModelConfiguration2TypedDict, str],
)
r"""Model configuration for this execution. Can override the agent manifest defaults if the agent already exists."""


StreamRunAgentModelConfiguration = TypeAliasType(
    "StreamRunAgentModelConfiguration", Union[StreamRunAgentModelConfiguration2, str]
)
r"""Model configuration for this execution. Can override the agent manifest defaults if the agent already exists."""


StreamRunAgentFallbackModelConfigurationVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


StreamRunAgentFallbackModelConfigurationFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class StreamRunAgentFallbackModelConfigurationAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: StreamRunAgentFallbackModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: StreamRunAgentFallbackModelConfigurationFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class StreamRunAgentFallbackModelConfigurationAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: StreamRunAgentFallbackModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[
        StreamRunAgentFallbackModelConfigurationFormat, pydantic.Field(alias="format")
    ]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsFallbackModelConfigurationType = Literal[
    "json_schema",
]


class StreamRunAgentResponseFormatAgentsRequestRequestBodyJSONSchemaTypedDict(
    TypedDict
):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[Nullable[bool]]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class StreamRunAgentResponseFormatAgentsRequestRequestBodyJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: OptionalNullable[bool] = UNSET
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "schema", "strict"]
        nullable_fields = ["strict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class StreamRunAgentResponseFormatAgentsRequestJSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsFallbackModelConfigurationType
    json_schema: StreamRunAgentResponseFormatAgentsRequestRequestBodyJSONSchemaTypedDict


class StreamRunAgentResponseFormatAgentsRequestJSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsFallbackModelConfigurationType

    json_schema: StreamRunAgentResponseFormatAgentsRequestRequestBodyJSONSchema


StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsType = Literal[
    "json_object",
]


class StreamRunAgentResponseFormatAgentsJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsType


class StreamRunAgentResponseFormatAgentsJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsType


StreamRunAgentResponseFormatAgentsRequestRequestBodyType = Literal["text",]


class StreamRunAgentResponseFormatAgentsTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyType


class StreamRunAgentResponseFormatAgentsText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyType


StreamRunAgentFallbackModelConfigurationResponseFormatTypedDict = TypeAliasType(
    "StreamRunAgentFallbackModelConfigurationResponseFormatTypedDict",
    Union[
        StreamRunAgentResponseFormatAgentsTextTypedDict,
        StreamRunAgentResponseFormatAgentsJSONObjectTypedDict,
        StreamRunAgentResponseFormatAgentsRequestJSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


StreamRunAgentFallbackModelConfigurationResponseFormat = TypeAliasType(
    "StreamRunAgentFallbackModelConfigurationResponseFormat",
    Union[
        StreamRunAgentResponseFormatAgentsText,
        StreamRunAgentResponseFormatAgentsJSONObject,
        StreamRunAgentResponseFormatAgentsRequestJSONSchema,
    ],
)
r"""An object specifying the format that the model must output"""


StreamRunAgentFallbackModelConfigurationStopTypedDict = TypeAliasType(
    "StreamRunAgentFallbackModelConfigurationStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


StreamRunAgentFallbackModelConfigurationStop = TypeAliasType(
    "StreamRunAgentFallbackModelConfigurationStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class StreamRunAgentFallbackModelConfigurationStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class StreamRunAgentFallbackModelConfigurationStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


StreamRunAgentFallbackModelConfigurationType = Literal[
    "enabled",
    "disabled",
]
r"""Enables or disables the thinking mode capability"""


StreamRunAgentFallbackModelConfigurationThinkingLevel = Literal[
    "low",
    "high",
]
r"""The level of reasoning the model should use. This setting is supported only by `gemini-3` models. If budget_tokens is specified and `thinking_level` is available, `budget_tokens` will be ignored."""


class StreamRunAgentFallbackModelConfigurationThinkingTypedDict(TypedDict):
    type: StreamRunAgentFallbackModelConfigurationType
    r"""Enables or disables the thinking mode capability"""
    budget_tokens: float
    r"""Determines how many tokens the model can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than `max_tokens`."""
    thinking_level: NotRequired[StreamRunAgentFallbackModelConfigurationThinkingLevel]
    r"""The level of reasoning the model should use. This setting is supported only by `gemini-3` models. If budget_tokens is specified and `thinking_level` is available, `budget_tokens` will be ignored."""


class StreamRunAgentFallbackModelConfigurationThinking(BaseModel):
    type: StreamRunAgentFallbackModelConfigurationType
    r"""Enables or disables the thinking mode capability"""

    budget_tokens: float
    r"""Determines how many tokens the model can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than `max_tokens`."""

    thinking_level: Optional[StreamRunAgentFallbackModelConfigurationThinkingLevel] = (
        None
    )
    r"""The level of reasoning the model should use. This setting is supported only by `gemini-3` models. If budget_tokens is specified and `thinking_level` is available, `budget_tokens` will be ignored."""


StreamRunAgentToolChoiceAgentsType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class StreamRunAgentToolChoiceAgentsFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""


class StreamRunAgentToolChoiceAgentsFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""


class StreamRunAgentToolChoiceAgents2TypedDict(TypedDict):
    function: StreamRunAgentToolChoiceAgentsFunctionTypedDict
    type: NotRequired[StreamRunAgentToolChoiceAgentsType]
    r"""The type of the tool. Currently, only function is supported."""


class StreamRunAgentToolChoiceAgents2(BaseModel):
    function: StreamRunAgentToolChoiceAgentsFunction

    type: Optional[StreamRunAgentToolChoiceAgentsType] = None
    r"""The type of the tool. Currently, only function is supported."""


StreamRunAgentToolChoiceAgents1 = Literal[
    "none",
    "auto",
    "required",
]


StreamRunAgentFallbackModelConfigurationToolChoiceTypedDict = TypeAliasType(
    "StreamRunAgentFallbackModelConfigurationToolChoiceTypedDict",
    Union[StreamRunAgentToolChoiceAgents2TypedDict, StreamRunAgentToolChoiceAgents1],
)
r"""Controls which (if any) tool is called by the model."""


StreamRunAgentFallbackModelConfigurationToolChoice = TypeAliasType(
    "StreamRunAgentFallbackModelConfigurationToolChoice",
    Union[StreamRunAgentToolChoiceAgents2, StreamRunAgentToolChoiceAgents1],
)
r"""Controls which (if any) tool is called by the model."""


StreamRunAgentFallbackModelConfigurationModalities = Literal[
    "text",
    "audio",
]


class StreamRunAgentFallbackModelConfigurationParametersTypedDict(TypedDict):
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    audio: NotRequired[Nullable[StreamRunAgentFallbackModelConfigurationAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[
        StreamRunAgentFallbackModelConfigurationResponseFormatTypedDict
    ]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[str]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[StreamRunAgentFallbackModelConfigurationStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[
        Nullable[StreamRunAgentFallbackModelConfigurationStreamOptionsTypedDict]
    ]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[StreamRunAgentFallbackModelConfigurationThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[
        StreamRunAgentFallbackModelConfigurationToolChoiceTypedDict
    ]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[
        Nullable[List[StreamRunAgentFallbackModelConfigurationModalities]]
    ]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""


class StreamRunAgentFallbackModelConfigurationParameters(BaseModel):
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    audio: OptionalNullable[StreamRunAgentFallbackModelConfigurationAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[
        StreamRunAgentFallbackModelConfigurationResponseFormat
    ] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[str] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[StreamRunAgentFallbackModelConfigurationStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[
        StreamRunAgentFallbackModelConfigurationStreamOptions
    ] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[StreamRunAgentFallbackModelConfigurationThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[StreamRunAgentFallbackModelConfigurationToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[
        List[StreamRunAgentFallbackModelConfigurationModalities]
    ] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "audio",
            "frequency_penalty",
            "max_tokens",
            "max_completion_tokens",
            "logprobs",
            "top_logprobs",
            "n",
            "presence_penalty",
            "response_format",
            "reasoning_effort",
            "verbosity",
            "seed",
            "stop",
            "stream_options",
            "thinking",
            "temperature",
            "top_p",
            "top_k",
            "tool_choice",
            "parallel_tool_calls",
            "modalities",
        ]
        nullable_fields = [
            "audio",
            "frequency_penalty",
            "max_tokens",
            "max_completion_tokens",
            "logprobs",
            "top_logprobs",
            "n",
            "presence_penalty",
            "seed",
            "stop",
            "stream_options",
            "temperature",
            "top_p",
            "top_k",
            "modalities",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class StreamRunAgentFallbackModelConfiguration2TypedDict(TypedDict):
    r"""Fallback model configuration with optional parameters."""

    id: str
    r"""A fallback model ID string. Must support tool calling."""
    parameters: NotRequired[StreamRunAgentFallbackModelConfigurationParametersTypedDict]
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""


class StreamRunAgentFallbackModelConfiguration2(BaseModel):
    r"""Fallback model configuration with optional parameters."""

    id: str
    r"""A fallback model ID string. Must support tool calling."""

    parameters: Optional[StreamRunAgentFallbackModelConfigurationParameters] = None
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""


StreamRunAgentFallbackModelConfigurationTypedDict = TypeAliasType(
    "StreamRunAgentFallbackModelConfigurationTypedDict",
    Union[StreamRunAgentFallbackModelConfiguration2TypedDict, str],
)
r"""Fallback model for automatic failover when primary model request fails. Supports optional parameter overrides. Can be a simple model ID string or a configuration object with model-specific parameters. Fallbacks are tried in order."""


StreamRunAgentFallbackModelConfiguration = TypeAliasType(
    "StreamRunAgentFallbackModelConfiguration",
    Union[StreamRunAgentFallbackModelConfiguration2, str],
)
r"""Fallback model for automatic failover when primary model request fails. Supports optional parameter overrides. Can be a simple model ID string or a configuration object with model-specific parameters. Fallbacks are tried in order."""


StreamRunAgentRoleToolMessage = Literal["tool",]
r"""Message containing tool execution results"""


StreamRunAgentRoleUserMessage = Literal["user",]
r"""Message from the end user"""


StreamRunAgentRoleTypedDict = TypeAliasType(
    "StreamRunAgentRoleTypedDict",
    Union[StreamRunAgentRoleUserMessage, StreamRunAgentRoleToolMessage],
)
r"""Message role (user or tool for continuing executions)"""


StreamRunAgentRole = TypeAliasType(
    "StreamRunAgentRole",
    Union[StreamRunAgentRoleUserMessage, StreamRunAgentRoleToolMessage],
)
r"""Message role (user or tool for continuing executions)"""


StreamRunAgentPublicMessagePartAgentsRequestKind = Literal["tool_result",]


class StreamRunAgentPublicMessagePartToolResultPartTypedDict(TypedDict):
    r"""Tool execution result part. Use this ONLY when providing results for a pending tool call from the agent. The tool_call_id must match the ID from the agent's tool call request."""

    kind: StreamRunAgentPublicMessagePartAgentsRequestKind
    tool_call_id: str
    result: NotRequired[Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPublicMessagePartToolResultPart(BaseModel):
    r"""Tool execution result part. Use this ONLY when providing results for a pending tool call from the agent. The tool_call_id must match the ID from the agent's tool call request."""

    kind: StreamRunAgentPublicMessagePartAgentsRequestKind

    tool_call_id: str

    result: Optional[Any] = None

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPublicMessagePartAgentsKind = Literal["file",]


class StreamRunAgentFileFileInURIFormatTypedDict(TypedDict):
    r"""File in URI format. Check in the model's documentation for the supported mime types for the URI format"""

    uri: str
    r"""URL for the File content"""
    mime_type: NotRequired[str]
    r"""Optional mimeType for the file"""
    name: NotRequired[str]
    r"""Optional name for the file"""


class StreamRunAgentFileFileInURIFormat(BaseModel):
    r"""File in URI format. Check in the model's documentation for the supported mime types for the URI format"""

    uri: str
    r"""URL for the File content"""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""Optional mimeType for the file"""

    name: Optional[str] = None
    r"""Optional name for the file"""


class StreamRunAgentFileBinaryFormatTypedDict(TypedDict):
    r"""Binary in base64 format. Check in the model's documentation for the supported mime types for the binary format."""

    bytes_: str
    r"""base64 encoded content of the file"""
    mime_type: NotRequired[str]
    r"""Optional mimeType for the file"""
    name: NotRequired[str]
    r"""Optional name for the file"""


class StreamRunAgentFileBinaryFormat(BaseModel):
    r"""Binary in base64 format. Check in the model's documentation for the supported mime types for the binary format."""

    bytes_: Annotated[str, pydantic.Field(alias="bytes")]
    r"""base64 encoded content of the file"""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""Optional mimeType for the file"""

    name: Optional[str] = None
    r"""Optional name for the file"""


StreamRunAgentPublicMessagePartFileTypedDict = TypeAliasType(
    "StreamRunAgentPublicMessagePartFileTypedDict",
    Union[
        StreamRunAgentFileBinaryFormatTypedDict,
        StreamRunAgentFileFileInURIFormatTypedDict,
    ],
)


StreamRunAgentPublicMessagePartFile = TypeAliasType(
    "StreamRunAgentPublicMessagePartFile",
    Union[StreamRunAgentFileBinaryFormat, StreamRunAgentFileFileInURIFormat],
)


class StreamRunAgentPublicMessagePartFilePartTypedDict(TypedDict):
    r"""File attachment part. Use this to send files (images, documents, etc.) to the agent for processing."""

    kind: StreamRunAgentPublicMessagePartAgentsKind
    file: StreamRunAgentPublicMessagePartFileTypedDict
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPublicMessagePartFilePart(BaseModel):
    r"""File attachment part. Use this to send files (images, documents, etc.) to the agent for processing."""

    kind: StreamRunAgentPublicMessagePartAgentsKind

    file: StreamRunAgentPublicMessagePartFile

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPublicMessagePartKind = Literal["text",]


class StreamRunAgentPublicMessagePartTextPartTypedDict(TypedDict):
    r"""Text content part. Use this to send text messages to the agent."""

    kind: StreamRunAgentPublicMessagePartKind
    text: str


class StreamRunAgentPublicMessagePartTextPart(BaseModel):
    r"""Text content part. Use this to send text messages to the agent."""

    kind: StreamRunAgentPublicMessagePartKind

    text: str


StreamRunAgentPublicMessagePartTypedDict = TypeAliasType(
    "StreamRunAgentPublicMessagePartTypedDict",
    Union[
        StreamRunAgentPublicMessagePartTextPartTypedDict,
        StreamRunAgentPublicMessagePartFilePartTypedDict,
        StreamRunAgentPublicMessagePartToolResultPartTypedDict,
    ],
)
r"""Message part that can be provided by users. Use \"text\" for regular messages, \"file\" for attachments, or \"tool_result\" when responding to tool call requests."""


StreamRunAgentPublicMessagePart = TypeAliasType(
    "StreamRunAgentPublicMessagePart",
    Union[
        StreamRunAgentPublicMessagePartTextPart,
        StreamRunAgentPublicMessagePartFilePart,
        StreamRunAgentPublicMessagePartToolResultPart,
    ],
)
r"""Message part that can be provided by users. Use \"text\" for regular messages, \"file\" for attachments, or \"tool_result\" when responding to tool call requests."""


class StreamRunAgentA2AMessageTypedDict(TypedDict):
    r"""The A2A format message containing the task for the agent to perform."""

    role: StreamRunAgentRoleTypedDict
    r"""Message role (user or tool for continuing executions)"""
    parts: List[StreamRunAgentPublicMessagePartTypedDict]
    r"""A2A message parts (text, file, or tool_result only)"""
    message_id: NotRequired[str]
    r"""Optional A2A message ID in ULID format"""


class StreamRunAgentA2AMessage(BaseModel):
    r"""The A2A format message containing the task for the agent to perform."""

    role: StreamRunAgentRole
    r"""Message role (user or tool for continuing executions)"""

    parts: List[StreamRunAgentPublicMessagePart]
    r"""A2A message parts (text, file, or tool_result only)"""

    message_id: Annotated[Optional[str], pydantic.Field(alias="messageId")] = None
    r"""Optional A2A message ID in ULID format"""


class StreamRunAgentContactTypedDict(TypedDict):
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""

    id: str
    r"""Unique identifier for the contact"""
    display_name: NotRequired[str]
    r"""Display name of the contact"""
    email: NotRequired[str]
    r"""Email address of the contact"""
    metadata: NotRequired[List[Dict[str, Any]]]
    r"""A hash of key/value pairs containing any other data about the contact"""
    logo_url: NotRequired[str]
    r"""URL to the contact's avatar or logo"""
    tags: NotRequired[List[str]]
    r"""A list of tags associated with the contact"""


class StreamRunAgentContact(BaseModel):
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""

    id: str
    r"""Unique identifier for the contact"""

    display_name: Optional[str] = None
    r"""Display name of the contact"""

    email: Optional[str] = None
    r"""Email address of the contact"""

    metadata: Optional[List[Dict[str, Any]]] = None
    r"""A hash of key/value pairs containing any other data about the contact"""

    logo_url: Optional[str] = None
    r"""URL to the contact's avatar or logo"""

    tags: Optional[List[str]] = None
    r"""A list of tags associated with the contact"""


class StreamRunAgentThreadTypedDict(TypedDict):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""
    tags: NotRequired[List[str]]
    r"""Optional tags to differentiate or categorize threads"""


class StreamRunAgentThread(BaseModel):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""

    tags: Optional[List[str]] = None
    r"""Optional tags to differentiate or categorize threads"""


class StreamRunAgentMemoryTypedDict(TypedDict):
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""

    entity_id: str
    r"""An entity ID used to link memory stores to a specific user, session, or conversation. This ID is used to isolate and retrieve memories specific to the entity across agent executions."""


class StreamRunAgentMemory(BaseModel):
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""

    entity_id: str
    r"""An entity ID used to link memory stores to a specific user, session, or conversation. This ID is used to isolate and retrieve memories specific to the entity across agent executions."""


class StreamRunAgentKnowledgeBasesTypedDict(TypedDict):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


class StreamRunAgentKnowledgeBases(BaseModel):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


class StreamRunAgentTeamOfAgentsTypedDict(TypedDict):
    key: str
    r"""The unique key of the agent within the workspace"""
    role: NotRequired[str]
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""


class StreamRunAgentTeamOfAgents(BaseModel):
    key: str
    r"""The unique key of the agent within the workspace"""

    role: Optional[str] = None
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools15Type = Literal[
    "mcp",
]


class StreamRunAgentAgentToolInputRunAgentsHeadersTypedDict(TypedDict):
    value: str
    encrypted: NotRequired[bool]


class StreamRunAgentAgentToolInputRunAgentsHeaders(BaseModel):
    value: str

    encrypted: Optional[bool] = False


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools15McpType = Literal[
    "object",
]


class AgentToolInputRunSchemaTypedDict(TypedDict):
    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools15McpType
    properties: NotRequired[Dict[str, Any]]
    required: NotRequired[List[str]]


class AgentToolInputRunSchema(BaseModel):
    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools15McpType

    properties: Optional[Dict[str, Any]] = None

    required: Optional[List[str]] = None


class AgentToolInputRunToolsTypedDict(TypedDict):
    name: str
    schema_: AgentToolInputRunSchemaTypedDict
    id: NotRequired[str]
    description: NotRequired[str]


class AgentToolInputRunTools(BaseModel):
    name: str

    schema_: Annotated[AgentToolInputRunSchema, pydantic.Field(alias="schema")]

    id: Optional[str] = "01KAGY2YDR4F1NAC0D11M51NK5"

    description: Optional[str] = None


AgentToolInputRunConnectionType = Literal[
    "http",
    "sse",
]
r"""The connection type used by the MCP server"""


class AgentToolInputRunMcpTypedDict(TypedDict):
    server_url: str
    r"""The MCP server URL (cached for execution)"""
    tools: List[AgentToolInputRunToolsTypedDict]
    r"""Array of tools available from the MCP server"""
    connection_type: AgentToolInputRunConnectionType
    r"""The connection type used by the MCP server"""
    headers: NotRequired[
        Dict[str, StreamRunAgentAgentToolInputRunAgentsHeadersTypedDict]
    ]
    r"""HTTP headers for MCP server requests with encryption support"""


class AgentToolInputRunMcp(BaseModel):
    server_url: str
    r"""The MCP server URL (cached for execution)"""

    tools: List[AgentToolInputRunTools]
    r"""Array of tools available from the MCP server"""

    connection_type: AgentToolInputRunConnectionType
    r"""The connection type used by the MCP server"""

    headers: Optional[Dict[str, StreamRunAgentAgentToolInputRunAgentsHeaders]] = None
    r"""HTTP headers for MCP server requests with encryption support"""


class AgentToolInputRunMCPToolRunTypedDict(TypedDict):
    r"""MCP tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools15Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""
    mcp: AgentToolInputRunMcpTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    requires_approval: NotRequired[bool]


class AgentToolInputRunMCPToolRun(BaseModel):
    r"""MCP tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools15Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""

    mcp: AgentToolInputRunMcp

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    requires_approval: Optional[bool] = False


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14Type = Literal[
    "function",
]


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14FunctionType = (
    Literal["object",]
)
r"""The type must be \"object\" """


class StreamRunAgentAgentToolInputRunAgentsParametersTypedDict(TypedDict):
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14FunctionType
    r"""The type must be \"object\" """
    properties: Dict[str, Any]
    r"""The properties of the function parameters"""
    required: List[str]
    r"""Array of required parameter names"""


class StreamRunAgentAgentToolInputRunAgentsParameters(BaseModel):
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14FunctionType
    r"""The type must be \"object\" """

    properties: Dict[str, Any]
    r"""The properties of the function parameters"""

    required: List[str]
    r"""Array of required parameter names"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class StreamRunAgentAgentToolInputRunFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the function does, used by the model to choose when and how to call the function."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Currently only compatible with `OpenAI` models."""
    parameters: NotRequired[StreamRunAgentAgentToolInputRunAgentsParametersTypedDict]
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""


class StreamRunAgentAgentToolInputRunFunction(BaseModel):
    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the function does, used by the model to choose when and how to call the function."""

    strict: Optional[bool] = None
    r"""Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Currently only compatible with `OpenAI` models."""

    parameters: Optional[StreamRunAgentAgentToolInputRunAgentsParameters] = None
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""


class AgentToolInputRunFunctionToolRunTypedDict(TypedDict):
    r"""Function tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    function: StreamRunAgentAgentToolInputRunFunctionTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    description: NotRequired[str]
    requires_approval: NotRequired[bool]


class AgentToolInputRunFunctionToolRun(BaseModel):
    r"""Function tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    function: StreamRunAgentAgentToolInputRunFunction

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    description: Optional[str] = None

    requires_approval: Optional[bool] = False


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13Type = Literal[
    "code",
]


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13CodeToolType = (
    Literal["object",]
)
r"""The type must be \"object\" """


class StreamRunAgentAgentToolInputRunParametersTypedDict(TypedDict):
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13CodeToolType
    r"""The type must be \"object\" """
    properties: Dict[str, Any]
    r"""The properties of the function parameters"""
    required: List[str]
    r"""Array of required parameter names"""


class StreamRunAgentAgentToolInputRunParameters(BaseModel):
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13CodeToolType
    r"""The type must be \"object\" """

    properties: Dict[str, Any]
    r"""The properties of the function parameters"""

    required: List[str]
    r"""Array of required parameter names"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


AgentToolInputRunLanguage = Literal["python",]


class AgentToolInputRunCodeToolTypedDict(TypedDict):
    language: AgentToolInputRunLanguage
    code: str
    r"""The code to execute."""
    parameters: NotRequired[StreamRunAgentAgentToolInputRunParametersTypedDict]
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""


class AgentToolInputRunCodeTool(BaseModel):
    language: AgentToolInputRunLanguage

    code: str
    r"""The code to execute."""

    parameters: Optional[StreamRunAgentAgentToolInputRunParameters] = None
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""


class AgentToolInputRunCodeToolRunTypedDict(TypedDict):
    r"""Code execution tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""
    code_tool: AgentToolInputRunCodeToolTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    requires_approval: NotRequired[bool]


class AgentToolInputRunCodeToolRun(BaseModel):
    r"""Code execution tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""

    code_tool: AgentToolInputRunCodeTool

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    requires_approval: Optional[bool] = False


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12Type = Literal[
    "http",
]


AgentToolInputRunMethod = Literal[
    "GET",
    "POST",
    "PUT",
    "DELETE",
]
r"""The HTTP method to use."""


class StreamRunAgentHeaders2TypedDict(TypedDict):
    value: str
    encrypted: NotRequired[bool]


class StreamRunAgentHeaders2(BaseModel):
    value: str

    encrypted: Optional[bool] = False


StreamRunAgentAgentToolInputRunHeadersTypedDict = TypeAliasType(
    "StreamRunAgentAgentToolInputRunHeadersTypedDict",
    Union[StreamRunAgentHeaders2TypedDict, str],
)


StreamRunAgentAgentToolInputRunHeaders = TypeAliasType(
    "StreamRunAgentAgentToolInputRunHeaders", Union[StreamRunAgentHeaders2, str]
)


class AgentToolInputRunBlueprintTypedDict(TypedDict):
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""

    url: str
    r"""The URL to send the request to."""
    method: AgentToolInputRunMethod
    r"""The HTTP method to use."""
    headers: NotRequired[Dict[str, StreamRunAgentAgentToolInputRunHeadersTypedDict]]
    r"""The headers to send with the request. Can be a string value or an object with value and encrypted properties."""
    body: NotRequired[Dict[str, Any]]
    r"""The body to send with the request."""


class AgentToolInputRunBlueprint(BaseModel):
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""

    url: str
    r"""The URL to send the request to."""

    method: AgentToolInputRunMethod
    r"""The HTTP method to use."""

    headers: Optional[Dict[str, StreamRunAgentAgentToolInputRunHeaders]] = None
    r"""The headers to send with the request. Can be a string value or an object with value and encrypted properties."""

    body: Optional[Dict[str, Any]] = None
    r"""The body to send with the request."""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12HTTPType = (
    Literal[
        "string",
        "number",
        "boolean",
    ]
)
r"""The type of the argument."""


AgentToolInputRunDefaultValueTypedDict = TypeAliasType(
    "AgentToolInputRunDefaultValueTypedDict", Union[str, float, bool]
)
r"""The default value of the argument."""


AgentToolInputRunDefaultValue = TypeAliasType(
    "AgentToolInputRunDefaultValue", Union[str, float, bool]
)
r"""The default value of the argument."""


class AgentToolInputRunArgumentsTypedDict(TypedDict):
    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12HTTPType
    r"""The type of the argument."""
    description: str
    r"""A description of the argument."""
    send_to_model: NotRequired[bool]
    r"""Whether to send the argument to the model. If set to false, the argument will not be sent to the model and needs to be provided by the user or it will be left blank."""
    default_value: NotRequired[AgentToolInputRunDefaultValueTypedDict]
    r"""The default value of the argument."""


class AgentToolInputRunArguments(BaseModel):
    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12HTTPType
    r"""The type of the argument."""

    description: str
    r"""A description of the argument."""

    send_to_model: Optional[bool] = True
    r"""Whether to send the argument to the model. If set to false, the argument will not be sent to the model and needs to be provided by the user or it will be left blank."""

    default_value: Optional[AgentToolInputRunDefaultValue] = None
    r"""The default value of the argument."""


class AgentToolInputRunHTTPTypedDict(TypedDict):
    blueprint: AgentToolInputRunBlueprintTypedDict
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""
    arguments: NotRequired[Dict[str, AgentToolInputRunArgumentsTypedDict]]
    r"""The arguments to send with the request. The keys will be used to replace the placeholders in the `blueprint` field."""


class AgentToolInputRunHTTP(BaseModel):
    blueprint: AgentToolInputRunBlueprint
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""

    arguments: Optional[Dict[str, AgentToolInputRunArguments]] = None
    r"""The arguments to send with the request. The keys will be used to replace the placeholders in the `blueprint` field."""


class AgentToolInputRunHTTPToolRunTypedDict(TypedDict):
    r"""HTTP tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""
    http: AgentToolInputRunHTTPTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    requires_approval: NotRequired[bool]


class AgentToolInputRunHTTPToolRun(BaseModel):
    r"""HTTP tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""

    http: AgentToolInputRunHTTP

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    requires_approval: Optional[bool] = False


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools11Type = Literal[
    "current_date",
]


class StreamRunAgentAgentToolInputRunCurrentDateToolTypedDict(TypedDict):
    r"""Returns the current date and time"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools11Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunCurrentDateTool(BaseModel):
    r"""Returns the current date and time"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools11Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools10Type = Literal[
    "query_knowledge_base",
]


class StreamRunAgentAgentToolInputRunQueryKnowledgeBaseToolTypedDict(TypedDict):
    r"""Queries knowledge bases for information"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools10Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunQueryKnowledgeBaseTool(BaseModel):
    r"""Queries knowledge bases for information"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools10Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools9Type = Literal[
    "retrieve_knowledge_bases",
]


class StreamRunAgentAgentToolInputRunRetrieveKnowledgeBasesToolTypedDict(TypedDict):
    r"""Lists available knowledge bases"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools9Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunRetrieveKnowledgeBasesTool(BaseModel):
    r"""Lists available knowledge bases"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools9Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools8Type = Literal[
    "delete_memory_document",
]


class StreamRunAgentAgentToolInputRunDeleteMemoryDocumentToolTypedDict(TypedDict):
    r"""Deletes documents from memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools8Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunDeleteMemoryDocumentTool(BaseModel):
    r"""Deletes documents from memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools8Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools7Type = Literal[
    "retrieve_memory_stores",
]


class StreamRunAgentAgentToolInputRunRetrieveMemoryStoresToolTypedDict(TypedDict):
    r"""Lists available memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools7Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunRetrieveMemoryStoresTool(BaseModel):
    r"""Lists available memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools7Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsToolsType = Literal[
    "write_memory_store",
]


class StreamRunAgentAgentToolInputRunWriteMemoryStoreToolTypedDict(TypedDict):
    r"""Writes information to agent memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsToolsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunWriteMemoryStoreTool(BaseModel):
    r"""Writes information to agent memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsToolsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsType = Literal[
    "query_memory_store",
]


class StreamRunAgentAgentToolInputRunQueryMemoryStoreToolTypedDict(TypedDict):
    r"""Queries agent memory stores for context"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunQueryMemoryStoreTool(BaseModel):
    r"""Queries agent memory stores for context"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodyType = Literal[
    "retrieve_agents",
]


class StreamRunAgentAgentToolInputRunRetrieveAgentsToolTypedDict(TypedDict):
    r"""Retrieves available agents in the system"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodyType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunRetrieveAgentsTool(BaseModel):
    r"""Retrieves available agents in the system"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodyType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestType = Literal["call_sub_agent",]


class StreamRunAgentAgentToolInputRunCallSubAgentToolTypedDict(TypedDict):
    r"""Delegates tasks to specialized sub-agents"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunCallSubAgentTool(BaseModel):
    r"""Delegates tasks to specialized sub-agents"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsType = Literal["web_scraper",]


class StreamRunAgentAgentToolInputRunWebScraperToolTypedDict(TypedDict):
    r"""Scrapes and extracts content from web pages"""

    type: StreamRunAgentAgentToolInputRunAgentsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunWebScraperTool(BaseModel):
    r"""Scrapes and extracts content from web pages"""

    type: StreamRunAgentAgentToolInputRunAgentsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunType = Literal["google_search",]


class StreamRunAgentAgentToolInputRunGoogleSearchToolTypedDict(TypedDict):
    r"""Performs Google searches to retrieve web content"""

    type: StreamRunAgentAgentToolInputRunType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunGoogleSearchTool(BaseModel):
    r"""Performs Google searches to retrieve web content"""

    type: StreamRunAgentAgentToolInputRunType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunTypedDict = TypeAliasType(
    "StreamRunAgentAgentToolInputRunTypedDict",
    Union[
        StreamRunAgentAgentToolInputRunGoogleSearchToolTypedDict,
        StreamRunAgentAgentToolInputRunWebScraperToolTypedDict,
        StreamRunAgentAgentToolInputRunCallSubAgentToolTypedDict,
        StreamRunAgentAgentToolInputRunRetrieveAgentsToolTypedDict,
        StreamRunAgentAgentToolInputRunQueryMemoryStoreToolTypedDict,
        StreamRunAgentAgentToolInputRunWriteMemoryStoreToolTypedDict,
        StreamRunAgentAgentToolInputRunRetrieveMemoryStoresToolTypedDict,
        StreamRunAgentAgentToolInputRunDeleteMemoryDocumentToolTypedDict,
        StreamRunAgentAgentToolInputRunRetrieveKnowledgeBasesToolTypedDict,
        StreamRunAgentAgentToolInputRunQueryKnowledgeBaseToolTypedDict,
        StreamRunAgentAgentToolInputRunCurrentDateToolTypedDict,
        AgentToolInputRunHTTPToolRunTypedDict,
        AgentToolInputRunCodeToolRunTypedDict,
        AgentToolInputRunFunctionToolRunTypedDict,
        AgentToolInputRunMCPToolRunTypedDict,
    ],
)
r"""Tool configuration for agent run operations. Built-in tools only require a type and requires_approval, while custom tools (HTTP, Code, Function, MCP) support full inline definitions for on-the-fly creation."""


StreamRunAgentAgentToolInputRun = TypeAliasType(
    "StreamRunAgentAgentToolInputRun",
    Union[
        StreamRunAgentAgentToolInputRunGoogleSearchTool,
        StreamRunAgentAgentToolInputRunWebScraperTool,
        StreamRunAgentAgentToolInputRunCallSubAgentTool,
        StreamRunAgentAgentToolInputRunRetrieveAgentsTool,
        StreamRunAgentAgentToolInputRunQueryMemoryStoreTool,
        StreamRunAgentAgentToolInputRunWriteMemoryStoreTool,
        StreamRunAgentAgentToolInputRunRetrieveMemoryStoresTool,
        StreamRunAgentAgentToolInputRunDeleteMemoryDocumentTool,
        StreamRunAgentAgentToolInputRunRetrieveKnowledgeBasesTool,
        StreamRunAgentAgentToolInputRunQueryKnowledgeBaseTool,
        StreamRunAgentAgentToolInputRunCurrentDateTool,
        AgentToolInputRunHTTPToolRun,
        AgentToolInputRunCodeToolRun,
        AgentToolInputRunFunctionToolRun,
        AgentToolInputRunMCPToolRun,
    ],
)
r"""Tool configuration for agent run operations. Built-in tools only require a type and requires_approval, while custom tools (HTTP, Code, Function, MCP) support full inline definitions for on-the-fly creation."""


StreamRunAgentToolApprovalRequired = Literal[
    "all",
    "respect_tool",
    "none",
]
r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""


StreamRunAgentExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class StreamRunAgentEvaluatorsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: StreamRunAgentExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class StreamRunAgentEvaluators(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: StreamRunAgentExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


StreamRunAgentAgentsExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class StreamRunAgentGuardrailsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: StreamRunAgentAgentsExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class StreamRunAgentGuardrails(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: StreamRunAgentAgentsExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class StreamRunAgentSettingsTypedDict(TypedDict):
    tools: NotRequired[List[StreamRunAgentAgentToolInputRunTypedDict]]
    r"""Tools available to the agent"""
    tool_approval_required: NotRequired[StreamRunAgentToolApprovalRequired]
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""
    max_iterations: NotRequired[int]
    r"""Maximum iterations(llm calls) before the agent will stop executing."""
    max_execution_time: NotRequired[int]
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""
    evaluators: NotRequired[List[StreamRunAgentEvaluatorsTypedDict]]
    r"""Configuration for an evaluator applied to the agent"""
    guardrails: NotRequired[List[StreamRunAgentGuardrailsTypedDict]]
    r"""Configuration for a guardrail applied to the agent"""


class StreamRunAgentSettings(BaseModel):
    tools: Optional[List[StreamRunAgentAgentToolInputRun]] = None
    r"""Tools available to the agent"""

    tool_approval_required: Optional[StreamRunAgentToolApprovalRequired] = "none"
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""

    max_iterations: Optional[int] = 15
    r"""Maximum iterations(llm calls) before the agent will stop executing."""

    max_execution_time: Optional[int] = 300
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""

    evaluators: Optional[List[StreamRunAgentEvaluators]] = None
    r"""Configuration for an evaluator applied to the agent"""

    guardrails: Optional[List[StreamRunAgentGuardrails]] = None
    r"""Configuration for a guardrail applied to the agent"""


class StreamRunAgentRequestBodyTypedDict(TypedDict):
    key: str
    r"""A unique identifier for the agent. This key must be unique within the same workspace and cannot be reused. When executing the agent, this key determines if the agent already exists. If the agent version differs, a new version is created at the end of the execution, except for the task. All agent parameters are evaluated to decide if a new version is needed."""
    model: StreamRunAgentModelConfigurationTypedDict
    r"""Model configuration for this execution. Can override the agent manifest defaults if the agent already exists."""
    role: str
    r"""Specifies the agent's function and area of expertise."""
    instructions: str
    r"""Provides context and purpose for the agent. Combined with the system prompt template to generate the agent's instructions."""
    message: StreamRunAgentA2AMessageTypedDict
    r"""The A2A format message containing the task for the agent to perform."""
    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """
    settings: StreamRunAgentSettingsTypedDict
    task_id: NotRequired[str]
    r"""Optional task ID to continue an existing agent execution. When provided, the agent will continue the conversation from the existing task state. The task must be in an inactive state to continue."""
    fallback_models: NotRequired[
        List[StreamRunAgentFallbackModelConfigurationTypedDict]
    ]
    r"""Optional array of fallback models used when the primary model fails. Fallbacks are attempted in order. All models must support tool calling."""
    variables: NotRequired[Dict[str, Any]]
    r"""Optional variables for template replacement in system prompt, instructions, and messages"""
    contact: NotRequired[StreamRunAgentContactTypedDict]
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""
    thread: NotRequired[StreamRunAgentThreadTypedDict]
    r"""Thread information to group related requests"""
    memory: NotRequired[StreamRunAgentMemoryTypedDict]
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""
    description: NotRequired[str]
    r"""A brief summary of the agent's purpose."""
    system_prompt: NotRequired[str]
    r"""A custom system prompt template for the agent. If omitted, the default template is used."""
    memory_stores: NotRequired[List[str]]
    r"""Array of memory store identifiers that are accessible to the agent. Accepts both memory store IDs and keys."""
    knowledge_bases: NotRequired[List[StreamRunAgentKnowledgeBasesTypedDict]]
    r"""Knowledge base configurations for the agent to access"""
    team_of_agents: NotRequired[List[StreamRunAgentTeamOfAgentsTypedDict]]
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""
    metadata: NotRequired[Dict[str, Any]]
    r"""Optional metadata for the agent run as key-value pairs that will be included in traces"""
    stream_timeout_seconds: NotRequired[float]
    r"""Stream timeout in seconds (1-3600). Default: 1800 (30 minutes)"""


class StreamRunAgentRequestBody(BaseModel):
    key: str
    r"""A unique identifier for the agent. This key must be unique within the same workspace and cannot be reused. When executing the agent, this key determines if the agent already exists. If the agent version differs, a new version is created at the end of the execution, except for the task. All agent parameters are evaluated to decide if a new version is needed."""

    model: StreamRunAgentModelConfiguration
    r"""Model configuration for this execution. Can override the agent manifest defaults if the agent already exists."""

    role: str
    r"""Specifies the agent's function and area of expertise."""

    instructions: str
    r"""Provides context and purpose for the agent. Combined with the system prompt template to generate the agent's instructions."""

    message: StreamRunAgentA2AMessage
    r"""The A2A format message containing the task for the agent to perform."""

    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """

    settings: StreamRunAgentSettings

    task_id: Optional[str] = None
    r"""Optional task ID to continue an existing agent execution. When provided, the agent will continue the conversation from the existing task state. The task must be in an inactive state to continue."""

    fallback_models: Optional[List[StreamRunAgentFallbackModelConfiguration]] = None
    r"""Optional array of fallback models used when the primary model fails. Fallbacks are attempted in order. All models must support tool calling."""

    variables: Optional[Dict[str, Any]] = None
    r"""Optional variables for template replacement in system prompt, instructions, and messages"""

    contact: Optional[StreamRunAgentContact] = None
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""

    thread: Optional[StreamRunAgentThread] = None
    r"""Thread information to group related requests"""

    memory: Optional[StreamRunAgentMemory] = None
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""

    description: Optional[str] = None
    r"""A brief summary of the agent's purpose."""

    system_prompt: Optional[str] = None
    r"""A custom system prompt template for the agent. If omitted, the default template is used."""

    memory_stores: Optional[List[str]] = None
    r"""Array of memory store identifiers that are accessible to the agent. Accepts both memory store IDs and keys."""

    knowledge_bases: Optional[List[StreamRunAgentKnowledgeBases]] = None
    r"""Knowledge base configurations for the agent to access"""

    team_of_agents: Optional[List[StreamRunAgentTeamOfAgents]] = None
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""

    metadata: Optional[Dict[str, Any]] = None
    r"""Optional metadata for the agent run as key-value pairs that will be included in traces"""

    stream_timeout_seconds: Optional[float] = None
    r"""Stream timeout in seconds (1-3600). Default: 1800 (30 minutes)"""


class StreamRunAgentAgentsResponseBodyData(BaseModel):
    message: str


@dataclass(unsafe_hash=True)
class StreamRunAgentAgentsResponseBody(OrqError):
    r"""Model not found"""

    data: StreamRunAgentAgentsResponseBodyData = field(hash=False)

    def __init__(
        self,
        data: StreamRunAgentAgentsResponseBodyData,
        raw_response: httpx.Response,
        body: Optional[str] = None,
    ):
        fallback = body or raw_response.text
        message = str(data.message) or fallback
        super().__init__(message, raw_response, body)
        object.__setattr__(self, "data", data)


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody17Type = Literal[
    "agents.error",
]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody17DataTypedDict(
    TypedDict
):
    error: str
    code: str


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody17Data(BaseModel):
    error: str

    code: str


class SeventeenTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody17Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody17DataTypedDict


class Seventeen(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody17Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody17Data


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody16Type = Literal[
    "agents.timeout",
]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody16DataTypedDict(
    TypedDict
):
    message: str


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody16Data(BaseModel):
    message: str


class SixteenTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody16Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody16DataTypedDict


class Sixteen(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody16Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody16Data


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody15Type = Literal[
    "event.workflow_events.tool_execution_failed",
]


class ErrorTypedDict(TypedDict):
    message: str
    stack: NotRequired[str]


class Error(BaseModel):
    message: str

    stack: Optional[str] = None


StreamRunAgentDataProduct = Literal[
    "remoteconfigs",
    "deployments",
    "experiments",
    "playgrounds",
    "spreadsheets",
    "spreadsheet_run",
    "llm_evaluator",
    "knowledge",
    "router",
    "workflows",
    "external_events",
    "agents",
    "memory-stores",
    "generic",
    "evaluators",
    "otel",
]
r"""Orquesta product"""


class StreamRunAgentDataAgentsMemoryTypedDict(TypedDict):
    entity_id: str


class StreamRunAgentDataAgentsMemory(BaseModel):
    entity_id: str


class StreamRunAgentDataToolExecutionContextTypedDict(TypedDict):
    action_id: str
    agent_tool_call_id: str
    workspace_id: str
    agent_manifest_id: str
    agent_execution_id: str
    product: StreamRunAgentDataProduct
    r"""Orquesta product"""
    memory: NotRequired[StreamRunAgentDataAgentsMemoryTypedDict]
    parent_id: NotRequired[str]


class StreamRunAgentDataToolExecutionContext(BaseModel):
    action_id: str

    agent_tool_call_id: str

    workspace_id: str

    agent_manifest_id: str

    agent_execution_id: str

    product: StreamRunAgentDataProduct
    r"""Orquesta product"""

    memory: Optional[StreamRunAgentDataAgentsMemory] = None

    parent_id: Optional[str] = None


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody15DataTypedDict(
    TypedDict
):
    error: ErrorTypedDict
    action_type: str
    tool_execution_context: StreamRunAgentDataToolExecutionContextTypedDict


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody15Data(BaseModel):
    error: Error

    action_type: str

    tool_execution_context: StreamRunAgentDataToolExecutionContext


class FifteenTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody15Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody15DataTypedDict


class Fifteen(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody15Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody15Data


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody14Type = Literal[
    "event.workflow_events.tool_execution_finished",
]


DataProduct = Literal[
    "remoteconfigs",
    "deployments",
    "experiments",
    "playgrounds",
    "spreadsheets",
    "spreadsheet_run",
    "llm_evaluator",
    "knowledge",
    "router",
    "workflows",
    "external_events",
    "agents",
    "memory-stores",
    "generic",
    "evaluators",
    "otel",
]
r"""Orquesta product"""


class StreamRunAgentDataMemoryTypedDict(TypedDict):
    entity_id: str


class StreamRunAgentDataMemory(BaseModel):
    entity_id: str


class DataToolExecutionContextTypedDict(TypedDict):
    action_id: str
    agent_tool_call_id: str
    workspace_id: str
    agent_manifest_id: str
    agent_execution_id: str
    product: DataProduct
    r"""Orquesta product"""
    memory: NotRequired[StreamRunAgentDataMemoryTypedDict]
    parent_id: NotRequired[str]


class DataToolExecutionContext(BaseModel):
    action_id: str

    agent_tool_call_id: str

    workspace_id: str

    agent_manifest_id: str

    agent_execution_id: str

    product: DataProduct
    r"""Orquesta product"""

    memory: Optional[StreamRunAgentDataMemory] = None

    parent_id: Optional[str] = None


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody14DataTypedDict(
    TypedDict
):
    action_type: str
    tool_execution_context: DataToolExecutionContextTypedDict
    result: NotRequired[Any]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody14Data(BaseModel):
    action_type: str

    tool_execution_context: DataToolExecutionContext

    result: Optional[Any] = None


class FourteenTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody14Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody14DataTypedDict


class Fourteen(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody14Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody14Data


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody13Type = Literal[
    "event.workflow_events.tool_execution_started",
]


Product = Literal[
    "remoteconfigs",
    "deployments",
    "experiments",
    "playgrounds",
    "spreadsheets",
    "spreadsheet_run",
    "llm_evaluator",
    "knowledge",
    "router",
    "workflows",
    "external_events",
    "agents",
    "memory-stores",
    "generic",
    "evaluators",
    "otel",
]
r"""Orquesta product"""


class DataMemoryTypedDict(TypedDict):
    entity_id: str


class DataMemory(BaseModel):
    entity_id: str


class ToolExecutionContextTypedDict(TypedDict):
    action_id: str
    agent_tool_call_id: str
    workspace_id: str
    agent_manifest_id: str
    agent_execution_id: str
    product: Product
    r"""Orquesta product"""
    memory: NotRequired[DataMemoryTypedDict]
    parent_id: NotRequired[str]


class ToolExecutionContext(BaseModel):
    action_id: str

    agent_tool_call_id: str

    workspace_id: str

    agent_manifest_id: str

    agent_execution_id: str

    product: Product
    r"""Orquesta product"""

    memory: Optional[DataMemory] = None

    parent_id: Optional[str] = None


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody13DataTypedDict(
    TypedDict
):
    tool_id: str
    action_type: str
    tool_arguments: Dict[str, Any]
    tool_execution_context: ToolExecutionContextTypedDict
    tool_key: NotRequired[str]
    tool_display_name: NotRequired[str]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody13Data(BaseModel):
    tool_id: str

    action_type: str

    tool_arguments: Dict[str, Any]

    tool_execution_context: ToolExecutionContext

    tool_key: Optional[str] = None

    tool_display_name: Optional[str] = None


class ThirteenTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody13Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody13DataTypedDict


class Thirteen(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody13Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody13Data


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody12Type = Literal[
    "event.agents.message-created",
]


StreamRunAgentDataAgentsRole = Literal[
    "user",
    "tool",
]


StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataMessage5Kind = Literal[
    "tool_result",
]


class StreamRunAgentPartsAgentsResponseToolResultPartTypedDict(TypedDict):
    r"""The result of a tool execution. Contains the tool call ID for correlation and the result data from the tool invocation."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataMessage5Kind
    tool_call_id: str
    result: NotRequired[Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsAgentsResponseToolResultPart(BaseModel):
    r"""The result of a tool execution. Contains the tool call ID for correlation and the result data from the tool invocation."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataMessage5Kind

    tool_call_id: str

    result: Optional[Any] = None

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataMessage4Kind = Literal[
    "tool_call",
]


class StreamRunAgentPartsAgentsResponseToolCallPartTypedDict(TypedDict):
    r"""A tool invocation request from an agent. Contains the tool name, unique call ID, and arguments for the tool execution."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataMessage4Kind
    tool_name: str
    tool_call_id: str
    arguments: Dict[str, Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsAgentsResponseToolCallPart(BaseModel):
    r"""A tool invocation request from an agent. Contains the tool name, unique call ID, and arguments for the tool execution."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataMessage4Kind

    tool_name: str

    tool_call_id: str

    arguments: Dict[str, Any]

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataMessageKind = (
    Literal["file",]
)


class StreamRunAgentFileAgentsResponse200FileInURIFormatTypedDict(TypedDict):
    r"""File in URI format. Check in the model's documentation for the supported mime types for the URI format"""

    uri: str
    r"""URL for the File content"""
    mime_type: NotRequired[str]
    r"""Optional mimeType for the file"""
    name: NotRequired[str]
    r"""Optional name for the file"""


class StreamRunAgentFileAgentsResponse200FileInURIFormat(BaseModel):
    r"""File in URI format. Check in the model's documentation for the supported mime types for the URI format"""

    uri: str
    r"""URL for the File content"""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""Optional mimeType for the file"""

    name: Optional[str] = None
    r"""Optional name for the file"""


class StreamRunAgentFileAgentsResponse200BinaryFormatTypedDict(TypedDict):
    r"""Binary in base64 format. Check in the model's documentation for the supported mime types for the binary format."""

    bytes_: str
    r"""base64 encoded content of the file"""
    mime_type: NotRequired[str]
    r"""Optional mimeType for the file"""
    name: NotRequired[str]
    r"""Optional name for the file"""


class StreamRunAgentFileAgentsResponse200BinaryFormat(BaseModel):
    r"""Binary in base64 format. Check in the model's documentation for the supported mime types for the binary format."""

    bytes_: Annotated[str, pydantic.Field(alias="bytes")]
    r"""base64 encoded content of the file"""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""Optional mimeType for the file"""

    name: Optional[str] = None
    r"""Optional name for the file"""


StreamRunAgentPartsAgentsResponseFileTypedDict = TypeAliasType(
    "StreamRunAgentPartsAgentsResponseFileTypedDict",
    Union[
        StreamRunAgentFileAgentsResponse200BinaryFormatTypedDict,
        StreamRunAgentFileAgentsResponse200FileInURIFormatTypedDict,
    ],
)


StreamRunAgentPartsAgentsResponseFile = TypeAliasType(
    "StreamRunAgentPartsAgentsResponseFile",
    Union[
        StreamRunAgentFileAgentsResponse200BinaryFormat,
        StreamRunAgentFileAgentsResponse200FileInURIFormat,
    ],
)


class StreamRunAgentPartsAgentsResponseFilePartTypedDict(TypedDict):
    r"""A file content part that can contain either base64-encoded bytes or a URI reference. Used for images, documents, and other binary content in agent communications."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataMessageKind
    file: StreamRunAgentPartsAgentsResponseFileTypedDict
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsAgentsResponseFilePart(BaseModel):
    r"""A file content part that can contain either base64-encoded bytes or a URI reference. Used for images, documents, and other binary content in agent communications."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataMessageKind

    file: StreamRunAgentPartsAgentsResponseFile

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataKind = Literal[
    "data",
]


class StreamRunAgentPartsAgentsResponseDataPartTypedDict(TypedDict):
    r"""A structured data part containing JSON-serializable key-value pairs. Used for passing structured information between agents and tools."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataKind
    data: Dict[str, Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsAgentsResponseDataPart(BaseModel):
    r"""A structured data part containing JSON-serializable key-value pairs. Used for passing structured information between agents and tools."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12DataKind

    data: Dict[str, Any]

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12Kind = Literal[
    "text",
]


class StreamRunAgentPartsAgentsResponseTextPartTypedDict(TypedDict):
    r"""A text content part containing plain text or markdown. Used for agent messages, user input, and text-based responses."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12Kind
    text: str


class StreamRunAgentPartsAgentsResponseTextPart(BaseModel):
    r"""A text content part containing plain text or markdown. Used for agent messages, user input, and text-based responses."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData12Kind

    text: str


StreamRunAgentDataAgentsPartsTypedDict = TypeAliasType(
    "StreamRunAgentDataAgentsPartsTypedDict",
    Union[
        StreamRunAgentPartsAgentsResponseTextPartTypedDict,
        StreamRunAgentPartsAgentsResponseDataPartTypedDict,
        StreamRunAgentPartsAgentsResponseFilePartTypedDict,
        StreamRunAgentPartsAgentsResponseToolResultPartTypedDict,
        StreamRunAgentPartsAgentsResponseToolCallPartTypedDict,
    ],
)


StreamRunAgentDataAgentsParts = TypeAliasType(
    "StreamRunAgentDataAgentsParts",
    Union[
        StreamRunAgentPartsAgentsResponseTextPart,
        StreamRunAgentPartsAgentsResponseDataPart,
        StreamRunAgentPartsAgentsResponseFilePart,
        StreamRunAgentPartsAgentsResponseToolResultPart,
        StreamRunAgentPartsAgentsResponseToolCallPart,
    ],
)


class DataMessageTypedDict(TypedDict):
    role: StreamRunAgentDataAgentsRole
    parts: List[StreamRunAgentDataAgentsPartsTypedDict]
    message_id: NotRequired[str]
    metadata: NotRequired[Dict[str, Any]]


class DataMessage(BaseModel):
    role: StreamRunAgentDataAgentsRole

    parts: List[StreamRunAgentDataAgentsParts]

    message_id: Annotated[Optional[str], pydantic.Field(alias="messageId")] = None

    metadata: Optional[Dict[str, Any]] = None


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody12DataTypedDict(
    TypedDict
):
    workflow_run_id: str
    span_id: str
    parent_id: str
    message: List[DataMessageTypedDict]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody12Data(BaseModel):
    workflow_run_id: Annotated[str, pydantic.Field(alias="workflowRunId")]

    span_id: Annotated[str, pydantic.Field(alias="spanId")]

    parent_id: Annotated[str, pydantic.Field(alias="parentId")]

    message: List[DataMessage]


class TwelveTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody12Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody12DataTypedDict


class Twelve(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody12Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody12Data


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody11Type = Literal[
    "event.agents.handed_off",
]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody11DataTypedDict(
    TypedDict
):
    agent_id: str
    input: str


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody11Data(BaseModel):
    agent_id: str

    input: str


class ElevenTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody11Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody11DataTypedDict


class Eleven(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody11Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody11Data


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody10Type = Literal[
    "event.agents.execution_named",
]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody10DataTypedDict(
    TypedDict
):
    name: str
    agent_manifest_id: str


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody10Data(BaseModel):
    name: str

    agent_manifest_id: str


class TenTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody10Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody10DataTypedDict


class Ten(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody10Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody10Data


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody9Type = Literal[
    "event.agents.execution_reviewed",
]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody9DataTypedDict(
    TypedDict
):
    pass


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody9Data(BaseModel):
    pass


class NineTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody9Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody9DataTypedDict


class Nine(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody9Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody9Data


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody8Type = Literal[
    "event.agents.execution_review_required",
]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody8DataTypedDict(
    TypedDict
):
    pass


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody8Data(BaseModel):
    pass


class EightTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody8Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody8DataTypedDict


class Eight(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody8Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody8Data


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody7Type = Literal[
    "event.agents.action_reviewed",
]


StreamRunAgentDataReview = Literal[
    "approved",
    "rejected",
]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBodyDataTypedDict(
    TypedDict
):
    agent_id: str
    action_id: str
    agent_tool_call_id: str
    review: StreamRunAgentDataReview
    mock_output: NotRequired[Dict[str, Any]]
    review_source: NotRequired[str]
    reviewed_by_id: NotRequired[str]


class StreamRunAgentDataAgentsResponse200TextEventStreamResponseBodyData(BaseModel):
    agent_id: str

    action_id: str

    agent_tool_call_id: str

    review: StreamRunAgentDataReview

    mock_output: Optional[Dict[str, Any]] = None

    review_source: Optional[str] = None

    reviewed_by_id: Optional[str] = None


class SevenTypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody7Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBodyDataTypedDict


class Seven(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody7Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBodyData


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBodyType = Literal[
    "event.agents.action_review_requested",
]


class DataConditionsTypedDict(TypedDict):
    condition: str
    r"""The argument of the tool call to evaluate"""
    operator: str
    r"""The operator to use"""
    value: str
    r"""The value to compare against"""


class DataConditions(BaseModel):
    condition: str
    r"""The argument of the tool call to evaluate"""

    operator: str
    r"""The operator to use"""

    value: str
    r"""The value to compare against"""


class StreamRunAgentDataToolTypedDict(TypedDict):
    id: str
    r"""The id of the resource"""
    action_type: str
    key: NotRequired[str]
    r"""Optional tool key for custom tools"""
    display_name: NotRequired[str]
    description: NotRequired[str]
    r"""Optional tool description"""
    requires_approval: NotRequired[bool]
    tool_id: NotRequired[str]
    r"""Nested tool ID for MCP tools (identifies specific tool within MCP server)"""
    conditions: NotRequired[List[DataConditionsTypedDict]]
    timeout: NotRequired[float]
    r"""Tool execution timeout in seconds (default: 2 minutes, max: 10 minutes)"""


class StreamRunAgentDataTool(BaseModel):
    id: str
    r"""The id of the resource"""

    action_type: str

    key: Optional[str] = None
    r"""Optional tool key for custom tools"""

    display_name: Optional[str] = None

    description: Optional[str] = None
    r"""Optional tool description"""

    requires_approval: Optional[bool] = False

    tool_id: Optional[str] = None
    r"""Nested tool ID for MCP tools (identifies specific tool within MCP server)"""

    conditions: Optional[List[DataConditions]] = None

    timeout: Optional[float] = 120
    r"""Tool execution timeout in seconds (default: 2 minutes, max: 10 minutes)"""


class StreamRunAgentDataAgentsResponse200TextEventStreamDataTypedDict(TypedDict):
    agent_id: str
    action_id: str
    requires_approval: bool
    tool: StreamRunAgentDataToolTypedDict
    input: Dict[str, Any]
    agent_tool_call_id: str


class StreamRunAgentDataAgentsResponse200TextEventStreamData(BaseModel):
    agent_id: str

    action_id: str

    requires_approval: bool

    tool: StreamRunAgentDataTool

    input: Dict[str, Any]

    agent_tool_call_id: str


class StreamRunAgentData6TypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBodyType
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200TextEventStreamDataTypedDict


class StreamRunAgentData6(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBodyType

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200TextEventStreamData


StreamRunAgentDataAgentsResponse200TextEventStreamType = Literal[
    "event.agents.errored",
]


class StreamRunAgentDataAgentsResponse200DataTypedDict(TypedDict):
    error: str
    code: float


class StreamRunAgentDataAgentsResponse200Data(BaseModel):
    error: str

    code: float


class StreamRunAgentData5TypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamType
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponse200DataTypedDict


class StreamRunAgentData5(BaseModel):
    type: StreamRunAgentDataAgentsResponse200TextEventStreamType

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponse200Data


StreamRunAgentDataAgentsResponse200Type = Literal["event.agents.inactive",]


FinishReason = Literal[
    "stop",
    "length",
    "tool_calls",
    "content_filter",
    "function_call",
    "max_iterations",
    "max_time",
]
r"""The reason why the agent execution became inactive"""


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody4Type = Literal[
    "function",
]


class StreamRunAgentDataFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    arguments: NotRequired[str]


class StreamRunAgentDataFunction(BaseModel):
    name: Optional[str] = None

    arguments: Optional[str] = None


class PendingToolCallsTypedDict(TypedDict):
    id: str
    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody4Type
    function: StreamRunAgentDataFunctionTypedDict


class PendingToolCalls(BaseModel):
    id: str

    type: StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody4Type

    function: StreamRunAgentDataFunction


class StreamRunAgentDataAgentsResponseDataTypedDict(TypedDict):
    last_message: str
    finish_reason: FinishReason
    r"""The reason why the agent execution became inactive"""
    pending_tool_calls: NotRequired[List[PendingToolCallsTypedDict]]
    r"""Tool calls that are pending user response (for function_call finish reason)"""


class StreamRunAgentDataAgentsResponseData(BaseModel):
    last_message: str

    finish_reason: FinishReason
    r"""The reason why the agent execution became inactive"""

    pending_tool_calls: Optional[List[PendingToolCalls]] = None
    r"""Tool calls that are pending user response (for function_call finish reason)"""


class Data4TypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponse200Type
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsResponseDataTypedDict


class Data4(BaseModel):
    type: StreamRunAgentDataAgentsResponse200Type

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsResponseData


StreamRunAgentDataAgentsResponseType = Literal["event.agents.thought",]


StreamRunAgentDataRole = Literal[
    "user",
    "agent",
    "tool",
    "system",
]


StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData3DataMessageDifferenceKind = Literal[
    "tool_result",
]


class StreamRunAgentPartsAgentsToolResultPartTypedDict(TypedDict):
    r"""The result of a tool execution. Contains the tool call ID for correlation and the result data from the tool invocation."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData3DataMessageDifferenceKind
    tool_call_id: str
    result: NotRequired[Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsAgentsToolResultPart(BaseModel):
    r"""The result of a tool execution. Contains the tool call ID for correlation and the result data from the tool invocation."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData3DataMessageDifferenceKind

    tool_call_id: str

    result: Optional[Any] = None

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData3DataKind = Literal[
    "tool_call",
]


class StreamRunAgentPartsAgentsToolCallPartTypedDict(TypedDict):
    r"""A tool invocation request from an agent. Contains the tool name, unique call ID, and arguments for the tool execution."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData3DataKind
    tool_name: str
    tool_call_id: str
    arguments: Dict[str, Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsAgentsToolCallPart(BaseModel):
    r"""A tool invocation request from an agent. Contains the tool name, unique call ID, and arguments for the tool execution."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData3DataKind

    tool_name: str

    tool_call_id: str

    arguments: Dict[str, Any]

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData3Kind = Literal[
    "file",
]


class StreamRunAgentFileAgentsResponseFileInURIFormatTypedDict(TypedDict):
    r"""File in URI format. Check in the model's documentation for the supported mime types for the URI format"""

    uri: str
    r"""URL for the File content"""
    mime_type: NotRequired[str]
    r"""Optional mimeType for the file"""
    name: NotRequired[str]
    r"""Optional name for the file"""


class StreamRunAgentFileAgentsResponseFileInURIFormat(BaseModel):
    r"""File in URI format. Check in the model's documentation for the supported mime types for the URI format"""

    uri: str
    r"""URL for the File content"""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""Optional mimeType for the file"""

    name: Optional[str] = None
    r"""Optional name for the file"""


class StreamRunAgentFileAgentsResponseBinaryFormatTypedDict(TypedDict):
    r"""Binary in base64 format. Check in the model's documentation for the supported mime types for the binary format."""

    bytes_: str
    r"""base64 encoded content of the file"""
    mime_type: NotRequired[str]
    r"""Optional mimeType for the file"""
    name: NotRequired[str]
    r"""Optional name for the file"""


class StreamRunAgentFileAgentsResponseBinaryFormat(BaseModel):
    r"""Binary in base64 format. Check in the model's documentation for the supported mime types for the binary format."""

    bytes_: Annotated[str, pydantic.Field(alias="bytes")]
    r"""base64 encoded content of the file"""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""Optional mimeType for the file"""

    name: Optional[str] = None
    r"""Optional name for the file"""


StreamRunAgentPartsAgentsFileTypedDict = TypeAliasType(
    "StreamRunAgentPartsAgentsFileTypedDict",
    Union[
        StreamRunAgentFileAgentsResponseBinaryFormatTypedDict,
        StreamRunAgentFileAgentsResponseFileInURIFormatTypedDict,
    ],
)


StreamRunAgentPartsAgentsFile = TypeAliasType(
    "StreamRunAgentPartsAgentsFile",
    Union[
        StreamRunAgentFileAgentsResponseBinaryFormat,
        StreamRunAgentFileAgentsResponseFileInURIFormat,
    ],
)


class StreamRunAgentPartsAgentsFilePartTypedDict(TypedDict):
    r"""A file content part that can contain either base64-encoded bytes or a URI reference. Used for images, documents, and other binary content in agent communications."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData3Kind
    file: StreamRunAgentPartsAgentsFileTypedDict
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsAgentsFilePart(BaseModel):
    r"""A file content part that can contain either base64-encoded bytes or a URI reference. Used for images, documents, and other binary content in agent communications."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyData3Kind

    file: StreamRunAgentPartsAgentsFile

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyDataKind = Literal[
    "data",
]


class StreamRunAgentPartsAgentsDataPartTypedDict(TypedDict):
    r"""A structured data part containing JSON-serializable key-value pairs. Used for passing structured information between agents and tools."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyDataKind
    data: Dict[str, Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsAgentsDataPart(BaseModel):
    r"""A structured data part containing JSON-serializable key-value pairs. Used for passing structured information between agents and tools."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyDataKind

    data: Dict[str, Any]

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyKind = Literal["text",]


class StreamRunAgentPartsAgentsTextPartTypedDict(TypedDict):
    r"""A text content part containing plain text or markdown. Used for agent messages, user input, and text-based responses."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyKind
    text: str


class StreamRunAgentPartsAgentsTextPart(BaseModel):
    r"""A text content part containing plain text or markdown. Used for agent messages, user input, and text-based responses."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamResponseBodyKind

    text: str


StreamRunAgentDataPartsTypedDict = TypeAliasType(
    "StreamRunAgentDataPartsTypedDict",
    Union[
        StreamRunAgentPartsAgentsTextPartTypedDict,
        StreamRunAgentPartsAgentsDataPartTypedDict,
        StreamRunAgentPartsAgentsFilePartTypedDict,
        StreamRunAgentPartsAgentsToolResultPartTypedDict,
        StreamRunAgentPartsAgentsToolCallPartTypedDict,
    ],
)


StreamRunAgentDataParts = TypeAliasType(
    "StreamRunAgentDataParts",
    Union[
        StreamRunAgentPartsAgentsTextPart,
        StreamRunAgentPartsAgentsDataPart,
        StreamRunAgentPartsAgentsFilePart,
        StreamRunAgentPartsAgentsToolResultPart,
        StreamRunAgentPartsAgentsToolCallPart,
    ],
)


class MessageDifferenceTypedDict(TypedDict):
    message_id: str
    role: StreamRunAgentDataRole
    parts: List[StreamRunAgentDataPartsTypedDict]
    agent_id: str
    agent_execution_id: str
    workspace_id: str
    metadata: NotRequired[Dict[str, Any]]


class MessageDifference(BaseModel):
    message_id: Annotated[str, pydantic.Field(alias="messageId")]

    role: StreamRunAgentDataRole

    parts: List[StreamRunAgentDataParts]

    agent_id: str

    agent_execution_id: str

    workspace_id: str

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentDataFinishReason = Literal[
    "stop",
    "length",
    "tool_calls",
    "content_filter",
    "function_call",
]
r"""The reason the model stopped generating tokens."""


StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody3Type = Literal[
    "function",
]


class StreamRunAgentDataAgentsFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class StreamRunAgentDataAgentsFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class DataToolCallsTypedDict(TypedDict):
    id: NotRequired[str]
    type: NotRequired[
        StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody3Type
    ]
    function: NotRequired[StreamRunAgentDataAgentsFunctionTypedDict]


class DataToolCalls(BaseModel):
    id: Optional[str] = None

    type: Optional[
        StreamRunAgentDataAgentsResponse200TextEventStreamResponseBody3Type
    ] = None

    function: Optional[StreamRunAgentDataAgentsFunction] = None


StreamRunAgentDataAgentsResponseRole = Literal["assistant",]


class DataAudioTypedDict(TypedDict):
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""

    id: str
    expires_at: int
    data: str
    transcript: str


class DataAudio(BaseModel):
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""

    id: str

    expires_at: int

    data: str

    transcript: str


class StreamRunAgentDataMessageTypedDict(TypedDict):
    r"""A chat completion message generated by the model."""

    content: NotRequired[Nullable[str]]
    refusal: NotRequired[Nullable[str]]
    tool_calls: NotRequired[List[DataToolCallsTypedDict]]
    role: NotRequired[StreamRunAgentDataAgentsResponseRole]
    reasoning: NotRequired[Nullable[str]]
    r"""Internal thought process of the model"""
    reasoning_signature: NotRequired[Nullable[str]]
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""
    redacted_reasoning: NotRequired[str]
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""
    audio: NotRequired[Nullable[DataAudioTypedDict]]
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""


class StreamRunAgentDataMessage(BaseModel):
    r"""A chat completion message generated by the model."""

    content: OptionalNullable[str] = UNSET

    refusal: OptionalNullable[str] = UNSET

    tool_calls: Optional[List[DataToolCalls]] = None

    role: Optional[StreamRunAgentDataAgentsResponseRole] = None

    reasoning: OptionalNullable[str] = UNSET
    r"""Internal thought process of the model"""

    reasoning_signature: OptionalNullable[str] = UNSET
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""

    redacted_reasoning: Optional[str] = None
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""

    audio: OptionalNullable[DataAudio] = UNSET
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "content",
            "refusal",
            "tool_calls",
            "role",
            "reasoning",
            "reasoning_signature",
            "redacted_reasoning",
            "audio",
        ]
        nullable_fields = [
            "content",
            "refusal",
            "reasoning",
            "reasoning_signature",
            "audio",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class TopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class TopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class DataContentTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[TopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class DataContent(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[TopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class DataTopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class DataTopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class RefusalTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[DataTopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class Refusal(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[DataTopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class LogprobsTypedDict(TypedDict):
    r"""Log probability information for the choice."""

    content: Nullable[List[DataContentTypedDict]]
    r"""A list of message content tokens with log probability information."""
    refusal: Nullable[List[RefusalTypedDict]]
    r"""A list of message refusal tokens with log probability information."""


class Logprobs(BaseModel):
    r"""Log probability information for the choice."""

    content: Nullable[List[DataContent]]
    r"""A list of message content tokens with log probability information."""

    refusal: Nullable[List[Refusal]]
    r"""A list of message refusal tokens with log probability information."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["content", "refusal"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ChoiceTypedDict(TypedDict):
    finish_reason: Nullable[StreamRunAgentDataFinishReason]
    r"""The reason the model stopped generating tokens."""
    message: StreamRunAgentDataMessageTypedDict
    r"""A chat completion message generated by the model."""
    index: NotRequired[float]
    r"""The index of the choice in the list of choices."""
    logprobs: NotRequired[Nullable[LogprobsTypedDict]]
    r"""Log probability information for the choice."""


class Choice(BaseModel):
    finish_reason: Nullable[StreamRunAgentDataFinishReason]
    r"""The reason the model stopped generating tokens."""

    message: StreamRunAgentDataMessage
    r"""A chat completion message generated by the model."""

    index: Optional[float] = 0
    r"""The index of the choice in the list of choices."""

    logprobs: OptionalNullable[Logprobs] = UNSET
    r"""Log probability information for the choice."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["index", "logprobs"]
        nullable_fields = ["finish_reason", "logprobs"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class StreamRunAgentDataAgentsDataTypedDict(TypedDict):
    agent_id: str
    message_difference: Dict[str, MessageDifferenceTypedDict]
    iteration: float
    accumulated_execution_time: float
    choice: NotRequired[ChoiceTypedDict]
    choice_index: NotRequired[float]


class StreamRunAgentDataAgentsData(BaseModel):
    agent_id: str

    message_difference: Dict[str, MessageDifference]

    iteration: float

    accumulated_execution_time: float

    choice: Optional[Choice] = None

    choice_index: Annotated[Optional[float], pydantic.Field(alias="choiceIndex")] = None


class Data3TypedDict(TypedDict):
    type: StreamRunAgentDataAgentsResponseType
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataAgentsDataTypedDict


class Data3(BaseModel):
    type: StreamRunAgentDataAgentsResponseType

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataAgentsData


StreamRunAgentDataAgentsType = Literal["event.agents.started",]


DataRole = Literal[
    "user",
    "agent",
    "tool",
    "system",
]
r"""Extended A2A message role"""


StreamRunAgentPartsAgentsResponse200TextEventStreamKind = Literal["tool_result",]


class StreamRunAgentPartsToolResultPartTypedDict(TypedDict):
    r"""The result of a tool execution. Contains the tool call ID for correlation and the result data from the tool invocation."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamKind
    tool_call_id: str
    result: NotRequired[Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsToolResultPart(BaseModel):
    r"""The result of a tool execution. Contains the tool call ID for correlation and the result data from the tool invocation."""

    kind: StreamRunAgentPartsAgentsResponse200TextEventStreamKind

    tool_call_id: str

    result: Optional[Any] = None

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsResponse200Kind = Literal["tool_call",]


class StreamRunAgentPartsToolCallPartTypedDict(TypedDict):
    r"""A tool invocation request from an agent. Contains the tool name, unique call ID, and arguments for the tool execution."""

    kind: StreamRunAgentPartsAgentsResponse200Kind
    tool_name: str
    tool_call_id: str
    arguments: Dict[str, Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsToolCallPart(BaseModel):
    r"""A tool invocation request from an agent. Contains the tool name, unique call ID, and arguments for the tool execution."""

    kind: StreamRunAgentPartsAgentsResponse200Kind

    tool_name: str

    tool_call_id: str

    arguments: Dict[str, Any]

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsResponseKind = Literal["file",]


class StreamRunAgentFileAgentsFileInURIFormatTypedDict(TypedDict):
    r"""File in URI format. Check in the model's documentation for the supported mime types for the URI format"""

    uri: str
    r"""URL for the File content"""
    mime_type: NotRequired[str]
    r"""Optional mimeType for the file"""
    name: NotRequired[str]
    r"""Optional name for the file"""


class StreamRunAgentFileAgentsFileInURIFormat(BaseModel):
    r"""File in URI format. Check in the model's documentation for the supported mime types for the URI format"""

    uri: str
    r"""URL for the File content"""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""Optional mimeType for the file"""

    name: Optional[str] = None
    r"""Optional name for the file"""


class StreamRunAgentFileAgentsBinaryFormatTypedDict(TypedDict):
    r"""Binary in base64 format. Check in the model's documentation for the supported mime types for the binary format."""

    bytes_: str
    r"""base64 encoded content of the file"""
    mime_type: NotRequired[str]
    r"""Optional mimeType for the file"""
    name: NotRequired[str]
    r"""Optional name for the file"""


class StreamRunAgentFileAgentsBinaryFormat(BaseModel):
    r"""Binary in base64 format. Check in the model's documentation for the supported mime types for the binary format."""

    bytes_: Annotated[str, pydantic.Field(alias="bytes")]
    r"""base64 encoded content of the file"""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""Optional mimeType for the file"""

    name: Optional[str] = None
    r"""Optional name for the file"""


StreamRunAgentPartsFileTypedDict = TypeAliasType(
    "StreamRunAgentPartsFileTypedDict",
    Union[
        StreamRunAgentFileAgentsBinaryFormatTypedDict,
        StreamRunAgentFileAgentsFileInURIFormatTypedDict,
    ],
)


StreamRunAgentPartsFile = TypeAliasType(
    "StreamRunAgentPartsFile",
    Union[
        StreamRunAgentFileAgentsBinaryFormat, StreamRunAgentFileAgentsFileInURIFormat
    ],
)


class StreamRunAgentPartsFilePartTypedDict(TypedDict):
    r"""A file content part that can contain either base64-encoded bytes or a URI reference. Used for images, documents, and other binary content in agent communications."""

    kind: StreamRunAgentPartsAgentsResponseKind
    file: StreamRunAgentPartsFileTypedDict
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsFilePart(BaseModel):
    r"""A file content part that can contain either base64-encoded bytes or a URI reference. Used for images, documents, and other binary content in agent communications."""

    kind: StreamRunAgentPartsAgentsResponseKind

    file: StreamRunAgentPartsFile

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsAgentsKind = Literal["data",]


class StreamRunAgentPartsDataPartTypedDict(TypedDict):
    r"""A structured data part containing JSON-serializable key-value pairs. Used for passing structured information between agents and tools."""

    kind: StreamRunAgentPartsAgentsKind
    data: Dict[str, Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPartsDataPart(BaseModel):
    r"""A structured data part containing JSON-serializable key-value pairs. Used for passing structured information between agents and tools."""

    kind: StreamRunAgentPartsAgentsKind

    data: Dict[str, Any]

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPartsKind = Literal["text",]


class StreamRunAgentPartsTextPartTypedDict(TypedDict):
    r"""A text content part containing plain text or markdown. Used for agent messages, user input, and text-based responses."""

    kind: StreamRunAgentPartsKind
    text: str


class StreamRunAgentPartsTextPart(BaseModel):
    r"""A text content part containing plain text or markdown. Used for agent messages, user input, and text-based responses."""

    kind: StreamRunAgentPartsKind

    text: str


DataPartsTypedDict = TypeAliasType(
    "DataPartsTypedDict",
    Union[
        StreamRunAgentPartsTextPartTypedDict,
        StreamRunAgentPartsDataPartTypedDict,
        StreamRunAgentPartsFilePartTypedDict,
        StreamRunAgentPartsToolResultPartTypedDict,
        StreamRunAgentPartsToolCallPartTypedDict,
    ],
)


DataParts = TypeAliasType(
    "DataParts",
    Union[
        StreamRunAgentPartsTextPart,
        StreamRunAgentPartsDataPart,
        StreamRunAgentPartsFilePart,
        StreamRunAgentPartsToolResultPart,
        StreamRunAgentPartsToolCallPart,
    ],
)


class InputMessageTypedDict(TypedDict):
    role: DataRole
    r"""Extended A2A message role"""
    parts: List[DataPartsTypedDict]
    message_id: NotRequired[str]
    metadata: NotRequired[Dict[str, Any]]


class InputMessage(BaseModel):
    role: DataRole
    r"""Extended A2A message role"""

    parts: List[DataParts]

    message_id: Annotated[Optional[str], pydantic.Field(alias="messageId")] = None

    metadata: Optional[Dict[str, Any]] = None


DataToolApprovalRequired = Literal[
    "all",
    "respect_tool",
    "none",
]
r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""


class StreamRunAgentDataConditionsTypedDict(TypedDict):
    condition: str
    r"""The argument of the tool call to evaluate"""
    operator: str
    r"""The operator to use"""
    value: str
    r"""The value to compare against"""


class StreamRunAgentDataConditions(BaseModel):
    condition: str
    r"""The argument of the tool call to evaluate"""

    operator: str
    r"""The operator to use"""

    value: str
    r"""The value to compare against"""


class StreamRunAgentDataToolsTypedDict(TypedDict):
    id: str
    r"""The id of the resource"""
    action_type: str
    key: NotRequired[str]
    r"""Optional tool key for custom tools"""
    display_name: NotRequired[str]
    description: NotRequired[str]
    r"""Optional tool description"""
    requires_approval: NotRequired[bool]
    tool_id: NotRequired[str]
    r"""Nested tool ID for MCP tools (identifies specific tool within MCP server)"""
    conditions: NotRequired[List[StreamRunAgentDataConditionsTypedDict]]
    timeout: NotRequired[float]
    r"""Tool execution timeout in seconds (default: 2 minutes, max: 10 minutes)"""


class StreamRunAgentDataTools(BaseModel):
    id: str
    r"""The id of the resource"""

    action_type: str

    key: Optional[str] = None
    r"""Optional tool key for custom tools"""

    display_name: Optional[str] = None

    description: Optional[str] = None
    r"""Optional tool description"""

    requires_approval: Optional[bool] = False

    tool_id: Optional[str] = None
    r"""Nested tool ID for MCP tools (identifies specific tool within MCP server)"""

    conditions: Optional[List[StreamRunAgentDataConditions]] = None

    timeout: Optional[float] = 120
    r"""Tool execution timeout in seconds (default: 2 minutes, max: 10 minutes)"""


DataExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class DataEvaluatorsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: DataExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class DataEvaluators(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: DataExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


StreamRunAgentDataExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class DataGuardrailsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: StreamRunAgentDataExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class DataGuardrails(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: StreamRunAgentDataExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class DataSettingsTypedDict(TypedDict):
    max_iterations: NotRequired[int]
    r"""Maximum iterations(llm calls) before the agent will stop executing."""
    max_execution_time: NotRequired[int]
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""
    tool_approval_required: NotRequired[DataToolApprovalRequired]
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""
    tools: NotRequired[List[StreamRunAgentDataToolsTypedDict]]
    evaluators: NotRequired[List[DataEvaluatorsTypedDict]]
    r"""Configuration for an evaluator applied to the agent"""
    guardrails: NotRequired[List[DataGuardrailsTypedDict]]
    r"""Configuration for a guardrail applied to the agent"""


class DataSettings(BaseModel):
    max_iterations: Optional[int] = 15
    r"""Maximum iterations(llm calls) before the agent will stop executing."""

    max_execution_time: Optional[int] = 300
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""

    tool_approval_required: Optional[DataToolApprovalRequired] = "respect_tool"
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""

    tools: Optional[List[StreamRunAgentDataTools]] = None

    evaluators: Optional[List[DataEvaluators]] = None
    r"""Configuration for an evaluator applied to the agent"""

    guardrails: Optional[List[DataGuardrails]] = None
    r"""Configuration for a guardrail applied to the agent"""


class StreamRunAgentDataDataTypedDict(TypedDict):
    input_message: InputMessageTypedDict
    model_id: str
    instructions: str
    system_prompt: str
    agent_manifest_id: str
    agent_key: str
    integration_id: NotRequired[str]
    settings: NotRequired[DataSettingsTypedDict]
    variables: NotRequired[Dict[str, Any]]
    tool_execution_id: NotRequired[str]
    is_continuation: NotRequired[bool]
    stream: NotRequired[bool]


class StreamRunAgentDataData(BaseModel):
    input_message: Annotated[InputMessage, pydantic.Field(alias="inputMessage")]

    model_id: Annotated[str, pydantic.Field(alias="modelId")]

    instructions: str

    system_prompt: str

    agent_manifest_id: str

    agent_key: str

    integration_id: Optional[str] = None

    settings: Optional[DataSettings] = None

    variables: Optional[Dict[str, Any]] = None

    tool_execution_id: Optional[str] = None

    is_continuation: Optional[bool] = None

    stream: Optional[bool] = None


class Data2TypedDict(TypedDict):
    type: StreamRunAgentDataAgentsType
    timestamp: str
    r"""ISO timestamp of the event"""
    data: StreamRunAgentDataDataTypedDict


class Data2(BaseModel):
    type: StreamRunAgentDataAgentsType

    timestamp: str
    r"""ISO timestamp of the event"""

    data: StreamRunAgentDataData


StreamRunAgentDataType = Literal["agents.execution_started",]


class DataDataTypedDict(TypedDict):
    agent_task_id: str
    r"""Agent execution task ID (ULID)"""
    workspace_id: str
    r"""Workspace ID"""
    trace_id: str
    r"""Trace ID for the workflow run"""


class DataData(BaseModel):
    agent_task_id: str
    r"""Agent execution task ID (ULID)"""

    workspace_id: str
    r"""Workspace ID"""

    trace_id: str
    r"""Trace ID for the workflow run"""


class Data1TypedDict(TypedDict):
    type: StreamRunAgentDataType
    timestamp: str
    r"""ISO timestamp of the event"""
    data: DataDataTypedDict


class Data1(BaseModel):
    type: StreamRunAgentDataType

    timestamp: str
    r"""ISO timestamp of the event"""

    data: DataData


StreamRunAgentDataTypedDict = TypeAliasType(
    "StreamRunAgentDataTypedDict",
    Union[
        Data1TypedDict,
        Data2TypedDict,
        Data3TypedDict,
        Data4TypedDict,
        StreamRunAgentData5TypedDict,
        StreamRunAgentData6TypedDict,
        SevenTypedDict,
        EightTypedDict,
        NineTypedDict,
        TenTypedDict,
        ElevenTypedDict,
        TwelveTypedDict,
        ThirteenTypedDict,
        FourteenTypedDict,
        FifteenTypedDict,
        SixteenTypedDict,
        SeventeenTypedDict,
    ],
)


StreamRunAgentData = TypeAliasType(
    "StreamRunAgentData",
    Union[
        Data1,
        Data2,
        Data3,
        Data4,
        StreamRunAgentData5,
        StreamRunAgentData6,
        Seven,
        Eight,
        Nine,
        Ten,
        Eleven,
        Twelve,
        Thirteen,
        Fourteen,
        Fifteen,
        Sixteen,
        Seventeen,
    ],
)


class StreamRunAgentResponseBodyTypedDict(TypedDict):
    r"""SSE stream of agent events"""

    data: StreamRunAgentDataTypedDict


class StreamRunAgentResponseBody(BaseModel):
    r"""SSE stream of agent events"""

    data: StreamRunAgentData
