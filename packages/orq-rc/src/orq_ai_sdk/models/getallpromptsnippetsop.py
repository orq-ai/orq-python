"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
import dateutil.parser
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import FieldMetadata, HeaderMetadata, QueryParamMetadata
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class GetAllPromptSnippetsGlobalsTypedDict(TypedDict):
    contact_id: NotRequired[str]


class GetAllPromptSnippetsGlobals(BaseModel):
    contact_id: Annotated[
        Optional[str],
        pydantic.Field(alias="contactId"),
        FieldMetadata(header=HeaderMetadata(style="simple", explode=False)),
    ] = None


class GetAllPromptSnippetsRequestTypedDict(TypedDict):
    limit: NotRequired[float]
    r"""A limit on the number of objects to be returned. Limit can range between 1 and 50, and the default is 10"""
    starting_after: NotRequired[str]
    r"""A cursor for use in pagination. `starting_after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 20 objects, ending with `01JJ1HDHN79XAS7A01WB3HYSDB`, your subsequent call can include `after=01JJ1HDHN79XAS7A01WB3HYSDB` in order to fetch the next page of the list."""
    ending_before: NotRequired[str]
    r"""A cursor for use in pagination. `ending_before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 20 objects, starting with `01JJ1HDHN79XAS7A01WB3HYSDB`, your subsequent call can include `before=01JJ1HDHN79XAS7A01WB3HYSDB` in order to fetch the previous page of the list."""


class GetAllPromptSnippetsRequest(BaseModel):
    limit: Annotated[
        Optional[float],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 10
    r"""A limit on the number of objects to be returned. Limit can range between 1 and 50, and the default is 10"""

    starting_after: Annotated[
        Optional[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = None
    r"""A cursor for use in pagination. `starting_after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 20 objects, ending with `01JJ1HDHN79XAS7A01WB3HYSDB`, your subsequent call can include `after=01JJ1HDHN79XAS7A01WB3HYSDB` in order to fetch the next page of the list."""

    ending_before: Annotated[
        Optional[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = None
    r"""A cursor for use in pagination. `ending_before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 20 objects, starting with `01JJ1HDHN79XAS7A01WB3HYSDB`, your subsequent call can include `before=01JJ1HDHN79XAS7A01WB3HYSDB` in order to fetch the previous page of the list."""


GetAllPromptSnippetsObject = Literal["list"]

GetAllPromptSnippetsOwner2 = Literal["vendor"]

GetAllPromptSnippetsOwnerTypedDict = TypeAliasType(
    "GetAllPromptSnippetsOwnerTypedDict", Union[str, GetAllPromptSnippetsOwner2]
)


GetAllPromptSnippetsOwner = TypeAliasType(
    "GetAllPromptSnippetsOwner", Union[str, GetAllPromptSnippetsOwner2]
)


GetAllPromptSnippetsModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

GetAllPromptSnippetsFormat = Literal["url", "b64_json", "text", "json_object"]
r"""Only supported on `image` models."""

GetAllPromptSnippetsQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

GetAllPromptSnippetsResponseFormatPromptSnippetsType = Literal["json_object"]


class GetAllPromptSnippetsResponseFormat2TypedDict(TypedDict):
    type: GetAllPromptSnippetsResponseFormatPromptSnippetsType


class GetAllPromptSnippetsResponseFormat2(BaseModel):
    type: GetAllPromptSnippetsResponseFormatPromptSnippetsType


GetAllPromptSnippetsResponseFormatType = Literal["json_schema"]


class GetAllPromptSnippetsResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptSnippetsResponseFormatJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptSnippetsResponseFormat1TypedDict(TypedDict):
    type: GetAllPromptSnippetsResponseFormatType
    json_schema: GetAllPromptSnippetsResponseFormatJSONSchemaTypedDict


class GetAllPromptSnippetsResponseFormat1(BaseModel):
    type: GetAllPromptSnippetsResponseFormatType

    json_schema: GetAllPromptSnippetsResponseFormatJSONSchema


GetAllPromptSnippetsResponseFormatTypedDict = TypeAliasType(
    "GetAllPromptSnippetsResponseFormatTypedDict",
    Union[
        GetAllPromptSnippetsResponseFormat2TypedDict,
        GetAllPromptSnippetsResponseFormat1TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptSnippetsResponseFormat = TypeAliasType(
    "GetAllPromptSnippetsResponseFormat",
    Union[GetAllPromptSnippetsResponseFormat2, GetAllPromptSnippetsResponseFormat1],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptSnippetsPhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

GetAllPromptSnippetsEncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""

GetAllPromptSnippetsReasoningEffort = Literal["low", "medium", "high"]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class GetAllPromptSnippetsModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[GetAllPromptSnippetsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptSnippetsQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[Nullable[GetAllPromptSnippetsResponseFormatTypedDict]]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[GetAllPromptSnippetsPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[GetAllPromptSnippetsEncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[GetAllPromptSnippetsReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class GetAllPromptSnippetsModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[
        Optional[GetAllPromptSnippetsFormat], pydantic.Field(alias="format")
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptSnippetsQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[GetAllPromptSnippetsResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptSnippetsPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[GetAllPromptSnippetsEncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[GetAllPromptSnippetsReasoningEffort],
        pydantic.Field(alias="reasoningEffort"),
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptSnippetsProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
]

GetAllPromptSnippetsRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

GetAllPromptSnippets2PromptSnippetsType = Literal["image_url"]


class GetAllPromptSnippets2ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptSnippets2ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptSnippets22TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptSnippets2PromptSnippetsType
    image_url: GetAllPromptSnippets2ImageURLTypedDict


class GetAllPromptSnippets22(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptSnippets2PromptSnippetsType

    image_url: GetAllPromptSnippets2ImageURL


GetAllPromptSnippets2Type = Literal["text"]


class GetAllPromptSnippets21TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPromptSnippets2Type
    text: str


class GetAllPromptSnippets21(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPromptSnippets2Type

    text: str


GetAllPromptSnippetsContent2TypedDict = TypeAliasType(
    "GetAllPromptSnippetsContent2TypedDict",
    Union[GetAllPromptSnippets21TypedDict, GetAllPromptSnippets22TypedDict],
)


GetAllPromptSnippetsContent2 = TypeAliasType(
    "GetAllPromptSnippetsContent2",
    Union[GetAllPromptSnippets21, GetAllPromptSnippets22],
)


GetAllPromptSnippetsContentTypedDict = TypeAliasType(
    "GetAllPromptSnippetsContentTypedDict",
    Union[str, List[GetAllPromptSnippetsContent2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptSnippetsContent = TypeAliasType(
    "GetAllPromptSnippetsContent", Union[str, List[GetAllPromptSnippetsContent2]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptSnippetsPromptSnippetsType = Literal["function"]


class GetAllPromptSnippetsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptSnippetsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptSnippetsToolCallsTypedDict(TypedDict):
    type: GetAllPromptSnippetsPromptSnippetsType
    function: GetAllPromptSnippetsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptSnippetsToolCalls(BaseModel):
    type: GetAllPromptSnippetsPromptSnippetsType

    function: GetAllPromptSnippetsFunction

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptSnippetsMessagesTypedDict(TypedDict):
    role: GetAllPromptSnippetsRole
    r"""The role of the prompt message"""
    content: GetAllPromptSnippetsContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[GetAllPromptSnippetsToolCallsTypedDict]]


class GetAllPromptSnippetsMessages(BaseModel):
    role: GetAllPromptSnippetsRole
    r"""The role of the prompt message"""

    content: GetAllPromptSnippetsContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[GetAllPromptSnippetsToolCalls]] = None


class GetAllPromptSnippetsPromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[GetAllPromptSnippetsMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[GetAllPromptSnippetsModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[GetAllPromptSnippetsModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptSnippetsProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptSnippetsPromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[GetAllPromptSnippetsMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[GetAllPromptSnippetsModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[GetAllPromptSnippetsModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptSnippetsProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptSnippetsUseCases = Literal[
    "Agents",
    "Agents simulations",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Documents QA",
    "Conversation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "SQL",
    "Summarization",
    "Tagging",
]

GetAllPromptSnippetsLanguage = Literal[
    "Chinese", "Dutch", "English", "French", "German", "Russian", "Spanish"
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class GetAllPromptSnippetsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[GetAllPromptSnippetsUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[GetAllPromptSnippetsLanguage]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class GetAllPromptSnippetsMetadata(BaseModel):
    use_cases: Optional[List[GetAllPromptSnippetsUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: Optional[GetAllPromptSnippetsLanguage] = None
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


GetAllPromptSnippetsType = Literal["snippet"]

GetAllPromptSnippetsPromptSnippetsModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

GetAllPromptSnippetsPromptSnippetsFormat = Literal[
    "url", "b64_json", "text", "json_object"
]
r"""Only supported on `image` models."""

GetAllPromptSnippetsPromptSnippetsQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

GetAllPromptSnippetsResponseFormatPromptSnippetsResponse200Type = Literal["json_object"]


class GetAllPromptSnippetsResponseFormatPromptSnippets2TypedDict(TypedDict):
    type: GetAllPromptSnippetsResponseFormatPromptSnippetsResponse200Type


class GetAllPromptSnippetsResponseFormatPromptSnippets2(BaseModel):
    type: GetAllPromptSnippetsResponseFormatPromptSnippetsResponse200Type


GetAllPromptSnippetsResponseFormatPromptSnippetsResponseType = Literal["json_schema"]


class GetAllPromptSnippetsResponseFormatPromptSnippetsJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptSnippetsResponseFormatPromptSnippetsJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptSnippetsResponseFormatPromptSnippets1TypedDict(TypedDict):
    type: GetAllPromptSnippetsResponseFormatPromptSnippetsResponseType
    json_schema: GetAllPromptSnippetsResponseFormatPromptSnippetsJSONSchemaTypedDict


class GetAllPromptSnippetsResponseFormatPromptSnippets1(BaseModel):
    type: GetAllPromptSnippetsResponseFormatPromptSnippetsResponseType

    json_schema: GetAllPromptSnippetsResponseFormatPromptSnippetsJSONSchema


GetAllPromptSnippetsPromptSnippetsResponseFormatTypedDict = TypeAliasType(
    "GetAllPromptSnippetsPromptSnippetsResponseFormatTypedDict",
    Union[
        GetAllPromptSnippetsResponseFormatPromptSnippets2TypedDict,
        GetAllPromptSnippetsResponseFormatPromptSnippets1TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptSnippetsPromptSnippetsResponseFormat = TypeAliasType(
    "GetAllPromptSnippetsPromptSnippetsResponseFormat",
    Union[
        GetAllPromptSnippetsResponseFormatPromptSnippets2,
        GetAllPromptSnippetsResponseFormatPromptSnippets1,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptSnippetsPromptSnippetsPhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

GetAllPromptSnippetsPromptSnippetsEncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""

GetAllPromptSnippetsPromptSnippetsReasoningEffort = Literal["low", "medium", "high"]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class GetAllPromptSnippetsPromptSnippetsModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[GetAllPromptSnippetsPromptSnippetsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptSnippetsPromptSnippetsQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[GetAllPromptSnippetsPromptSnippetsResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[GetAllPromptSnippetsPromptSnippetsPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[GetAllPromptSnippetsPromptSnippetsEncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[GetAllPromptSnippetsPromptSnippetsReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class GetAllPromptSnippetsPromptSnippetsModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[
        Optional[GetAllPromptSnippetsPromptSnippetsFormat],
        pydantic.Field(alias="format"),
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptSnippetsPromptSnippetsQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[GetAllPromptSnippetsPromptSnippetsResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptSnippetsPromptSnippetsPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[GetAllPromptSnippetsPromptSnippetsEncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[GetAllPromptSnippetsPromptSnippetsReasoningEffort],
        pydantic.Field(alias="reasoningEffort"),
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptSnippetsPromptSnippetsProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
]

GetAllPromptSnippetsPromptSnippetsRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

GetAllPromptSnippets2PromptSnippetsResponse200Type = Literal["image_url"]


class GetAllPromptSnippets2PromptSnippetsImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptSnippets2PromptSnippetsImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptSnippets2PromptSnippets2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptSnippets2PromptSnippetsResponse200Type
    image_url: GetAllPromptSnippets2PromptSnippetsImageURLTypedDict


class GetAllPromptSnippets2PromptSnippets2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptSnippets2PromptSnippetsResponse200Type

    image_url: GetAllPromptSnippets2PromptSnippetsImageURL


GetAllPromptSnippets2PromptSnippetsResponseType = Literal["text"]


class GetAllPromptSnippets2PromptSnippets1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPromptSnippets2PromptSnippetsResponseType
    text: str


class GetAllPromptSnippets2PromptSnippets1(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPromptSnippets2PromptSnippetsResponseType

    text: str


GetAllPromptSnippetsContentPromptSnippets2TypedDict = TypeAliasType(
    "GetAllPromptSnippetsContentPromptSnippets2TypedDict",
    Union[
        GetAllPromptSnippets2PromptSnippets1TypedDict,
        GetAllPromptSnippets2PromptSnippets2TypedDict,
    ],
)


GetAllPromptSnippetsContentPromptSnippets2 = TypeAliasType(
    "GetAllPromptSnippetsContentPromptSnippets2",
    Union[GetAllPromptSnippets2PromptSnippets1, GetAllPromptSnippets2PromptSnippets2],
)


GetAllPromptSnippetsPromptSnippetsContentTypedDict = TypeAliasType(
    "GetAllPromptSnippetsPromptSnippetsContentTypedDict",
    Union[str, List[GetAllPromptSnippetsContentPromptSnippets2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptSnippetsPromptSnippetsContent = TypeAliasType(
    "GetAllPromptSnippetsPromptSnippetsContent",
    Union[str, List[GetAllPromptSnippetsContentPromptSnippets2]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptSnippetsPromptSnippetsResponseType = Literal["function"]


class GetAllPromptSnippetsPromptSnippetsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptSnippetsPromptSnippetsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptSnippetsPromptSnippetsToolCallsTypedDict(TypedDict):
    type: GetAllPromptSnippetsPromptSnippetsResponseType
    function: GetAllPromptSnippetsPromptSnippetsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptSnippetsPromptSnippetsToolCalls(BaseModel):
    type: GetAllPromptSnippetsPromptSnippetsResponseType

    function: GetAllPromptSnippetsPromptSnippetsFunction

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptSnippetsPromptSnippetsMessagesTypedDict(TypedDict):
    role: GetAllPromptSnippetsPromptSnippetsRole
    r"""The role of the prompt message"""
    content: GetAllPromptSnippetsPromptSnippetsContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[GetAllPromptSnippetsPromptSnippetsToolCallsTypedDict]]


class GetAllPromptSnippetsPromptSnippetsMessages(BaseModel):
    role: GetAllPromptSnippetsPromptSnippetsRole
    r"""The role of the prompt message"""

    content: GetAllPromptSnippetsPromptSnippetsContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[GetAllPromptSnippetsPromptSnippetsToolCalls]] = None


class GetAllPromptSnippetsPromptSnippetsPromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[GetAllPromptSnippetsPromptSnippetsMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[GetAllPromptSnippetsPromptSnippetsModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[
        GetAllPromptSnippetsPromptSnippetsModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptSnippetsPromptSnippetsProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptSnippetsPromptSnippetsPromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[GetAllPromptSnippetsPromptSnippetsMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[GetAllPromptSnippetsPromptSnippetsModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[GetAllPromptSnippetsPromptSnippetsModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptSnippetsPromptSnippetsProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptSnippetsPromptSnippetsUseCases = Literal[
    "Agents",
    "Agents simulations",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Documents QA",
    "Conversation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "SQL",
    "Summarization",
    "Tagging",
]

GetAllPromptSnippetsPromptSnippetsLanguage = Literal[
    "Chinese", "Dutch", "English", "French", "German", "Russian", "Spanish"
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class GetAllPromptSnippetsPromptSnippetsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[GetAllPromptSnippetsPromptSnippetsUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[GetAllPromptSnippetsPromptSnippetsLanguage]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class GetAllPromptSnippetsPromptSnippetsMetadata(BaseModel):
    use_cases: Optional[List[GetAllPromptSnippetsPromptSnippetsUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: Optional[GetAllPromptSnippetsPromptSnippetsLanguage] = None
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class GetAllPromptSnippetsVersionsTypedDict(TypedDict):
    r"""Prompt version model returned from the API"""

    id: str
    prompt_config: GetAllPromptSnippetsPromptSnippetsPromptConfigTypedDict
    r"""A list of messages compatible with the openAI schema"""
    metadata: GetAllPromptSnippetsPromptSnippetsMetadataTypedDict
    created_by_id: str
    updated_by_id: str
    timestamp: str
    description: NotRequired[Nullable[str]]
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""


class GetAllPromptSnippetsVersions(BaseModel):
    r"""Prompt version model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    prompt_config: GetAllPromptSnippetsPromptSnippetsPromptConfig
    r"""A list of messages compatible with the openAI schema"""

    metadata: GetAllPromptSnippetsPromptSnippetsMetadata

    created_by_id: str

    updated_by_id: str

    timestamp: str

    description: OptionalNullable[str] = UNSET
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptSnippetsDataTypedDict(TypedDict):
    r"""Prompt snippet model returned from the API"""

    id: str
    owner: GetAllPromptSnippetsOwnerTypedDict
    domain_id: str
    key: str
    prompt_config: GetAllPromptSnippetsPromptConfigTypedDict
    r"""A list of messages compatible with the openAI schema"""
    metadata: GetAllPromptSnippetsMetadataTypedDict
    created_by_id: str
    updated_by_id: str
    type: GetAllPromptSnippetsType
    versions: List[GetAllPromptSnippetsVersionsTypedDict]
    description: NotRequired[Nullable[str]]
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""
    created: NotRequired[datetime]
    r"""The date and time the resource was created"""
    updated: NotRequired[datetime]
    r"""The date and time the resource was last updated"""


class GetAllPromptSnippetsData(BaseModel):
    r"""Prompt snippet model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    owner: GetAllPromptSnippetsOwner

    domain_id: str

    key: str

    prompt_config: GetAllPromptSnippetsPromptConfig
    r"""A list of messages compatible with the openAI schema"""

    metadata: GetAllPromptSnippetsMetadata

    created_by_id: str

    updated_by_id: str

    type: GetAllPromptSnippetsType

    versions: List[GetAllPromptSnippetsVersions]

    description: OptionalNullable[str] = UNSET
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    created: Optional[datetime] = None
    r"""The date and time the resource was created"""

    updated: Optional[datetime] = dateutil.parser.isoparse("2025-02-06T21:24:42.771Z")
    r"""The date and time the resource was last updated"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "created", "updated"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptSnippetsResponseBodyTypedDict(TypedDict):
    r"""Prompt snippets retrieved."""

    object: GetAllPromptSnippetsObject
    data: List[GetAllPromptSnippetsDataTypedDict]
    has_more: bool


class GetAllPromptSnippetsResponseBody(BaseModel):
    r"""Prompt snippets retrieved."""

    object: GetAllPromptSnippetsObject

    data: List[GetAllPromptSnippetsData]

    has_more: bool
