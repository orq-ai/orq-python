"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .datapart import DataPart, DataPartTypedDict
from .errorpart import ErrorPart, ErrorPartTypedDict
from .filepart import FilePart, FilePartTypedDict
from .textpart import TextPart, TextPartTypedDict
from .thinkingconfigdisabledschema import (
    ThinkingConfigDisabledSchema,
    ThinkingConfigDisabledSchemaTypedDict,
)
from .thinkingconfigenabledschema import (
    ThinkingConfigEnabledSchema,
    ThinkingConfigEnabledSchemaTypedDict,
)
from .toolcallpart import ToolCallPart, ToolCallPartTypedDict
from .toolresultpart import ToolResultPart, ToolResultPartTypedDict
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import get_discriminator
import pydantic
from pydantic import ConfigDict, Discriminator, Tag, model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import (
    Annotated,
    NotRequired,
    TypeAliasType,
    TypedDict,
    deprecated,
)


RunAgentModelConfigurationVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


RunAgentModelConfigurationFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class RunAgentModelConfigurationAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: RunAgentModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: RunAgentModelConfigurationFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class RunAgentModelConfigurationAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: RunAgentModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[RunAgentModelConfigurationFormat, pydantic.Field(alias="format")]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


RunAgentResponseFormatAgentsRequestType = Literal["json_schema",]


class RunAgentResponseFormatAgentsJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class RunAgentResponseFormatAgentsJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = False
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "schema", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentResponseFormatJSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: RunAgentResponseFormatAgentsRequestType
    json_schema: RunAgentResponseFormatAgentsJSONSchemaTypedDict


class RunAgentResponseFormatJSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: RunAgentResponseFormatAgentsRequestType

    json_schema: RunAgentResponseFormatAgentsJSONSchema


RunAgentResponseFormatAgentsType = Literal["json_object",]


class RunAgentResponseFormatJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: RunAgentResponseFormatAgentsType


class RunAgentResponseFormatJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: RunAgentResponseFormatAgentsType


RunAgentResponseFormatType = Literal["text",]


class RunAgentResponseFormatTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: RunAgentResponseFormatType


class RunAgentResponseFormatText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: RunAgentResponseFormatType


RunAgentModelConfigurationResponseFormatTypedDict = TypeAliasType(
    "RunAgentModelConfigurationResponseFormatTypedDict",
    Union[
        RunAgentResponseFormatTextTypedDict,
        RunAgentResponseFormatJSONObjectTypedDict,
        RunAgentResponseFormatJSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


RunAgentModelConfigurationResponseFormat = Annotated[
    Union[
        Annotated[RunAgentResponseFormatText, Tag("text")],
        Annotated[RunAgentResponseFormatJSONObject, Tag("json_object")],
        Annotated[RunAgentResponseFormatJSONSchema, Tag("json_schema")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An object specifying the format that the model must output"""


RunAgentModelConfigurationReasoningEffort = Literal[
    "none",
    "minimal",
    "low",
    "medium",
    "high",
    "xhigh",
]
r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

- `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
- All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
- The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
- `xhigh` is currently only supported for `gpt-5.1-codex-max`.

Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
"""


RunAgentModelConfigurationStopTypedDict = TypeAliasType(
    "RunAgentModelConfigurationStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


RunAgentModelConfigurationStop = TypeAliasType(
    "RunAgentModelConfigurationStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class RunAgentModelConfigurationStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class RunAgentModelConfigurationStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentModelConfigurationThinkingTypedDict = TypeAliasType(
    "RunAgentModelConfigurationThinkingTypedDict",
    Union[ThinkingConfigDisabledSchemaTypedDict, ThinkingConfigEnabledSchemaTypedDict],
)


RunAgentModelConfigurationThinking = Annotated[
    Union[
        Annotated[ThinkingConfigDisabledSchema, Tag("disabled")],
        Annotated[ThinkingConfigEnabledSchema, Tag("enabled")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


RunAgentToolChoiceType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class RunAgentToolChoiceFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to call."""


class RunAgentToolChoiceFunction(BaseModel):
    name: str
    r"""The name of the function to call."""


class RunAgentToolChoice2TypedDict(TypedDict):
    function: RunAgentToolChoiceFunctionTypedDict
    type: NotRequired[RunAgentToolChoiceType]
    r"""The type of the tool. Currently, only function is supported."""


class RunAgentToolChoice2(BaseModel):
    function: RunAgentToolChoiceFunction

    type: Optional[RunAgentToolChoiceType] = None
    r"""The type of the tool. Currently, only function is supported."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentToolChoice1 = Literal[
    "none",
    "auto",
    "required",
]


RunAgentModelConfigurationToolChoiceTypedDict = TypeAliasType(
    "RunAgentModelConfigurationToolChoiceTypedDict",
    Union[RunAgentToolChoice2TypedDict, RunAgentToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


RunAgentModelConfigurationToolChoice = TypeAliasType(
    "RunAgentModelConfigurationToolChoice",
    Union[RunAgentToolChoice2, RunAgentToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


RunAgentModelConfigurationModalities = Literal[
    "text",
    "audio",
]


RunAgentID1 = Literal[
    "orq_pii_detection",
    "orq_sexual_moderation",
    "orq_harmful_moderation",
]
r"""The key of the guardrail."""


RunAgentModelConfigurationIDTypedDict = TypeAliasType(
    "RunAgentModelConfigurationIDTypedDict", Union[RunAgentID1, str]
)


RunAgentModelConfigurationID = TypeAliasType(
    "RunAgentModelConfigurationID", Union[RunAgentID1, str]
)


RunAgentModelConfigurationExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RunAgentModelConfigurationGuardrailsTypedDict(TypedDict):
    id: RunAgentModelConfigurationIDTypedDict
    execute_on: RunAgentModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RunAgentModelConfigurationGuardrails(BaseModel):
    id: RunAgentModelConfigurationID

    execute_on: RunAgentModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RunAgentModelConfigurationFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class RunAgentModelConfigurationFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


class RunAgentModelConfigurationRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class RunAgentModelConfigurationRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentModelConfigurationType = Literal["exact_match",]


class RunAgentModelConfigurationCacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: RunAgentModelConfigurationType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class RunAgentModelConfigurationCache(BaseModel):
    r"""Cache configuration for the request."""

    type: RunAgentModelConfigurationType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentLoadBalancerType = Literal["weight_based",]


class RunAgentLoadBalancerModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class RunAgentLoadBalancerModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentLoadBalancer1TypedDict(TypedDict):
    type: RunAgentLoadBalancerType
    models: List[RunAgentLoadBalancerModelsTypedDict]


class RunAgentLoadBalancer1(BaseModel):
    type: RunAgentLoadBalancerType

    models: List[RunAgentLoadBalancerModels]


RunAgentModelConfigurationLoadBalancerTypedDict = RunAgentLoadBalancer1TypedDict
r"""Load balancer configuration for the request."""


RunAgentModelConfigurationLoadBalancer = RunAgentLoadBalancer1
r"""Load balancer configuration for the request."""


class RunAgentModelConfigurationTimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class RunAgentModelConfigurationTimeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class RunAgentModelConfigurationParametersTypedDict(TypedDict):
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""

    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    audio: NotRequired[Nullable[RunAgentModelConfigurationAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[RunAgentModelConfigurationResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[RunAgentModelConfigurationReasoningEffort]
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[RunAgentModelConfigurationStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[
        Nullable[RunAgentModelConfigurationStreamOptionsTypedDict]
    ]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[RunAgentModelConfigurationThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[RunAgentModelConfigurationToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[Nullable[List[RunAgentModelConfigurationModalities]]]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    guardrails: NotRequired[List[RunAgentModelConfigurationGuardrailsTypedDict]]
    r"""A list of guardrails to apply to the request."""
    fallbacks: NotRequired[List[RunAgentModelConfigurationFallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    retry: NotRequired[RunAgentModelConfigurationRetryTypedDict]
    r"""Retry configuration for the request"""
    cache: NotRequired[RunAgentModelConfigurationCacheTypedDict]
    r"""Cache configuration for the request."""
    load_balancer: NotRequired[RunAgentModelConfigurationLoadBalancerTypedDict]
    r"""Load balancer configuration for the request."""
    timeout: NotRequired[RunAgentModelConfigurationTimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


class RunAgentModelConfigurationParameters(BaseModel):
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    audio: OptionalNullable[RunAgentModelConfigurationAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[RunAgentModelConfigurationResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[RunAgentModelConfigurationReasoningEffort] = None
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[RunAgentModelConfigurationStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[RunAgentModelConfigurationStreamOptions] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[RunAgentModelConfigurationThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[RunAgentModelConfigurationToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[RunAgentModelConfigurationModalities]] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    guardrails: Optional[List[RunAgentModelConfigurationGuardrails]] = None
    r"""A list of guardrails to apply to the request."""

    fallbacks: Optional[List[RunAgentModelConfigurationFallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    retry: Optional[RunAgentModelConfigurationRetry] = None
    r"""Retry configuration for the request"""

    cache: Optional[RunAgentModelConfigurationCache] = None
    r"""Cache configuration for the request."""

    load_balancer: Optional[RunAgentModelConfigurationLoadBalancer] = None
    r"""Load balancer configuration for the request."""

    timeout: Optional[RunAgentModelConfigurationTimeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "name",
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "response_format",
                "reasoning_effort",
                "verbosity",
                "seed",
                "stop",
                "stream_options",
                "thinking",
                "temperature",
                "top_p",
                "top_k",
                "tool_choice",
                "parallel_tool_calls",
                "modalities",
                "guardrails",
                "fallbacks",
                "retry",
                "cache",
                "load_balancer",
                "timeout",
            ]
        )
        nullable_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "seed",
                "stop",
                "stream_options",
                "temperature",
                "top_p",
                "top_k",
                "modalities",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class RunAgentModelConfigurationAgentsRetryTypedDict(TypedDict):
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class RunAgentModelConfigurationAgentsRetry(BaseModel):
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentModelConfiguration2TypedDict(TypedDict):
    r"""

    Model configuration with parameters and retry settings.
    """

    id: str
    r"""A model ID string (e.g., `openai/gpt-4o` or `anthropic/claude-haiku-4-5-20251001`). Only models that support tool calling can be used with agents."""
    parameters: NotRequired[RunAgentModelConfigurationParametersTypedDict]
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""
    retry: NotRequired[RunAgentModelConfigurationAgentsRetryTypedDict]
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""


class RunAgentModelConfiguration2(BaseModel):
    r"""

    Model configuration with parameters and retry settings.
    """

    id: str
    r"""A model ID string (e.g., `openai/gpt-4o` or `anthropic/claude-haiku-4-5-20251001`). Only models that support tool calling can be used with agents."""

    parameters: Optional[RunAgentModelConfigurationParameters] = None
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""

    retry: Optional[RunAgentModelConfigurationAgentsRetry] = None
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["parameters", "retry"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentModelConfigurationTypedDict = TypeAliasType(
    "RunAgentModelConfigurationTypedDict",
    Union[RunAgentModelConfiguration2TypedDict, str],
)
r"""Model configuration for this execution. Can override the agent manifest defaults if the agent already exists."""


RunAgentModelConfiguration = TypeAliasType(
    "RunAgentModelConfiguration", Union[RunAgentModelConfiguration2, str]
)
r"""Model configuration for this execution. Can override the agent manifest defaults if the agent already exists."""


RunAgentFallbackModelConfigurationVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


RunAgentFallbackModelConfigurationFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class RunAgentFallbackModelConfigurationAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: RunAgentFallbackModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: RunAgentFallbackModelConfigurationFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class RunAgentFallbackModelConfigurationAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: RunAgentFallbackModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[
        RunAgentFallbackModelConfigurationFormat, pydantic.Field(alias="format")
    ]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


RunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsFallbackModelConfigurationType = Literal[
    "json_schema",
]


class RunAgentResponseFormatAgentsRequestRequestBodyJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class RunAgentResponseFormatAgentsRequestRequestBodyJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = False
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "schema", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentResponseFormatAgentsRequestJSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: RunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsFallbackModelConfigurationType
    json_schema: RunAgentResponseFormatAgentsRequestRequestBodyJSONSchemaTypedDict


class RunAgentResponseFormatAgentsRequestJSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: RunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsFallbackModelConfigurationType

    json_schema: RunAgentResponseFormatAgentsRequestRequestBodyJSONSchema


RunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsType = Literal[
    "json_object",
]


class RunAgentResponseFormatAgentsJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: RunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsType


class RunAgentResponseFormatAgentsJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: RunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsType


RunAgentResponseFormatAgentsRequestRequestBodyType = Literal["text",]


class RunAgentResponseFormatAgentsTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: RunAgentResponseFormatAgentsRequestRequestBodyType


class RunAgentResponseFormatAgentsText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: RunAgentResponseFormatAgentsRequestRequestBodyType


RunAgentFallbackModelConfigurationResponseFormatTypedDict = TypeAliasType(
    "RunAgentFallbackModelConfigurationResponseFormatTypedDict",
    Union[
        RunAgentResponseFormatAgentsTextTypedDict,
        RunAgentResponseFormatAgentsJSONObjectTypedDict,
        RunAgentResponseFormatAgentsRequestJSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


RunAgentFallbackModelConfigurationResponseFormat = Annotated[
    Union[
        Annotated[RunAgentResponseFormatAgentsText, Tag("text")],
        Annotated[RunAgentResponseFormatAgentsJSONObject, Tag("json_object")],
        Annotated[RunAgentResponseFormatAgentsRequestJSONSchema, Tag("json_schema")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An object specifying the format that the model must output"""


RunAgentFallbackModelConfigurationReasoningEffort = Literal[
    "none",
    "minimal",
    "low",
    "medium",
    "high",
    "xhigh",
]
r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

- `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
- All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
- The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
- `xhigh` is currently only supported for `gpt-5.1-codex-max`.

Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
"""


RunAgentFallbackModelConfigurationStopTypedDict = TypeAliasType(
    "RunAgentFallbackModelConfigurationStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


RunAgentFallbackModelConfigurationStop = TypeAliasType(
    "RunAgentFallbackModelConfigurationStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class RunAgentFallbackModelConfigurationStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class RunAgentFallbackModelConfigurationStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentFallbackModelConfigurationThinkingTypedDict = TypeAliasType(
    "RunAgentFallbackModelConfigurationThinkingTypedDict",
    Union[ThinkingConfigDisabledSchemaTypedDict, ThinkingConfigEnabledSchemaTypedDict],
)


RunAgentFallbackModelConfigurationThinking = Annotated[
    Union[
        Annotated[ThinkingConfigDisabledSchema, Tag("disabled")],
        Annotated[ThinkingConfigEnabledSchema, Tag("enabled")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


RunAgentToolChoiceAgentsType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class RunAgentToolChoiceAgentsFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to call."""


class RunAgentToolChoiceAgentsFunction(BaseModel):
    name: str
    r"""The name of the function to call."""


class RunAgentToolChoiceAgents2TypedDict(TypedDict):
    function: RunAgentToolChoiceAgentsFunctionTypedDict
    type: NotRequired[RunAgentToolChoiceAgentsType]
    r"""The type of the tool. Currently, only function is supported."""


class RunAgentToolChoiceAgents2(BaseModel):
    function: RunAgentToolChoiceAgentsFunction

    type: Optional[RunAgentToolChoiceAgentsType] = None
    r"""The type of the tool. Currently, only function is supported."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentToolChoiceAgents1 = Literal[
    "none",
    "auto",
    "required",
]


RunAgentFallbackModelConfigurationToolChoiceTypedDict = TypeAliasType(
    "RunAgentFallbackModelConfigurationToolChoiceTypedDict",
    Union[RunAgentToolChoiceAgents2TypedDict, RunAgentToolChoiceAgents1],
)
r"""Controls which (if any) tool is called by the model."""


RunAgentFallbackModelConfigurationToolChoice = TypeAliasType(
    "RunAgentFallbackModelConfigurationToolChoice",
    Union[RunAgentToolChoiceAgents2, RunAgentToolChoiceAgents1],
)
r"""Controls which (if any) tool is called by the model."""


RunAgentFallbackModelConfigurationModalities = Literal[
    "text",
    "audio",
]


RunAgentIDAgents1 = Literal[
    "orq_pii_detection",
    "orq_sexual_moderation",
    "orq_harmful_moderation",
]
r"""The key of the guardrail."""


RunAgentFallbackModelConfigurationIDTypedDict = TypeAliasType(
    "RunAgentFallbackModelConfigurationIDTypedDict", Union[RunAgentIDAgents1, str]
)


RunAgentFallbackModelConfigurationID = TypeAliasType(
    "RunAgentFallbackModelConfigurationID", Union[RunAgentIDAgents1, str]
)


RunAgentFallbackModelConfigurationExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RunAgentFallbackModelConfigurationGuardrailsTypedDict(TypedDict):
    id: RunAgentFallbackModelConfigurationIDTypedDict
    execute_on: RunAgentFallbackModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RunAgentFallbackModelConfigurationGuardrails(BaseModel):
    id: RunAgentFallbackModelConfigurationID

    execute_on: RunAgentFallbackModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RunAgentFallbackModelConfigurationFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class RunAgentFallbackModelConfigurationFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


class RunAgentFallbackModelConfigurationRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class RunAgentFallbackModelConfigurationRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentFallbackModelConfigurationType = Literal["exact_match",]


class RunAgentFallbackModelConfigurationCacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: RunAgentFallbackModelConfigurationType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class RunAgentFallbackModelConfigurationCache(BaseModel):
    r"""Cache configuration for the request."""

    type: RunAgentFallbackModelConfigurationType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentLoadBalancerAgentsType = Literal["weight_based",]


class RunAgentLoadBalancerAgentsModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class RunAgentLoadBalancerAgentsModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentLoadBalancerAgents1TypedDict(TypedDict):
    type: RunAgentLoadBalancerAgentsType
    models: List[RunAgentLoadBalancerAgentsModelsTypedDict]


class RunAgentLoadBalancerAgents1(BaseModel):
    type: RunAgentLoadBalancerAgentsType

    models: List[RunAgentLoadBalancerAgentsModels]


RunAgentFallbackModelConfigurationLoadBalancerTypedDict = (
    RunAgentLoadBalancerAgents1TypedDict
)
r"""Load balancer configuration for the request."""


RunAgentFallbackModelConfigurationLoadBalancer = RunAgentLoadBalancerAgents1
r"""Load balancer configuration for the request."""


class RunAgentFallbackModelConfigurationTimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class RunAgentFallbackModelConfigurationTimeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class RunAgentFallbackModelConfigurationParametersTypedDict(TypedDict):
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    audio: NotRequired[Nullable[RunAgentFallbackModelConfigurationAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[
        RunAgentFallbackModelConfigurationResponseFormatTypedDict
    ]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[RunAgentFallbackModelConfigurationReasoningEffort]
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[RunAgentFallbackModelConfigurationStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[
        Nullable[RunAgentFallbackModelConfigurationStreamOptionsTypedDict]
    ]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[RunAgentFallbackModelConfigurationThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[RunAgentFallbackModelConfigurationToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[
        Nullable[List[RunAgentFallbackModelConfigurationModalities]]
    ]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    guardrails: NotRequired[List[RunAgentFallbackModelConfigurationGuardrailsTypedDict]]
    r"""A list of guardrails to apply to the request."""
    fallbacks: NotRequired[List[RunAgentFallbackModelConfigurationFallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    retry: NotRequired[RunAgentFallbackModelConfigurationRetryTypedDict]
    r"""Retry configuration for the request"""
    cache: NotRequired[RunAgentFallbackModelConfigurationCacheTypedDict]
    r"""Cache configuration for the request."""
    load_balancer: NotRequired[RunAgentFallbackModelConfigurationLoadBalancerTypedDict]
    r"""Load balancer configuration for the request."""
    timeout: NotRequired[RunAgentFallbackModelConfigurationTimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


class RunAgentFallbackModelConfigurationParameters(BaseModel):
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    audio: OptionalNullable[RunAgentFallbackModelConfigurationAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[RunAgentFallbackModelConfigurationResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[RunAgentFallbackModelConfigurationReasoningEffort] = None
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[RunAgentFallbackModelConfigurationStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[
        RunAgentFallbackModelConfigurationStreamOptions
    ] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[RunAgentFallbackModelConfigurationThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[RunAgentFallbackModelConfigurationToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[RunAgentFallbackModelConfigurationModalities]] = (
        UNSET
    )
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    guardrails: Optional[List[RunAgentFallbackModelConfigurationGuardrails]] = None
    r"""A list of guardrails to apply to the request."""

    fallbacks: Optional[List[RunAgentFallbackModelConfigurationFallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    retry: Optional[RunAgentFallbackModelConfigurationRetry] = None
    r"""Retry configuration for the request"""

    cache: Optional[RunAgentFallbackModelConfigurationCache] = None
    r"""Cache configuration for the request."""

    load_balancer: Optional[RunAgentFallbackModelConfigurationLoadBalancer] = None
    r"""Load balancer configuration for the request."""

    timeout: Optional[RunAgentFallbackModelConfigurationTimeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "name",
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "response_format",
                "reasoning_effort",
                "verbosity",
                "seed",
                "stop",
                "stream_options",
                "thinking",
                "temperature",
                "top_p",
                "top_k",
                "tool_choice",
                "parallel_tool_calls",
                "modalities",
                "guardrails",
                "fallbacks",
                "retry",
                "cache",
                "load_balancer",
                "timeout",
            ]
        )
        nullable_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "seed",
                "stop",
                "stream_options",
                "temperature",
                "top_p",
                "top_k",
                "modalities",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class RunAgentFallbackModelConfigurationAgentsRetryTypedDict(TypedDict):
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class RunAgentFallbackModelConfigurationAgentsRetry(BaseModel):
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentFallbackModelConfiguration2TypedDict(TypedDict):
    r"""Fallback model configuration with optional parameters and retry settings."""

    id: str
    r"""A fallback model ID string. Must support tool calling."""
    parameters: NotRequired[RunAgentFallbackModelConfigurationParametersTypedDict]
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""
    retry: NotRequired[RunAgentFallbackModelConfigurationAgentsRetryTypedDict]
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""


class RunAgentFallbackModelConfiguration2(BaseModel):
    r"""Fallback model configuration with optional parameters and retry settings."""

    id: str
    r"""A fallback model ID string. Must support tool calling."""

    parameters: Optional[RunAgentFallbackModelConfigurationParameters] = None
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    retry: Optional[RunAgentFallbackModelConfigurationAgentsRetry] = None
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["parameters", "retry"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentFallbackModelConfigurationTypedDict = TypeAliasType(
    "RunAgentFallbackModelConfigurationTypedDict",
    Union[RunAgentFallbackModelConfiguration2TypedDict, str],
)
r"""Fallback model for automatic failover when primary model request fails. Supports optional parameter overrides. Can be a simple model ID string or a configuration object with model-specific parameters. Fallbacks are tried in order."""


RunAgentFallbackModelConfiguration = TypeAliasType(
    "RunAgentFallbackModelConfiguration",
    Union[RunAgentFallbackModelConfiguration2, str],
)
r"""Fallback model for automatic failover when primary model request fails. Supports optional parameter overrides. Can be a simple model ID string or a configuration object with model-specific parameters. Fallbacks are tried in order."""


RunAgentRoleToolMessage = Literal["tool",]
r"""Message containing tool execution results"""


RunAgentRoleUserMessage = Literal["user",]
r"""Message from the end user"""


RunAgentRoleTypedDict = TypeAliasType(
    "RunAgentRoleTypedDict", Union[RunAgentRoleUserMessage, RunAgentRoleToolMessage]
)
r"""Message role (user or tool for continuing executions)"""


RunAgentRole = TypeAliasType(
    "RunAgentRole", Union[RunAgentRoleUserMessage, RunAgentRoleToolMessage]
)
r"""Message role (user or tool for continuing executions)"""


RunAgentPublicMessagePartTypedDict = TypeAliasType(
    "RunAgentPublicMessagePartTypedDict",
    Union[
        TextPartTypedDict,
        FilePartTypedDict,
        ErrorPartTypedDict,
        ToolResultPartTypedDict,
    ],
)
r"""Message part that can be provided by users. Use \"text\" for regular messages, \"file\" for attachments, or \"tool_result\" when responding to tool call requests."""


RunAgentPublicMessagePart = Annotated[
    Union[
        Annotated[TextPart, Tag("text")],
        Annotated[FilePart, Tag("file")],
        Annotated[ToolResultPart, Tag("tool_result")],
        Annotated[ErrorPart, Tag("error")],
    ],
    Discriminator(lambda m: get_discriminator(m, "kind", "kind")),
]
r"""Message part that can be provided by users. Use \"text\" for regular messages, \"file\" for attachments, or \"tool_result\" when responding to tool call requests."""


class RunAgentA2AMessageTypedDict(TypedDict):
    r"""The A2A format message containing the task for the agent to perform."""

    role: RunAgentRoleTypedDict
    r"""Message role (user or tool for continuing executions)"""
    parts: List[RunAgentPublicMessagePartTypedDict]
    r"""A2A message parts (text, file, or tool_result only)"""
    message_id: NotRequired[str]
    r"""Optional A2A message ID in ULID format"""


class RunAgentA2AMessage(BaseModel):
    r"""The A2A format message containing the task for the agent to perform."""

    role: RunAgentRole
    r"""Message role (user or tool for continuing executions)"""

    parts: List[RunAgentPublicMessagePart]
    r"""A2A message parts (text, file, or tool_result only)"""

    message_id: Annotated[Optional[str], pydantic.Field(alias="messageId")] = None
    r"""Optional A2A message ID in ULID format"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["messageId"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentIdentityTypedDict(TypedDict):
    r"""Information about the identity making the request. If the identity does not exist, it will be created automatically."""

    id: str
    r"""Unique identifier for the contact"""
    display_name: NotRequired[str]
    r"""Display name of the contact"""
    email: NotRequired[str]
    r"""Email address of the contact"""
    metadata: NotRequired[List[Dict[str, Any]]]
    r"""A hash of key/value pairs containing any other data about the contact"""
    logo_url: NotRequired[str]
    r"""URL to the contact's avatar or logo"""
    tags: NotRequired[List[str]]
    r"""A list of tags associated with the contact"""


class RunAgentIdentity(BaseModel):
    r"""Information about the identity making the request. If the identity does not exist, it will be created automatically."""

    id: str
    r"""Unique identifier for the contact"""

    display_name: Optional[str] = None
    r"""Display name of the contact"""

    email: Optional[str] = None
    r"""Email address of the contact"""

    metadata: Optional[List[Dict[str, Any]]] = None
    r"""A hash of key/value pairs containing any other data about the contact"""

    logo_url: Optional[str] = None
    r"""URL to the contact's avatar or logo"""

    tags: Optional[List[str]] = None
    r"""A list of tags associated with the contact"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["display_name", "email", "metadata", "logo_url", "tags"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


@deprecated(
    "warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
)
class RunAgentContactTypedDict(TypedDict):
    r"""@deprecated Use identity instead. Information about the contact making the request."""

    id: str
    r"""Unique identifier for the contact"""
    display_name: NotRequired[str]
    r"""Display name of the contact"""
    email: NotRequired[str]
    r"""Email address of the contact"""
    metadata: NotRequired[List[Dict[str, Any]]]
    r"""A hash of key/value pairs containing any other data about the contact"""
    logo_url: NotRequired[str]
    r"""URL to the contact's avatar or logo"""
    tags: NotRequired[List[str]]
    r"""A list of tags associated with the contact"""


@deprecated(
    "warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
)
class RunAgentContact(BaseModel):
    r"""@deprecated Use identity instead. Information about the contact making the request."""

    id: str
    r"""Unique identifier for the contact"""

    display_name: Optional[str] = None
    r"""Display name of the contact"""

    email: Optional[str] = None
    r"""Email address of the contact"""

    metadata: Optional[List[Dict[str, Any]]] = None
    r"""A hash of key/value pairs containing any other data about the contact"""

    logo_url: Optional[str] = None
    r"""URL to the contact's avatar or logo"""

    tags: Optional[List[str]] = None
    r"""A list of tags associated with the contact"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["display_name", "email", "metadata", "logo_url", "tags"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentThreadTypedDict(TypedDict):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""
    tags: NotRequired[List[str]]
    r"""Optional tags to differentiate or categorize threads"""


class RunAgentThread(BaseModel):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""

    tags: Optional[List[str]] = None
    r"""Optional tags to differentiate or categorize threads"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["tags"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentMemoryTypedDict(TypedDict):
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""

    entity_id: str
    r"""An entity ID used to link memory stores to a specific user, session, or conversation. This ID is used to isolate and retrieve memories specific to the entity across agent executions."""


class RunAgentMemory(BaseModel):
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""

    entity_id: str
    r"""An entity ID used to link memory stores to a specific user, session, or conversation. This ID is used to isolate and retrieve memories specific to the entity across agent executions."""


class RunAgentKnowledgeBasesTypedDict(TypedDict):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


class RunAgentKnowledgeBases(BaseModel):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


class RunAgentTeamOfAgentsTypedDict(TypedDict):
    key: str
    r"""The unique key of the agent within the workspace"""
    role: NotRequired[str]
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""


class RunAgentTeamOfAgents(BaseModel):
    key: str
    r"""The unique key of the agent within the workspace"""

    role: Optional[str] = None
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["role"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools16Type = Literal["mcp",]


class AgentToolInputRunHeadersTypedDict(TypedDict):
    value: str
    encrypted: NotRequired[bool]


class AgentToolInputRunHeaders(BaseModel):
    value: str

    encrypted: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["encrypted"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools16McpType = Literal[
    "object",
]


class AgentToolInputRunSchemaTypedDict(TypedDict):
    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools16McpType
    properties: NotRequired[Dict[str, Any]]
    required: NotRequired[List[str]]


class AgentToolInputRunSchema(BaseModel):
    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools16McpType

    properties: Optional[Dict[str, Any]] = None

    required: Optional[List[str]] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["properties", "required"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentAgentToolInputRunToolsTypedDict(TypedDict):
    name: str
    schema_: AgentToolInputRunSchemaTypedDict
    id: NotRequired[str]
    description: NotRequired[str]


class RunAgentAgentToolInputRunTools(BaseModel):
    name: str

    schema_: Annotated[AgentToolInputRunSchema, pydantic.Field(alias="schema")]

    id: Optional[str] = "01KHT0V9R6WPTT7E5D169DX045"

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["id", "description"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ConnectionType = Literal[
    "http",
    "sse",
]
r"""The connection type used by the MCP server"""


class McpTypedDict(TypedDict):
    server_url: str
    r"""The MCP server URL (cached for execution)"""
    tools: List[RunAgentAgentToolInputRunToolsTypedDict]
    r"""Array of tools available from the MCP server"""
    connection_type: ConnectionType
    r"""The connection type used by the MCP server"""
    headers: NotRequired[Dict[str, AgentToolInputRunHeadersTypedDict]]
    r"""HTTP headers for MCP server requests with encryption support"""


class Mcp(BaseModel):
    server_url: str
    r"""The MCP server URL (cached for execution)"""

    tools: List[RunAgentAgentToolInputRunTools]
    r"""Array of tools available from the MCP server"""

    connection_type: ConnectionType
    r"""The connection type used by the MCP server"""

    headers: Optional[Dict[str, AgentToolInputRunHeaders]] = None
    r"""HTTP headers for MCP server requests with encryption support"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["headers"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class MCPToolRunTypedDict(TypedDict):
    r"""MCP tool with inline definition for on-the-fly creation in run endpoint"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools16Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""
    mcp: McpTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    requires_approval: NotRequired[bool]


class MCPToolRun(BaseModel):
    r"""MCP tool with inline definition for on-the-fly creation in run endpoint"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools16Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""

    mcp: Mcp

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    requires_approval: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["_id", "display_name", "requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools15Type = Literal[
    "json_schema",
]


class SchemaTypedDict(TypedDict):
    r"""The schema for the response format, described as a JSON Schema object. See the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    type: str
    r"""The JSON Schema type"""
    properties: Dict[str, Any]
    r"""The properties of the JSON Schema object"""
    required: List[str]
    r"""Array of required property names"""


class Schema(BaseModel):
    r"""The schema for the response format, described as a JSON Schema object. See the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: str
    r"""The JSON Schema type"""

    properties: Dict[str, Any]
    r"""The properties of the JSON Schema object"""

    required: List[str]
    r"""Array of required property names"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class AgentToolInputRunJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: str
    r"""A description of what the response format is for. This will be shown to the user."""
    schema_: SchemaTypedDict
    r"""The schema for the response format, described as a JSON Schema object. See the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the `schema` field. Only a subset of JSON Schema is supported when `strict` is `true`. Only compatible with `OpenAI` models."""


class AgentToolInputRunJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: str
    r"""A description of what the response format is for. This will be shown to the user."""

    schema_: Annotated[Schema, pydantic.Field(alias="schema")]
    r"""The schema for the response format, described as a JSON Schema object. See the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    strict: Optional[bool] = None
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the `schema` field. Only a subset of JSON Schema is supported when `strict` is `true`. Only compatible with `OpenAI` models."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class JSONSchemaToolRunTypedDict(TypedDict):
    r"""JSON Schema tool with inline definition for on-the-fly creation in run endpoint"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools15Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""
    json_schema: AgentToolInputRunJSONSchemaTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    requires_approval: NotRequired[bool]


class JSONSchemaToolRun(BaseModel):
    r"""JSON Schema tool with inline definition for on-the-fly creation in run endpoint"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools15Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""

    json_schema: AgentToolInputRunJSONSchema

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    requires_approval: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["_id", "display_name", "requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14Type = Literal[
    "function",
]


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14FunctionType = Literal[
    "object",
]
r"""The type must be \"object\" """


class RunAgentAgentToolInputRunParametersTypedDict(TypedDict):
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14FunctionType
    r"""The type must be \"object\" """
    properties: Dict[str, Any]
    r"""The properties of the function parameters"""
    required: List[str]
    r"""Array of required parameter names"""


class RunAgentAgentToolInputRunParameters(BaseModel):
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14FunctionType
    r"""The type must be \"object\" """

    properties: Dict[str, Any]
    r"""The properties of the function parameters"""

    required: List[str]
    r"""Array of required parameter names"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class AgentToolInputRunFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the function does, used by the model to choose when and how to call the function."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Currently only compatible with `OpenAI` models."""
    parameters: NotRequired[RunAgentAgentToolInputRunParametersTypedDict]
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""


class AgentToolInputRunFunction(BaseModel):
    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the function does, used by the model to choose when and how to call the function."""

    strict: Optional[bool] = None
    r"""Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Currently only compatible with `OpenAI` models."""

    parameters: Optional[RunAgentAgentToolInputRunParameters] = None
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "strict", "parameters"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class FunctionToolRunTypedDict(TypedDict):
    r"""Function tool with inline definition for on-the-fly creation in run endpoint"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    function: AgentToolInputRunFunctionTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    description: NotRequired[str]
    requires_approval: NotRequired[bool]


class FunctionToolRun(BaseModel):
    r"""Function tool with inline definition for on-the-fly creation in run endpoint"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    function: AgentToolInputRunFunction

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    description: Optional[str] = None

    requires_approval: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["_id", "display_name", "description", "requires_approval"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13Type = Literal["code",]


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13CodeToolType = Literal[
    "object",
]
r"""The type must be \"object\" """


class AgentToolInputRunParametersTypedDict(TypedDict):
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13CodeToolType
    r"""The type must be \"object\" """
    properties: Dict[str, Any]
    r"""The properties of the function parameters"""
    required: List[str]
    r"""Array of required parameter names"""


class AgentToolInputRunParameters(BaseModel):
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13CodeToolType
    r"""The type must be \"object\" """

    properties: Dict[str, Any]
    r"""The properties of the function parameters"""

    required: List[str]
    r"""Array of required parameter names"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


Language = Literal["python",]


class CodeToolTypedDict(TypedDict):
    language: Language
    code: str
    r"""The code to execute."""
    parameters: NotRequired[AgentToolInputRunParametersTypedDict]
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""


class CodeTool(BaseModel):
    language: Language

    code: str
    r"""The code to execute."""

    parameters: Optional[AgentToolInputRunParameters] = None
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["parameters"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CodeToolRunTypedDict(TypedDict):
    r"""Code execution tool with inline definition for on-the-fly creation in run endpoint"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""
    code_tool: CodeToolTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    requires_approval: NotRequired[bool]


class CodeToolRun(BaseModel):
    r"""Code execution tool with inline definition for on-the-fly creation in run endpoint"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""

    code_tool: CodeTool

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    requires_approval: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["_id", "display_name", "requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12Type = Literal["http",]


Method = Literal[
    "GET",
    "POST",
    "PUT",
    "DELETE",
]
r"""The HTTP method to use."""


class Headers2TypedDict(TypedDict):
    value: str
    encrypted: NotRequired[bool]


class Headers2(BaseModel):
    value: str

    encrypted: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["encrypted"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


HeadersTypedDict = TypeAliasType("HeadersTypedDict", Union[Headers2TypedDict, str])


Headers = TypeAliasType("Headers", Union[Headers2, str])


class BlueprintTypedDict(TypedDict):
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""

    url: str
    r"""The URL to send the request to."""
    method: Method
    r"""The HTTP method to use."""
    headers: NotRequired[Dict[str, HeadersTypedDict]]
    r"""The headers to send with the request. Can be a string value or an object with value and encrypted properties."""
    body: NotRequired[Dict[str, Any]]
    r"""The body to send with the request."""


class Blueprint(BaseModel):
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""

    url: str
    r"""The URL to send the request to."""

    method: Method
    r"""The HTTP method to use."""

    headers: Optional[Dict[str, Headers]] = None
    r"""The headers to send with the request. Can be a string value or an object with value and encrypted properties."""

    body: Optional[Dict[str, Any]] = None
    r"""The body to send with the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["headers", "body"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12HTTPType = Literal[
    "string",
    "number",
    "boolean",
]
r"""The type of the argument."""


DefaultValueTypedDict = TypeAliasType("DefaultValueTypedDict", Union[str, float, bool])
r"""The default value of the argument."""


DefaultValue = TypeAliasType("DefaultValue", Union[str, float, bool])
r"""The default value of the argument."""


class ArgumentsTypedDict(TypedDict):
    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12HTTPType
    r"""The type of the argument."""
    description: str
    r"""A description of the argument."""
    send_to_model: NotRequired[bool]
    r"""Whether to send the argument to the model. If set to false, the argument will not be sent to the model and needs to be provided by the user or it will be left blank."""
    default_value: NotRequired[DefaultValueTypedDict]
    r"""The default value of the argument."""


class Arguments(BaseModel):
    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12HTTPType
    r"""The type of the argument."""

    description: str
    r"""A description of the argument."""

    send_to_model: Optional[bool] = True
    r"""Whether to send the argument to the model. If set to false, the argument will not be sent to the model and needs to be provided by the user or it will be left blank."""

    default_value: Optional[DefaultValue] = None
    r"""The default value of the argument."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["send_to_model", "default_value"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class HTTPTypedDict(TypedDict):
    blueprint: BlueprintTypedDict
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""
    arguments: NotRequired[Dict[str, ArgumentsTypedDict]]
    r"""The arguments to send with the request. The keys will be used to replace the placeholders in the `blueprint` field."""


class HTTP(BaseModel):
    blueprint: Blueprint
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""

    arguments: Optional[Dict[str, Arguments]] = None
    r"""The arguments to send with the request. The keys will be used to replace the placeholders in the `blueprint` field."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["arguments"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class HTTPToolRunTypedDict(TypedDict):
    r"""HTTP tool with inline definition for on-the-fly creation in run endpoint"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""
    http: HTTPTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    requires_approval: NotRequired[bool]


class HTTPToolRun(BaseModel):
    r"""HTTP tool with inline definition for on-the-fly creation in run endpoint"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""

    http: HTTP

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    requires_approval: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["_id", "display_name", "requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools11Type = Literal[
    "current_date",
]


class AgentToolInputRunCurrentDateToolTypedDict(TypedDict):
    r"""Returns the current date and time"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools11Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunCurrentDateTool(BaseModel):
    r"""Returns the current date and time"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools11Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools10Type = Literal[
    "query_knowledge_base",
]


class AgentToolInputRunQueryKnowledgeBaseToolTypedDict(TypedDict):
    r"""Queries knowledge bases for information"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools10Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunQueryKnowledgeBaseTool(BaseModel):
    r"""Queries knowledge bases for information"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools10Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools9Type = Literal[
    "retrieve_knowledge_bases",
]


class AgentToolInputRunRetrieveKnowledgeBasesToolTypedDict(TypedDict):
    r"""Lists available knowledge bases"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools9Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunRetrieveKnowledgeBasesTool(BaseModel):
    r"""Lists available knowledge bases"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools9Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools8Type = Literal[
    "delete_memory_document",
]


class AgentToolInputRunDeleteMemoryDocumentToolTypedDict(TypedDict):
    r"""Deletes documents from memory stores"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools8Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunDeleteMemoryDocumentTool(BaseModel):
    r"""Deletes documents from memory stores"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools8Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsToolsType = Literal[
    "retrieve_memory_stores",
]


class AgentToolInputRunRetrieveMemoryStoresToolTypedDict(TypedDict):
    r"""Lists available memory stores"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsToolsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunRetrieveMemoryStoresTool(BaseModel):
    r"""Lists available memory stores"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsToolsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsType = Literal[
    "write_memory_store",
]


class AgentToolInputRunWriteMemoryStoreToolTypedDict(TypedDict):
    r"""Writes information to agent memory stores"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunWriteMemoryStoreTool(BaseModel):
    r"""Writes information to agent memory stores"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodySettingsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestRequestBodyType = Literal["query_memory_store",]


class AgentToolInputRunQueryMemoryStoreToolTypedDict(TypedDict):
    r"""Queries agent memory stores for context"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodyType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunQueryMemoryStoreTool(BaseModel):
    r"""Queries agent memory stores for context"""

    type: RunAgentAgentToolInputRunAgentsRequestRequestBodyType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsRequestType = Literal["retrieve_agents",]


class AgentToolInputRunRetrieveAgentsToolTypedDict(TypedDict):
    r"""Retrieves available agents in the system"""

    type: RunAgentAgentToolInputRunAgentsRequestType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunRetrieveAgentsTool(BaseModel):
    r"""Retrieves available agents in the system"""

    type: RunAgentAgentToolInputRunAgentsRequestType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunAgentsType = Literal["call_sub_agent",]


class AgentToolInputRunCallSubAgentToolTypedDict(TypedDict):
    r"""Delegates tasks to specialized sub-agents"""

    type: RunAgentAgentToolInputRunAgentsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunCallSubAgentTool(BaseModel):
    r"""Delegates tasks to specialized sub-agents"""

    type: RunAgentAgentToolInputRunAgentsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentToolInputRunType = Literal["web_scraper",]


class AgentToolInputRunWebScraperToolTypedDict(TypedDict):
    r"""Scrapes and extracts content from web pages"""

    type: RunAgentAgentToolInputRunType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunWebScraperTool(BaseModel):
    r"""Scrapes and extracts content from web pages"""

    type: RunAgentAgentToolInputRunType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


AgentToolInputRunType = Literal["google_search",]


class AgentToolInputRunGoogleSearchToolTypedDict(TypedDict):
    r"""Performs Google searches to retrieve web content"""

    type: AgentToolInputRunType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class AgentToolInputRunGoogleSearchTool(BaseModel):
    r"""Performs Google searches to retrieve web content"""

    type: AgentToolInputRunType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


AgentToolInputRunTypedDict = TypeAliasType(
    "AgentToolInputRunTypedDict",
    Union[
        AgentToolInputRunGoogleSearchToolTypedDict,
        AgentToolInputRunWebScraperToolTypedDict,
        AgentToolInputRunCallSubAgentToolTypedDict,
        AgentToolInputRunRetrieveAgentsToolTypedDict,
        AgentToolInputRunQueryMemoryStoreToolTypedDict,
        AgentToolInputRunWriteMemoryStoreToolTypedDict,
        AgentToolInputRunRetrieveMemoryStoresToolTypedDict,
        AgentToolInputRunDeleteMemoryDocumentToolTypedDict,
        AgentToolInputRunRetrieveKnowledgeBasesToolTypedDict,
        AgentToolInputRunQueryKnowledgeBaseToolTypedDict,
        AgentToolInputRunCurrentDateToolTypedDict,
        HTTPToolRunTypedDict,
        CodeToolRunTypedDict,
        FunctionToolRunTypedDict,
        JSONSchemaToolRunTypedDict,
        MCPToolRunTypedDict,
    ],
)
r"""Tool configuration for agent run operations. Built-in tools only require a type and requires_approval, while custom tools (HTTP, Code, Function, JSON Schema, MCP) support full inline definitions for on-the-fly creation."""


AgentToolInputRun = Annotated[
    Union[
        Annotated[AgentToolInputRunGoogleSearchTool, Tag("google_search")],
        Annotated[AgentToolInputRunWebScraperTool, Tag("web_scraper")],
        Annotated[AgentToolInputRunCallSubAgentTool, Tag("call_sub_agent")],
        Annotated[AgentToolInputRunRetrieveAgentsTool, Tag("retrieve_agents")],
        Annotated[AgentToolInputRunQueryMemoryStoreTool, Tag("query_memory_store")],
        Annotated[AgentToolInputRunWriteMemoryStoreTool, Tag("write_memory_store")],
        Annotated[
            AgentToolInputRunRetrieveMemoryStoresTool, Tag("retrieve_memory_stores")
        ],
        Annotated[
            AgentToolInputRunDeleteMemoryDocumentTool, Tag("delete_memory_document")
        ],
        Annotated[
            AgentToolInputRunRetrieveKnowledgeBasesTool, Tag("retrieve_knowledge_bases")
        ],
        Annotated[AgentToolInputRunQueryKnowledgeBaseTool, Tag("query_knowledge_base")],
        Annotated[AgentToolInputRunCurrentDateTool, Tag("current_date")],
        Annotated[HTTPToolRun, Tag("http")],
        Annotated[CodeToolRun, Tag("code")],
        Annotated[FunctionToolRun, Tag("function")],
        Annotated[JSONSchemaToolRun, Tag("json_schema")],
        Annotated[MCPToolRun, Tag("mcp")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""Tool configuration for agent run operations. Built-in tools only require a type and requires_approval, while custom tools (HTTP, Code, Function, JSON Schema, MCP) support full inline definitions for on-the-fly creation."""


RunAgentToolApprovalRequired = Literal[
    "all",
    "respect_tool",
    "none",
]
r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""


RunAgentExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class RunAgentEvaluatorsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: RunAgentExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class RunAgentEvaluators(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: RunAgentExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["sample_rate"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentAgentsExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class RunAgentGuardrailsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: RunAgentAgentsExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class RunAgentGuardrails(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: RunAgentAgentsExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["sample_rate"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentSettingsTypedDict(TypedDict):
    tools: NotRequired[List[AgentToolInputRunTypedDict]]
    r"""Tools available to the agent"""
    tool_approval_required: NotRequired[RunAgentToolApprovalRequired]
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""
    max_iterations: NotRequired[int]
    r"""Maximum iterations(llm calls) before the agent will stop executing."""
    max_execution_time: NotRequired[int]
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""
    max_cost: NotRequired[float]
    r"""Maximum cost in USD for the agent execution. When the accumulated cost exceeds this limit, the agent will stop executing. Set to 0 for unlimited. Only supported in v3 responses"""
    evaluators: NotRequired[List[RunAgentEvaluatorsTypedDict]]
    r"""Configuration for an evaluator applied to the agent"""
    guardrails: NotRequired[List[RunAgentGuardrailsTypedDict]]
    r"""Configuration for a guardrail applied to the agent"""


class RunAgentSettings(BaseModel):
    tools: Optional[List[AgentToolInputRun]] = None
    r"""Tools available to the agent"""

    tool_approval_required: Optional[RunAgentToolApprovalRequired] = "none"
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""

    max_iterations: Optional[int] = 100
    r"""Maximum iterations(llm calls) before the agent will stop executing."""

    max_execution_time: Optional[int] = 600
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""

    max_cost: Optional[float] = 0
    r"""Maximum cost in USD for the agent execution. When the accumulated cost exceeds this limit, the agent will stop executing. Set to 0 for unlimited. Only supported in v3 responses"""

    evaluators: Optional[List[RunAgentEvaluators]] = None
    r"""Configuration for an evaluator applied to the agent"""

    guardrails: Optional[List[RunAgentGuardrails]] = None
    r"""Configuration for a guardrail applied to the agent"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "tools",
                "tool_approval_required",
                "max_iterations",
                "max_execution_time",
                "max_cost",
                "evaluators",
                "guardrails",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentRequestBodyTypedDict(TypedDict):
    key: str
    r"""A unique identifier for the agent. This key must be unique within the same workspace and cannot be reused. When executing the agent, this key determines if the agent already exists. If the agent version differs, a new version is created at the end of the execution, except for the task. All agent parameters are evaluated to decide if a new version is needed."""
    model: RunAgentModelConfigurationTypedDict
    r"""Model configuration for this execution. Can override the agent manifest defaults if the agent already exists."""
    role: str
    r"""Specifies the agent's function and area of expertise."""
    instructions: str
    r"""Provides context and purpose for the agent. Combined with the system prompt template to generate the agent's instructions."""
    message: RunAgentA2AMessageTypedDict
    r"""The A2A format message containing the task for the agent to perform."""
    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """
    settings: RunAgentSettingsTypedDict
    task_id: NotRequired[str]
    r"""Optional task ID to continue an existing agent execution. When provided, the agent will continue the conversation from the existing task state. The task must be in an inactive state to continue."""
    fallback_models: NotRequired[List[RunAgentFallbackModelConfigurationTypedDict]]
    r"""Optional array of fallback models used when the primary model fails. Fallbacks are attempted in order. All models must support tool calling."""
    variables: NotRequired[Dict[str, Any]]
    r"""Optional variables for template replacement in system prompt, instructions, and messages"""
    identity: NotRequired[RunAgentIdentityTypedDict]
    r"""Information about the identity making the request. If the identity does not exist, it will be created automatically."""
    contact: NotRequired[RunAgentContactTypedDict]
    r"""@deprecated Use identity instead. Information about the contact making the request."""
    thread: NotRequired[RunAgentThreadTypedDict]
    r"""Thread information to group related requests"""
    memory: NotRequired[RunAgentMemoryTypedDict]
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""
    description: NotRequired[str]
    r"""A brief summary of the agent's purpose."""
    system_prompt: NotRequired[str]
    r"""A custom system prompt template for the agent. If omitted, the default template is used."""
    memory_stores: NotRequired[List[str]]
    r"""Array of memory store identifiers that are accessible to the agent. Accepts both memory store IDs and keys."""
    knowledge_bases: NotRequired[List[RunAgentKnowledgeBasesTypedDict]]
    r"""Knowledge base configurations for the agent to access"""
    team_of_agents: NotRequired[List[RunAgentTeamOfAgentsTypedDict]]
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""
    metadata: NotRequired[Dict[str, Any]]
    r"""Optional metadata for the agent run as key-value pairs that will be included in traces"""


class RunAgentRequestBody(BaseModel):
    key: str
    r"""A unique identifier for the agent. This key must be unique within the same workspace and cannot be reused. When executing the agent, this key determines if the agent already exists. If the agent version differs, a new version is created at the end of the execution, except for the task. All agent parameters are evaluated to decide if a new version is needed."""

    model: RunAgentModelConfiguration
    r"""Model configuration for this execution. Can override the agent manifest defaults if the agent already exists."""

    role: str
    r"""Specifies the agent's function and area of expertise."""

    instructions: str
    r"""Provides context and purpose for the agent. Combined with the system prompt template to generate the agent's instructions."""

    message: RunAgentA2AMessage
    r"""The A2A format message containing the task for the agent to perform."""

    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """

    settings: RunAgentSettings

    task_id: Optional[str] = None
    r"""Optional task ID to continue an existing agent execution. When provided, the agent will continue the conversation from the existing task state. The task must be in an inactive state to continue."""

    fallback_models: Optional[List[RunAgentFallbackModelConfiguration]] = None
    r"""Optional array of fallback models used when the primary model fails. Fallbacks are attempted in order. All models must support tool calling."""

    variables: Optional[Dict[str, Any]] = None
    r"""Optional variables for template replacement in system prompt, instructions, and messages"""

    identity: Optional[RunAgentIdentity] = None
    r"""Information about the identity making the request. If the identity does not exist, it will be created automatically."""

    contact: Annotated[
        Optional[RunAgentContact],
        pydantic.Field(
            deprecated="warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
        ),
    ] = None
    r"""@deprecated Use identity instead. Information about the contact making the request."""

    thread: Optional[RunAgentThread] = None
    r"""Thread information to group related requests"""

    memory: Optional[RunAgentMemory] = None
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""

    description: Optional[str] = None
    r"""A brief summary of the agent's purpose."""

    system_prompt: Optional[str] = None
    r"""A custom system prompt template for the agent. If omitted, the default template is used."""

    memory_stores: Optional[List[str]] = None
    r"""Array of memory store identifiers that are accessible to the agent. Accepts both memory store IDs and keys."""

    knowledge_bases: Optional[List[RunAgentKnowledgeBases]] = None
    r"""Knowledge base configurations for the agent to access"""

    team_of_agents: Optional[List[RunAgentTeamOfAgents]] = None
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""

    metadata: Optional[Dict[str, Any]] = None
    r"""Optional metadata for the agent run as key-value pairs that will be included in traces"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "task_id",
                "fallback_models",
                "variables",
                "identity",
                "contact",
                "thread",
                "memory",
                "description",
                "system_prompt",
                "memory_stores",
                "knowledge_bases",
                "team_of_agents",
                "metadata",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RunAgentKind = Literal["task",]
r"""A2A entity type identifier"""


RunAgentTaskState = Literal[
    "submitted",
    "working",
    "input-required",
    "auth-required",
    "completed",
    "failed",
    "canceled",
    "rejected",
]
r"""Current state of the agent task execution. Values: submitted (queued), working (executing), input-required (awaiting user input), completed (finished successfully), failed (error occurred). Note: auth-required, canceled, and rejected statuses are defined for A2A protocol compatibility but are not currently supported in task execution."""


RunAgentAgentsKind = Literal["message",]


RunAgentExtendedMessageRole = Literal[
    "user",
    "agent",
    "tool",
    "system",
]
r"""Role of the message sender in the A2A protocol. Values: user (end user), agent (AI agent), tool (tool execution result), system (system instructions/prompts)."""


RunAgentPartsTypedDict = TypeAliasType(
    "RunAgentPartsTypedDict",
    Union[
        TextPartTypedDict,
        ErrorPartTypedDict,
        DataPartTypedDict,
        FilePartTypedDict,
        ToolResultPartTypedDict,
        ToolCallPartTypedDict,
    ],
)


RunAgentParts = Annotated[
    Union[
        Annotated[TextPart, Tag("text")],
        Annotated[ErrorPart, Tag("error")],
        Annotated[DataPart, Tag("data")],
        Annotated[FilePart, Tag("file")],
        Annotated[ToolCallPart, Tag("tool_call")],
        Annotated[ToolResultPart, Tag("tool_result")],
    ],
    Discriminator(lambda m: get_discriminator(m, "kind", "kind")),
]


class RunAgentTaskStatusMessageTypedDict(TypedDict):
    r"""Optional A2A message providing additional context about the current status"""

    kind: RunAgentAgentsKind
    message_id: str
    role: RunAgentExtendedMessageRole
    r"""Role of the message sender in the A2A protocol. Values: user (end user), agent (AI agent), tool (tool execution result), system (system instructions/prompts)."""
    parts: List[RunAgentPartsTypedDict]


class RunAgentTaskStatusMessage(BaseModel):
    r"""Optional A2A message providing additional context about the current status"""

    kind: RunAgentAgentsKind

    message_id: Annotated[str, pydantic.Field(alias="messageId")]

    role: RunAgentExtendedMessageRole
    r"""Role of the message sender in the A2A protocol. Values: user (end user), agent (AI agent), tool (tool execution result), system (system instructions/prompts)."""

    parts: List[RunAgentParts]


class RunAgentTaskStatusTypedDict(TypedDict):
    r"""Current task status information"""

    state: RunAgentTaskState
    r"""Current state of the agent task execution. Values: submitted (queued), working (executing), input-required (awaiting user input), completed (finished successfully), failed (error occurred). Note: auth-required, canceled, and rejected statuses are defined for A2A protocol compatibility but are not currently supported in task execution."""
    timestamp: NotRequired[str]
    r"""ISO 8601 timestamp of when the status was updated"""
    message: NotRequired[RunAgentTaskStatusMessageTypedDict]
    r"""Optional A2A message providing additional context about the current status"""


class RunAgentTaskStatus(BaseModel):
    r"""Current task status information"""

    state: RunAgentTaskState
    r"""Current state of the agent task execution. Values: submitted (queued), working (executing), input-required (awaiting user input), completed (finished successfully), failed (error occurred). Note: auth-required, canceled, and rejected statuses are defined for A2A protocol compatibility but are not currently supported in task execution."""

    timestamp: Optional[str] = None
    r"""ISO 8601 timestamp of when the status was updated"""

    message: Optional[RunAgentTaskStatusMessage] = None
    r"""Optional A2A message providing additional context about the current status"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["timestamp", "message"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunAgentA2ATaskResponseTypedDict(TypedDict):
    r"""Response format following the Agent-to-Agent (A2A) protocol. Returned when starting or continuing an agent task execution."""

    id: str
    r"""The unique ID of the created agent execution task"""
    context_id: str
    r"""The correlation ID for this execution (used for tracking)"""
    kind: RunAgentKind
    r"""A2A entity type identifier"""
    status: RunAgentTaskStatusTypedDict
    r"""Current task status information"""
    metadata: NotRequired[Dict[str, Any]]
    r"""Task metadata containing workspace_id and trace_id for feedback and tracking"""


class RunAgentA2ATaskResponse(BaseModel):
    r"""Response format following the Agent-to-Agent (A2A) protocol. Returned when starting or continuing an agent task execution."""

    id: str
    r"""The unique ID of the created agent execution task"""

    context_id: Annotated[str, pydantic.Field(alias="contextId")]
    r"""The correlation ID for this execution (used for tracking)"""

    kind: RunAgentKind
    r"""A2A entity type identifier"""

    status: RunAgentTaskStatus
    r"""Current task status information"""

    metadata: Optional[Dict[str, Any]] = None
    r"""Task metadata containing workspace_id and trace_id for feedback and tracking"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["metadata"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


try:
    RunAgentModelConfigurationAudio.model_rebuild()
except NameError:
    pass
try:
    RunAgentResponseFormatAgentsJSONSchema.model_rebuild()
except NameError:
    pass
try:
    RunAgentFallbackModelConfigurationAudio.model_rebuild()
except NameError:
    pass
try:
    RunAgentResponseFormatAgentsRequestRequestBodyJSONSchema.model_rebuild()
except NameError:
    pass
try:
    RunAgentA2AMessage.model_rebuild()
except NameError:
    pass
try:
    RunAgentAgentToolInputRunTools.model_rebuild()
except NameError:
    pass
try:
    MCPToolRun.model_rebuild()
except NameError:
    pass
try:
    AgentToolInputRunJSONSchema.model_rebuild()
except NameError:
    pass
try:
    JSONSchemaToolRun.model_rebuild()
except NameError:
    pass
try:
    FunctionToolRun.model_rebuild()
except NameError:
    pass
try:
    CodeToolRun.model_rebuild()
except NameError:
    pass
try:
    HTTPToolRun.model_rebuild()
except NameError:
    pass
try:
    RunAgentTaskStatusMessage.model_rebuild()
except NameError:
    pass
try:
    RunAgentA2ATaskResponse.model_rebuild()
except NameError:
    pass
