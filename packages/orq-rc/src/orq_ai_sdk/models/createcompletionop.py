"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .publiccontact import PublicContact, PublicContactTypedDict
from .publicidentity import PublicIdentity, PublicIdentityTypedDict
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import eventstreaming
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import (
    Annotated,
    NotRequired,
    TypeAliasType,
    TypedDict,
    deprecated,
)


CreateCompletionStopTypedDict = TypeAliasType(
    "CreateCompletionStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""


CreateCompletionStop = TypeAliasType("CreateCompletionStop", Union[str, List[str]])
r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""


class CreateCompletionFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class CreateCompletionFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


class CreateCompletionRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class CreateCompletionRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateCompletionType = Literal["exact_match",]


class CreateCompletionCacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: CreateCompletionType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class CreateCompletionCache(BaseModel):
    r"""Cache configuration for the request."""

    type: CreateCompletionType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateCompletionLoadBalancerType = Literal["weight_based",]


class CreateCompletionLoadBalancerModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class CreateCompletionLoadBalancerModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateCompletionLoadBalancer1TypedDict(TypedDict):
    type: CreateCompletionLoadBalancerType
    models: List[CreateCompletionLoadBalancerModelsTypedDict]


class CreateCompletionLoadBalancer1(BaseModel):
    type: CreateCompletionLoadBalancerType

    models: List[CreateCompletionLoadBalancerModels]


CreateCompletionLoadBalancerTypedDict = CreateCompletionLoadBalancer1TypedDict
r"""Load balancer configuration for the request."""


CreateCompletionLoadBalancer = CreateCompletionLoadBalancer1
r"""Load balancer configuration for the request."""


class CreateCompletionTimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class CreateCompletionTimeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class CreateCompletionRouterCompletionsRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class CreateCompletionRouterCompletionsRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateCompletionRouterCompletionsFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class CreateCompletionRouterCompletionsFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


CreateCompletionVersion = Literal["latest",]
r"""Version of the prompt to use (currently only \"latest\" supported)"""


class CreateCompletionPromptTypedDict(TypedDict):
    r"""Prompt configuration for the request"""

    id: str
    r"""Unique identifier of the prompt to use"""
    version: CreateCompletionVersion
    r"""Version of the prompt to use (currently only \"latest\" supported)"""


class CreateCompletionPrompt(BaseModel):
    r"""Prompt configuration for the request"""

    id: str
    r"""Unique identifier of the prompt to use"""

    version: CreateCompletionVersion
    r"""Version of the prompt to use (currently only \"latest\" supported)"""


class CreateCompletionThreadTypedDict(TypedDict):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""
    tags: NotRequired[List[str]]
    r"""Optional tags to differentiate or categorize threads"""


class CreateCompletionThread(BaseModel):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""

    tags: Optional[List[str]] = None
    r"""Optional tags to differentiate or categorize threads"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["tags"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateCompletionInputs2TypedDict(TypedDict):
    key: str
    value: NotRequired[Any]
    is_pii: NotRequired[bool]


class CreateCompletionInputs2(BaseModel):
    key: str

    value: Optional[Any] = None

    is_pii: Optional[bool] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["value", "is_pii"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateCompletionInputsTypedDict = TypeAliasType(
    "CreateCompletionInputsTypedDict",
    Union[Dict[str, Any], List[CreateCompletionInputs2TypedDict]],
)
r"""Values to replace in the prompt messages using {{variableName}} syntax"""


CreateCompletionInputs = TypeAliasType(
    "CreateCompletionInputs", Union[Dict[str, Any], List[CreateCompletionInputs2]]
)
r"""Values to replace in the prompt messages using {{variableName}} syntax"""


CreateCompletionRouterCompletionsType = Literal["exact_match",]


class CreateCompletionRouterCompletionsCacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: CreateCompletionRouterCompletionsType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class CreateCompletionRouterCompletionsCache(BaseModel):
    r"""Cache configuration for the request."""

    type: CreateCompletionRouterCompletionsType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateCompletionSearchType = Literal[
    "vector_search",
    "keyword_search",
    "hybrid_search",
]
r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""


class CreateCompletionOrExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class CreateCompletionOrExists(BaseModel):
    r"""Exists"""

    exists: bool


CreateCompletionOrRouterCompletionsNinTypedDict = TypeAliasType(
    "CreateCompletionOrRouterCompletionsNinTypedDict", Union[str, float, bool]
)


CreateCompletionOrRouterCompletionsNin = TypeAliasType(
    "CreateCompletionOrRouterCompletionsNin", Union[str, float, bool]
)


class CreateCompletionOrNinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[CreateCompletionOrRouterCompletionsNinTypedDict]


class CreateCompletionOrNin(BaseModel):
    r"""Not in"""

    nin: List[CreateCompletionOrRouterCompletionsNin]


CreateCompletionOrRouterCompletionsInTypedDict = TypeAliasType(
    "CreateCompletionOrRouterCompletionsInTypedDict", Union[str, float, bool]
)


CreateCompletionOrRouterCompletionsIn = TypeAliasType(
    "CreateCompletionOrRouterCompletionsIn", Union[str, float, bool]
)


class CreateCompletionOrInTypedDict(TypedDict):
    r"""In"""

    in_: List[CreateCompletionOrRouterCompletionsInTypedDict]


class CreateCompletionOrIn(BaseModel):
    r"""In"""

    in_: Annotated[
        List[CreateCompletionOrRouterCompletionsIn], pydantic.Field(alias="in")
    ]


class CreateCompletionOrLteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class CreateCompletionOrLte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class CreateCompletionOrLtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class CreateCompletionOrLt(BaseModel):
    r"""Less than"""

    lt: float


class CreateCompletionOrGteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class CreateCompletionOrGte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class CreateCompletionOrGtTypedDict(TypedDict):
    r"""Greater than"""

    gt: float


class CreateCompletionOrGt(BaseModel):
    r"""Greater than"""

    gt: float


CreateCompletionOrRouterCompletionsNeTypedDict = TypeAliasType(
    "CreateCompletionOrRouterCompletionsNeTypedDict", Union[str, float, bool]
)


CreateCompletionOrRouterCompletionsNe = TypeAliasType(
    "CreateCompletionOrRouterCompletionsNe", Union[str, float, bool]
)


class CreateCompletionOrNeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: CreateCompletionOrRouterCompletionsNeTypedDict


class CreateCompletionOrNe(BaseModel):
    r"""Not equal to"""

    ne: CreateCompletionOrRouterCompletionsNe


CreateCompletionOrRouterCompletionsEqTypedDict = TypeAliasType(
    "CreateCompletionOrRouterCompletionsEqTypedDict", Union[str, float, bool]
)


CreateCompletionOrRouterCompletionsEq = TypeAliasType(
    "CreateCompletionOrRouterCompletionsEq", Union[str, float, bool]
)


class CreateCompletionOrEqTypedDict(TypedDict):
    r"""Equal to"""

    eq: CreateCompletionOrRouterCompletionsEqTypedDict


class CreateCompletionOrEq(BaseModel):
    r"""Equal to"""

    eq: CreateCompletionOrRouterCompletionsEq


CreateCompletionFilterByRouterCompletionsOrTypedDict = TypeAliasType(
    "CreateCompletionFilterByRouterCompletionsOrTypedDict",
    Union[
        CreateCompletionOrEqTypedDict,
        CreateCompletionOrNeTypedDict,
        CreateCompletionOrGtTypedDict,
        CreateCompletionOrGteTypedDict,
        CreateCompletionOrLtTypedDict,
        CreateCompletionOrLteTypedDict,
        CreateCompletionOrInTypedDict,
        CreateCompletionOrNinTypedDict,
        CreateCompletionOrExistsTypedDict,
    ],
)


CreateCompletionFilterByRouterCompletionsOr = TypeAliasType(
    "CreateCompletionFilterByRouterCompletionsOr",
    Union[
        CreateCompletionOrEq,
        CreateCompletionOrNe,
        CreateCompletionOrGt,
        CreateCompletionOrGte,
        CreateCompletionOrLt,
        CreateCompletionOrLte,
        CreateCompletionOrIn,
        CreateCompletionOrNin,
        CreateCompletionOrExists,
    ],
)


class CreateCompletionFilterByOrTypedDict(TypedDict):
    r"""Or"""

    or_: List[Dict[str, CreateCompletionFilterByRouterCompletionsOrTypedDict]]


class CreateCompletionFilterByOr(BaseModel):
    r"""Or"""

    or_: Annotated[
        List[Dict[str, CreateCompletionFilterByRouterCompletionsOr]],
        pydantic.Field(alias="or"),
    ]


class CreateCompletionAndExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class CreateCompletionAndExists(BaseModel):
    r"""Exists"""

    exists: bool


CreateCompletionAndRouterCompletionsNinTypedDict = TypeAliasType(
    "CreateCompletionAndRouterCompletionsNinTypedDict", Union[str, float, bool]
)


CreateCompletionAndRouterCompletionsNin = TypeAliasType(
    "CreateCompletionAndRouterCompletionsNin", Union[str, float, bool]
)


class CreateCompletionAndNinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[CreateCompletionAndRouterCompletionsNinTypedDict]


class CreateCompletionAndNin(BaseModel):
    r"""Not in"""

    nin: List[CreateCompletionAndRouterCompletionsNin]


CreateCompletionAndRouterCompletionsInTypedDict = TypeAliasType(
    "CreateCompletionAndRouterCompletionsInTypedDict", Union[str, float, bool]
)


CreateCompletionAndRouterCompletionsIn = TypeAliasType(
    "CreateCompletionAndRouterCompletionsIn", Union[str, float, bool]
)


class CreateCompletionAndInTypedDict(TypedDict):
    r"""In"""

    in_: List[CreateCompletionAndRouterCompletionsInTypedDict]


class CreateCompletionAndIn(BaseModel):
    r"""In"""

    in_: Annotated[
        List[CreateCompletionAndRouterCompletionsIn], pydantic.Field(alias="in")
    ]


class CreateCompletionAndLteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class CreateCompletionAndLte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class CreateCompletionAndLtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class CreateCompletionAndLt(BaseModel):
    r"""Less than"""

    lt: float


class CreateCompletionAndGteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class CreateCompletionAndGte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class CreateCompletionAndGtTypedDict(TypedDict):
    r"""Greater than"""

    gt: float


class CreateCompletionAndGt(BaseModel):
    r"""Greater than"""

    gt: float


CreateCompletionAndRouterCompletionsNeTypedDict = TypeAliasType(
    "CreateCompletionAndRouterCompletionsNeTypedDict", Union[str, float, bool]
)


CreateCompletionAndRouterCompletionsNe = TypeAliasType(
    "CreateCompletionAndRouterCompletionsNe", Union[str, float, bool]
)


class CreateCompletionAndNeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: CreateCompletionAndRouterCompletionsNeTypedDict


class CreateCompletionAndNe(BaseModel):
    r"""Not equal to"""

    ne: CreateCompletionAndRouterCompletionsNe


CreateCompletionAndRouterCompletionsEqTypedDict = TypeAliasType(
    "CreateCompletionAndRouterCompletionsEqTypedDict", Union[str, float, bool]
)


CreateCompletionAndRouterCompletionsEq = TypeAliasType(
    "CreateCompletionAndRouterCompletionsEq", Union[str, float, bool]
)


class CreateCompletionAndEqTypedDict(TypedDict):
    r"""Equal to"""

    eq: CreateCompletionAndRouterCompletionsEqTypedDict


class CreateCompletionAndEq(BaseModel):
    r"""Equal to"""

    eq: CreateCompletionAndRouterCompletionsEq


CreateCompletionFilterByRouterCompletionsAndTypedDict = TypeAliasType(
    "CreateCompletionFilterByRouterCompletionsAndTypedDict",
    Union[
        CreateCompletionAndEqTypedDict,
        CreateCompletionAndNeTypedDict,
        CreateCompletionAndGtTypedDict,
        CreateCompletionAndGteTypedDict,
        CreateCompletionAndLtTypedDict,
        CreateCompletionAndLteTypedDict,
        CreateCompletionAndInTypedDict,
        CreateCompletionAndNinTypedDict,
        CreateCompletionAndExistsTypedDict,
    ],
)


CreateCompletionFilterByRouterCompletionsAnd = TypeAliasType(
    "CreateCompletionFilterByRouterCompletionsAnd",
    Union[
        CreateCompletionAndEq,
        CreateCompletionAndNe,
        CreateCompletionAndGt,
        CreateCompletionAndGte,
        CreateCompletionAndLt,
        CreateCompletionAndLte,
        CreateCompletionAndIn,
        CreateCompletionAndNin,
        CreateCompletionAndExists,
    ],
)


class CreateCompletionFilterByAndTypedDict(TypedDict):
    r"""And"""

    and_: List[Dict[str, CreateCompletionFilterByRouterCompletionsAndTypedDict]]


class CreateCompletionFilterByAnd(BaseModel):
    r"""And"""

    and_: Annotated[
        List[Dict[str, CreateCompletionFilterByRouterCompletionsAnd]],
        pydantic.Field(alias="and"),
    ]


class CreateCompletion1ExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class CreateCompletion1Exists(BaseModel):
    r"""Exists"""

    exists: bool


CreateCompletion1RouterCompletionsNinTypedDict = TypeAliasType(
    "CreateCompletion1RouterCompletionsNinTypedDict", Union[str, float, bool]
)


CreateCompletion1RouterCompletionsNin = TypeAliasType(
    "CreateCompletion1RouterCompletionsNin", Union[str, float, bool]
)


class CreateCompletion1NinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[CreateCompletion1RouterCompletionsNinTypedDict]


class CreateCompletion1Nin(BaseModel):
    r"""Not in"""

    nin: List[CreateCompletion1RouterCompletionsNin]


CreateCompletion1RouterCompletionsInTypedDict = TypeAliasType(
    "CreateCompletion1RouterCompletionsInTypedDict", Union[str, float, bool]
)


CreateCompletion1RouterCompletionsIn = TypeAliasType(
    "CreateCompletion1RouterCompletionsIn", Union[str, float, bool]
)


class CreateCompletion1InTypedDict(TypedDict):
    r"""In"""

    in_: List[CreateCompletion1RouterCompletionsInTypedDict]


class CreateCompletion1In(BaseModel):
    r"""In"""

    in_: Annotated[
        List[CreateCompletion1RouterCompletionsIn], pydantic.Field(alias="in")
    ]


class CreateCompletion1LteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class CreateCompletion1Lte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class CreateCompletion1LtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class CreateCompletion1Lt(BaseModel):
    r"""Less than"""

    lt: float


class CreateCompletion1GteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class CreateCompletion1Gte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class CreateCompletion1GtTypedDict(TypedDict):
    r"""Greater than"""

    gt: float


class CreateCompletion1Gt(BaseModel):
    r"""Greater than"""

    gt: float


CreateCompletion1RouterCompletionsNeTypedDict = TypeAliasType(
    "CreateCompletion1RouterCompletionsNeTypedDict", Union[str, float, bool]
)


CreateCompletion1RouterCompletionsNe = TypeAliasType(
    "CreateCompletion1RouterCompletionsNe", Union[str, float, bool]
)


class CreateCompletion1NeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: CreateCompletion1RouterCompletionsNeTypedDict


class CreateCompletion1Ne(BaseModel):
    r"""Not equal to"""

    ne: CreateCompletion1RouterCompletionsNe


CreateCompletion1RouterCompletionsEqTypedDict = TypeAliasType(
    "CreateCompletion1RouterCompletionsEqTypedDict", Union[str, float, bool]
)


CreateCompletion1RouterCompletionsEq = TypeAliasType(
    "CreateCompletion1RouterCompletionsEq", Union[str, float, bool]
)


class CreateCompletion1EqTypedDict(TypedDict):
    r"""Equal to"""

    eq: CreateCompletion1RouterCompletionsEqTypedDict


class CreateCompletion1Eq(BaseModel):
    r"""Equal to"""

    eq: CreateCompletion1RouterCompletionsEq


CreateCompletionFilterBy1TypedDict = TypeAliasType(
    "CreateCompletionFilterBy1TypedDict",
    Union[
        CreateCompletion1EqTypedDict,
        CreateCompletion1NeTypedDict,
        CreateCompletion1GtTypedDict,
        CreateCompletion1GteTypedDict,
        CreateCompletion1LtTypedDict,
        CreateCompletion1LteTypedDict,
        CreateCompletion1InTypedDict,
        CreateCompletion1NinTypedDict,
        CreateCompletion1ExistsTypedDict,
    ],
)


CreateCompletionFilterBy1 = TypeAliasType(
    "CreateCompletionFilterBy1",
    Union[
        CreateCompletion1Eq,
        CreateCompletion1Ne,
        CreateCompletion1Gt,
        CreateCompletion1Gte,
        CreateCompletion1Lt,
        CreateCompletion1Lte,
        CreateCompletion1In,
        CreateCompletion1Nin,
        CreateCompletion1Exists,
    ],
)


CreateCompletionFilterByTypedDict = TypeAliasType(
    "CreateCompletionFilterByTypedDict",
    Union[
        CreateCompletionFilterByAndTypedDict,
        CreateCompletionFilterByOrTypedDict,
        Dict[str, CreateCompletionFilterBy1TypedDict],
    ],
)
r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://docs.orq.ai/docs/knowledge/api#knowledge-base-search) for more information."""


CreateCompletionFilterBy = TypeAliasType(
    "CreateCompletionFilterBy",
    Union[
        CreateCompletionFilterByAnd,
        CreateCompletionFilterByOr,
        Dict[str, CreateCompletionFilterBy1],
    ],
)
r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://docs.orq.ai/docs/knowledge/api#knowledge-base-search) for more information."""


class CreateCompletionSearchOptionsTypedDict(TypedDict):
    r"""Additional search options"""

    include_vectors: NotRequired[bool]
    r"""Whether to include the vector in the chunk"""
    include_metadata: NotRequired[bool]
    r"""Whether to include the metadata in the chunk"""
    include_scores: NotRequired[bool]
    r"""Whether to include the scores in the chunk"""


class CreateCompletionSearchOptions(BaseModel):
    r"""Additional search options"""

    include_vectors: Optional[bool] = None
    r"""Whether to include the vector in the chunk"""

    include_metadata: Optional[bool] = None
    r"""Whether to include the metadata in the chunk"""

    include_scores: Optional[bool] = None
    r"""Whether to include the scores in the chunk"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_vectors", "include_metadata", "include_scores"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateCompletionRerankConfigTypedDict(TypedDict):
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    model: str
    r"""The name of the rerank model to use. Refer to the [model list](https://docs.orq.ai/docs/proxy#/rerank-models)."""
    threshold: NotRequired[float]
    r"""The threshold value used to filter the rerank results, only documents with a relevance score greater than the threshold will be returned"""
    top_k: NotRequired[int]
    r"""The number of top results to return after reranking. If not provided, will default to the knowledge base configured `top_k`."""


class CreateCompletionRerankConfig(BaseModel):
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    model: str
    r"""The name of the rerank model to use. Refer to the [model list](https://docs.orq.ai/docs/proxy#/rerank-models)."""

    threshold: Optional[float] = 0
    r"""The threshold value used to filter the rerank results, only documents with a relevance score greater than the threshold will be returned"""

    top_k: Optional[int] = 10
    r"""The number of top results to return after reranking. If not provided, will default to the knowledge base configured `top_k`."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["threshold", "top_k"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateCompletionAgenticRagConfigTypedDict(TypedDict):
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    model: str
    r"""The name of the model for the Agent to use. Refer to the [model list](https://docs.orq.ai/docs/proxy#/chat-models)."""


class CreateCompletionAgenticRagConfig(BaseModel):
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    model: str
    r"""The name of the model for the Agent to use. Refer to the [model list](https://docs.orq.ai/docs/proxy#/chat-models)."""


class CreateCompletionKnowledgeBasesTypedDict(TypedDict):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""
    top_k: NotRequired[int]
    r"""The number of results to return. If not provided, will default to the knowledge base configured `top_k`."""
    threshold: NotRequired[float]
    r"""The threshold to apply to the search. If not provided, will default to the knowledge base configured `threshold`"""
    search_type: NotRequired[CreateCompletionSearchType]
    r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""
    filter_by: NotRequired[CreateCompletionFilterByTypedDict]
    r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://docs.orq.ai/docs/knowledge/api#knowledge-base-search) for more information."""
    search_options: NotRequired[CreateCompletionSearchOptionsTypedDict]
    r"""Additional search options"""
    rerank_config: NotRequired[CreateCompletionRerankConfigTypedDict]
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""
    agentic_rag_config: NotRequired[CreateCompletionAgenticRagConfigTypedDict]
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""
    query: NotRequired[str]
    r"""The query to use to search the knowledge base. If not provided we will use the last user message from the messages of the requests"""


class CreateCompletionKnowledgeBases(BaseModel):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""

    top_k: Optional[int] = None
    r"""The number of results to return. If not provided, will default to the knowledge base configured `top_k`."""

    threshold: Optional[float] = None
    r"""The threshold to apply to the search. If not provided, will default to the knowledge base configured `threshold`"""

    search_type: Optional[CreateCompletionSearchType] = "hybrid_search"
    r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""

    filter_by: Optional[CreateCompletionFilterBy] = None
    r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://docs.orq.ai/docs/knowledge/api#knowledge-base-search) for more information."""

    search_options: Optional[CreateCompletionSearchOptions] = None
    r"""Additional search options"""

    rerank_config: Optional[CreateCompletionRerankConfig] = None
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    agentic_rag_config: Optional[CreateCompletionAgenticRagConfig] = None
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    query: Optional[str] = None
    r"""The query to use to search the knowledge base. If not provided we will use the last user message from the messages of the requests"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "top_k",
                "threshold",
                "search_type",
                "filter_by",
                "search_options",
                "rerank_config",
                "agentic_rag_config",
                "query",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateCompletionLoadBalancerRouterCompletionsType = Literal["weight_based",]


class CreateCompletionLoadBalancerRouterCompletionsModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class CreateCompletionLoadBalancerRouterCompletionsModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateCompletionLoadBalancerRouterCompletions1TypedDict(TypedDict):
    type: CreateCompletionLoadBalancerRouterCompletionsType
    models: List[CreateCompletionLoadBalancerRouterCompletionsModelsTypedDict]


class CreateCompletionLoadBalancerRouterCompletions1(BaseModel):
    type: CreateCompletionLoadBalancerRouterCompletionsType

    models: List[CreateCompletionLoadBalancerRouterCompletionsModels]


CreateCompletionRouterCompletionsLoadBalancerTypedDict = (
    CreateCompletionLoadBalancerRouterCompletions1TypedDict
)
r"""Array of models with weights for load balancing requests"""


CreateCompletionRouterCompletionsLoadBalancer = (
    CreateCompletionLoadBalancerRouterCompletions1
)
r"""Array of models with weights for load balancing requests"""


class CreateCompletionRouterCompletionsTimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class CreateCompletionRouterCompletionsTimeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


@deprecated(
    "warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
)
class CreateCompletionOrqTypedDict(TypedDict):
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, identity-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    retry: NotRequired[CreateCompletionRouterCompletionsRetryTypedDict]
    r"""Retry configuration for the request"""
    fallbacks: NotRequired[List[CreateCompletionRouterCompletionsFallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    prompt: NotRequired[CreateCompletionPromptTypedDict]
    r"""Prompt configuration for the request"""
    identity: NotRequired[PublicIdentityTypedDict]
    r"""Information about the identity making the request. If the identity does not exist, it will be created automatically."""
    contact: NotRequired[PublicContactTypedDict]
    r"""@deprecated Use identity instead. Information about the contact making the request."""
    thread: NotRequired[CreateCompletionThreadTypedDict]
    r"""Thread information to group related requests"""
    inputs: NotRequired[CreateCompletionInputsTypedDict]
    r"""Values to replace in the prompt messages using {{variableName}} syntax"""
    cache: NotRequired[CreateCompletionRouterCompletionsCacheTypedDict]
    r"""Cache configuration for the request."""
    knowledge_bases: NotRequired[List[CreateCompletionKnowledgeBasesTypedDict]]
    load_balancer: NotRequired[CreateCompletionRouterCompletionsLoadBalancerTypedDict]
    r"""Array of models with weights for load balancing requests"""
    timeout: NotRequired[CreateCompletionRouterCompletionsTimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


@deprecated(
    "warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
)
class CreateCompletionOrq(BaseModel):
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, identity-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    retry: Optional[CreateCompletionRouterCompletionsRetry] = None
    r"""Retry configuration for the request"""

    fallbacks: Optional[List[CreateCompletionRouterCompletionsFallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    prompt: Optional[CreateCompletionPrompt] = None
    r"""Prompt configuration for the request"""

    identity: Optional[PublicIdentity] = None
    r"""Information about the identity making the request. If the identity does not exist, it will be created automatically."""

    contact: Annotated[
        Optional[PublicContact],
        pydantic.Field(
            deprecated="warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
        ),
    ] = None
    r"""@deprecated Use identity instead. Information about the contact making the request."""

    thread: Optional[CreateCompletionThread] = None
    r"""Thread information to group related requests"""

    inputs: Optional[CreateCompletionInputs] = None
    r"""Values to replace in the prompt messages using {{variableName}} syntax"""

    cache: Optional[CreateCompletionRouterCompletionsCache] = None
    r"""Cache configuration for the request."""

    knowledge_bases: Optional[List[CreateCompletionKnowledgeBases]] = None

    load_balancer: Optional[CreateCompletionRouterCompletionsLoadBalancer] = None
    r"""Array of models with weights for load balancing requests"""

    timeout: Optional[CreateCompletionRouterCompletionsTimeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "name",
                "retry",
                "fallbacks",
                "prompt",
                "identity",
                "contact",
                "thread",
                "inputs",
                "cache",
                "knowledge_bases",
                "load_balancer",
                "timeout",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateCompletionRequestBodyTypedDict(TypedDict):
    model: str
    r"""ID of the model to use"""
    prompt: str
    r"""The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays."""
    echo: NotRequired[Nullable[bool]]
    r"""Echo back the prompt in addition to the completion"""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[float]]
    r"""The maximum number of tokens that can be generated in the completion."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[CreateCompletionStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."""
    n: NotRequired[Nullable[float]]
    r"""How many completions to generate for each prompt. Note: Because this parameter generates many completions, it can quickly consume your token quota."""
    user: NotRequired[str]
    r"""A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse."""
    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    fallbacks: NotRequired[List[CreateCompletionFallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    retry: NotRequired[CreateCompletionRetryTypedDict]
    r"""Retry configuration for the request"""
    cache: NotRequired[CreateCompletionCacheTypedDict]
    r"""Cache configuration for the request."""
    load_balancer: NotRequired[CreateCompletionLoadBalancerTypedDict]
    r"""Load balancer configuration for the request."""
    timeout: NotRequired[CreateCompletionTimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""
    orq: NotRequired[CreateCompletionOrqTypedDict]
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, identity-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""
    stream: NotRequired[bool]


class CreateCompletionRequestBody(BaseModel):
    model: str
    r"""ID of the model to use"""

    prompt: str
    r"""The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays."""

    echo: OptionalNullable[bool] = False
    r"""Echo back the prompt in addition to the completion"""

    frequency_penalty: OptionalNullable[float] = 0
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[float] = 16
    r"""The maximum number of tokens that can be generated in the completion."""

    presence_penalty: OptionalNullable[float] = 0
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[CreateCompletionStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""

    temperature: OptionalNullable[float] = 1
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = 1
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."""

    n: OptionalNullable[float] = 1
    r"""How many completions to generate for each prompt. Note: Because this parameter generates many completions, it can quickly consume your token quota."""

    user: Optional[str] = None
    r"""A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    fallbacks: Optional[List[CreateCompletionFallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    retry: Optional[CreateCompletionRetry] = None
    r"""Retry configuration for the request"""

    cache: Optional[CreateCompletionCache] = None
    r"""Cache configuration for the request."""

    load_balancer: Optional[CreateCompletionLoadBalancer] = None
    r"""Load balancer configuration for the request."""

    timeout: Optional[CreateCompletionTimeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    orq: Annotated[
        Optional[CreateCompletionOrq],
        pydantic.Field(
            deprecated="warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
        ),
    ] = None
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, identity-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    stream: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "echo",
                "frequency_penalty",
                "max_tokens",
                "presence_penalty",
                "seed",
                "stop",
                "temperature",
                "top_p",
                "n",
                "user",
                "name",
                "fallbacks",
                "retry",
                "cache",
                "load_balancer",
                "timeout",
                "orq",
                "stream",
            ]
        )
        nullable_fields = set(
            [
                "echo",
                "frequency_penalty",
                "max_tokens",
                "presence_penalty",
                "seed",
                "stop",
                "temperature",
                "top_p",
                "n",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateCompletionRouterCompletionsFinishReason = Literal[
    "stop",
    "length",
    "content_filter",
    "tool_calls",
]
r"""The reason the model stopped generating tokens."""


class CreateCompletionRouterCompletionsChoicesTypedDict(TypedDict):
    finish_reason: CreateCompletionRouterCompletionsFinishReason
    r"""The reason the model stopped generating tokens."""
    index: float
    r"""The index of the choice in the list of choices."""
    text: str


class CreateCompletionRouterCompletionsChoices(BaseModel):
    finish_reason: CreateCompletionRouterCompletionsFinishReason
    r"""The reason the model stopped generating tokens."""

    index: float
    r"""The index of the choice in the list of choices."""

    text: str


CreateCompletionCreatedTypedDict = TypeAliasType(
    "CreateCompletionCreatedTypedDict", Union[str, float]
)
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


CreateCompletionCreated = TypeAliasType("CreateCompletionCreated", Union[str, float])
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


class CreateCompletionRouterCompletionsPromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[int]]
    cache_creation_tokens: NotRequired[Nullable[int]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio input tokens consumed by the request."""


class CreateCompletionRouterCompletionsPromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[int] = UNSET

    cache_creation_tokens: OptionalNullable[int] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio input tokens consumed by the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["cached_tokens", "cache_creation_tokens", "audio_tokens"]
        )
        nullable_fields = set(
            ["cached_tokens", "cache_creation_tokens", "audio_tokens"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateCompletionRouterCompletionsCompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]
    accepted_prediction_tokens: NotRequired[Nullable[float]]
    rejected_prediction_tokens: NotRequired[Nullable[float]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio output tokens produced by the response."""


class CreateCompletionRouterCompletionsCompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    accepted_prediction_tokens: OptionalNullable[float] = UNSET

    rejected_prediction_tokens: OptionalNullable[float] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio output tokens produced by the response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "reasoning_tokens",
                "accepted_prediction_tokens",
                "rejected_prediction_tokens",
                "audio_tokens",
            ]
        )
        nullable_fields = set(
            [
                "reasoning_tokens",
                "accepted_prediction_tokens",
                "rejected_prediction_tokens",
                "audio_tokens",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateCompletionRouterCompletionsUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""
    prompt_tokens_details: NotRequired[
        Nullable[CreateCompletionRouterCompletionsPromptTokensDetailsTypedDict]
    ]
    completion_tokens_details: NotRequired[
        Nullable[CreateCompletionRouterCompletionsCompletionTokensDetailsTypedDict]
    ]


class CreateCompletionRouterCompletionsUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""

    prompt_tokens_details: OptionalNullable[
        CreateCompletionRouterCompletionsPromptTokensDetails
    ] = UNSET

    completion_tokens_details: OptionalNullable[
        CreateCompletionRouterCompletionsCompletionTokensDetails
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "completion_tokens",
                "prompt_tokens",
                "total_tokens",
                "prompt_tokens_details",
                "completion_tokens_details",
            ]
        )
        nullable_fields = set(["prompt_tokens_details", "completion_tokens_details"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateCompletionDataTypedDict(TypedDict):
    id: str
    r"""A unique identifier for the completion."""
    choices: List[CreateCompletionRouterCompletionsChoicesTypedDict]
    r"""The list of completion choices the model generated for the input prompt."""
    model: str
    r"""The model used for the chat completion."""
    object: str
    r"""The object type"""
    created: NotRequired[CreateCompletionCreatedTypedDict]
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    system_fingerprint: NotRequired[str]
    r"""This fingerprint represents the backend configuration that the model runs with."""
    usage: NotRequired[CreateCompletionRouterCompletionsUsageTypedDict]
    r"""Usage statistics for the completion request."""


class CreateCompletionData(BaseModel):
    id: str
    r"""A unique identifier for the completion."""

    choices: List[CreateCompletionRouterCompletionsChoices]
    r"""The list of completion choices the model generated for the input prompt."""

    model: str
    r"""The model used for the chat completion."""

    object: str
    r"""The object type"""

    created: Optional[CreateCompletionCreated] = None
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    system_fingerprint: Optional[str] = None
    r"""This fingerprint represents the backend configuration that the model runs with."""

    usage: Optional[CreateCompletionRouterCompletionsUsage] = None
    r"""Usage statistics for the completion request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["created", "system_fingerprint", "usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateCompletionRouterCompletionsResponseBodyTypedDict(TypedDict):
    r"""Represents a completion response from the API."""

    data: NotRequired[CreateCompletionDataTypedDict]


class CreateCompletionRouterCompletionsResponseBody(BaseModel):
    r"""Represents a completion response from the API."""

    data: Optional[CreateCompletionData] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["data"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateCompletionFinishReason = Literal[
    "stop",
    "length",
    "content_filter",
    "tool_calls",
]
r"""The reason the model stopped generating tokens."""


class CreateCompletionChoicesTypedDict(TypedDict):
    finish_reason: CreateCompletionFinishReason
    r"""The reason the model stopped generating tokens."""
    index: float
    r"""The index of the choice in the list of choices."""
    text: str


class CreateCompletionChoices(BaseModel):
    finish_reason: CreateCompletionFinishReason
    r"""The reason the model stopped generating tokens."""

    index: float
    r"""The index of the choice in the list of choices."""

    text: str


CreatedTypedDict = TypeAliasType("CreatedTypedDict", Union[str, float])
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


Created = TypeAliasType("Created", Union[str, float])
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


class CreateCompletionPromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[int]]
    cache_creation_tokens: NotRequired[Nullable[int]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio input tokens consumed by the request."""


class CreateCompletionPromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[int] = UNSET

    cache_creation_tokens: OptionalNullable[int] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio input tokens consumed by the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["cached_tokens", "cache_creation_tokens", "audio_tokens"]
        )
        nullable_fields = set(
            ["cached_tokens", "cache_creation_tokens", "audio_tokens"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateCompletionCompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]
    accepted_prediction_tokens: NotRequired[Nullable[float]]
    rejected_prediction_tokens: NotRequired[Nullable[float]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio output tokens produced by the response."""


class CreateCompletionCompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    accepted_prediction_tokens: OptionalNullable[float] = UNSET

    rejected_prediction_tokens: OptionalNullable[float] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio output tokens produced by the response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "reasoning_tokens",
                "accepted_prediction_tokens",
                "rejected_prediction_tokens",
                "audio_tokens",
            ]
        )
        nullable_fields = set(
            [
                "reasoning_tokens",
                "accepted_prediction_tokens",
                "rejected_prediction_tokens",
                "audio_tokens",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateCompletionUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""
    prompt_tokens_details: NotRequired[
        Nullable[CreateCompletionPromptTokensDetailsTypedDict]
    ]
    completion_tokens_details: NotRequired[
        Nullable[CreateCompletionCompletionTokensDetailsTypedDict]
    ]


class CreateCompletionUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""

    prompt_tokens_details: OptionalNullable[CreateCompletionPromptTokensDetails] = UNSET

    completion_tokens_details: OptionalNullable[
        CreateCompletionCompletionTokensDetails
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "completion_tokens",
                "prompt_tokens",
                "total_tokens",
                "prompt_tokens_details",
                "completion_tokens_details",
            ]
        )
        nullable_fields = set(["prompt_tokens_details", "completion_tokens_details"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateCompletionResponseBodyTypedDict(TypedDict):
    r"""Represents a completion response from the API."""

    id: str
    r"""A unique identifier for the completion."""
    choices: List[CreateCompletionChoicesTypedDict]
    r"""The list of completion choices the model generated for the input prompt."""
    model: str
    r"""The model used for the chat completion."""
    object: str
    r"""The object type"""
    created: NotRequired[CreatedTypedDict]
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    system_fingerprint: NotRequired[str]
    r"""This fingerprint represents the backend configuration that the model runs with."""
    usage: NotRequired[CreateCompletionUsageTypedDict]
    r"""Usage statistics for the completion request."""


class CreateCompletionResponseBody(BaseModel):
    r"""Represents a completion response from the API."""

    id: str
    r"""A unique identifier for the completion."""

    choices: List[CreateCompletionChoices]
    r"""The list of completion choices the model generated for the input prompt."""

    model: str
    r"""The model used for the chat completion."""

    object: str
    r"""The object type"""

    created: Optional[Created] = None
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    system_fingerprint: Optional[str] = None
    r"""This fingerprint represents the backend configuration that the model runs with."""

    usage: Optional[CreateCompletionUsage] = None
    r"""Usage statistics for the completion request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["created", "system_fingerprint", "usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateCompletionResponseTypedDict = TypeAliasType(
    "CreateCompletionResponseTypedDict",
    Union[
        CreateCompletionResponseBodyTypedDict,
        Union[
            eventstreaming.EventStream[
                CreateCompletionRouterCompletionsResponseBodyTypedDict
            ],
            eventstreaming.EventStreamAsync[
                CreateCompletionRouterCompletionsResponseBodyTypedDict
            ],
        ],
    ],
)


CreateCompletionResponse = TypeAliasType(
    "CreateCompletionResponse",
    Union[
        CreateCompletionResponseBody,
        Union[
            eventstreaming.EventStream[CreateCompletionRouterCompletionsResponseBody],
            eventstreaming.EventStreamAsync[
                CreateCompletionRouterCompletionsResponseBody
            ],
        ],
    ],
)
