"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
import dateutil.parser
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import FieldMetadata, QueryParamMetadata, RequestMetadata
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


GetAllPromptsFiltersPromptsRequestRequestBodyType = Literal["string_array"]

GetAllPromptsFiltersPromptsOperator = Literal["in"]


class Filters4TypedDict(TypedDict):
    type: GetAllPromptsFiltersPromptsRequestRequestBodyType
    operator: GetAllPromptsFiltersPromptsOperator
    values: List[str]
    path: str


class Filters4(BaseModel):
    type: GetAllPromptsFiltersPromptsRequestRequestBodyType

    operator: GetAllPromptsFiltersPromptsOperator

    values: List[str]

    path: str


GetAllPromptsFiltersPromptsRequestType = Literal["string"]

GetAllPromptsFiltersOperator = Literal["contains", "equals"]


class GetAllPromptsFilters3TypedDict(TypedDict):
    type: GetAllPromptsFiltersPromptsRequestType
    operator: GetAllPromptsFiltersOperator
    value: str
    path: str


class GetAllPromptsFilters3(BaseModel):
    type: GetAllPromptsFiltersPromptsRequestType

    operator: GetAllPromptsFiltersOperator

    value: str

    path: str


GetAllPromptsFiltersPromptsType = Literal["search"]


class GetAllPromptsFilters2TypedDict(TypedDict):
    type: GetAllPromptsFiltersPromptsType
    value: str
    search_paths: List[str]


class GetAllPromptsFilters2(BaseModel):
    type: GetAllPromptsFiltersPromptsType

    value: str

    search_paths: Annotated[List[str], pydantic.Field(alias="searchPaths")]


GetAllPromptsFiltersType = Literal["id"]


class GetAllPromptsFilters1TypedDict(TypedDict):
    type: GetAllPromptsFiltersType
    id: str
    path: str


class GetAllPromptsFilters1(BaseModel):
    type: GetAllPromptsFiltersType

    id: str

    path: str


GetAllPromptsFiltersTypedDict = TypeAliasType(
    "GetAllPromptsFiltersTypedDict",
    Union[
        GetAllPromptsFilters1TypedDict,
        GetAllPromptsFilters2TypedDict,
        GetAllPromptsFilters3TypedDict,
        Filters4TypedDict,
    ],
)


GetAllPromptsFilters = TypeAliasType(
    "GetAllPromptsFilters",
    Union[
        GetAllPromptsFilters1, GetAllPromptsFilters2, GetAllPromptsFilters3, Filters4
    ],
)


class GetAllPromptsRequestBodyTypedDict(TypedDict):
    filters: List[GetAllPromptsFiltersTypedDict]


class GetAllPromptsRequestBody(BaseModel):
    filters: List[GetAllPromptsFilters]


class GetAllPromptsRequestTypedDict(TypedDict):
    page: NotRequired[str]
    limit: NotRequired[str]
    request_body: NotRequired[GetAllPromptsRequestBodyTypedDict]


class GetAllPromptsRequest(BaseModel):
    page: Annotated[
        Optional[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = None

    limit: Annotated[
        Optional[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = None

    request_body: Annotated[
        Optional[GetAllPromptsRequestBody],
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ] = None


GetAllPromptsOwnerPromptsResponse2 = Literal["vendor"]

GetAllPromptsItemsPromptsResponseOwnerTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsResponseOwnerTypedDict",
    Union[str, GetAllPromptsOwnerPromptsResponse2],
)


GetAllPromptsItemsPromptsResponseOwner = TypeAliasType(
    "GetAllPromptsItemsPromptsResponseOwner",
    Union[str, GetAllPromptsOwnerPromptsResponse2],
)


GetAllPromptsItemsPromptsResponseModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

GetAllPromptsItemsPromptsResponse200Format = Literal[
    "url", "b64_json", "text", "json_object"
]
r"""Only supported on `image` models."""

GetAllPromptsItemsPromptsResponseQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyType = Literal[
    "json_object"
]


class GetAllPromptsResponseFormatPromptsResponse2TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyType


class GetAllPromptsResponseFormatPromptsResponse2(BaseModel):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyType


GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONType = Literal[
    "json_schema"
]


class GetAllPromptsResponseFormatPromptsResponseJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptsResponseFormatPromptsResponseJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptsResponseFormatPromptsResponse1TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONType
    json_schema: GetAllPromptsResponseFormatPromptsResponseJSONSchemaTypedDict


class GetAllPromptsResponseFormatPromptsResponse1(BaseModel):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONType

    json_schema: GetAllPromptsResponseFormatPromptsResponseJSONSchema


GetAllPromptsItemsPromptsResponseResponseFormatTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsResponseResponseFormatTypedDict",
    Union[
        GetAllPromptsResponseFormatPromptsResponse2TypedDict,
        GetAllPromptsResponseFormatPromptsResponse1TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPromptsResponseResponseFormat = TypeAliasType(
    "GetAllPromptsItemsPromptsResponseResponseFormat",
    Union[
        GetAllPromptsResponseFormatPromptsResponse2,
        GetAllPromptsResponseFormatPromptsResponse1,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPromptsResponsePhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

GetAllPromptsItemsPromptsResponseEncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""


class GetAllPromptsItemsPromptsResponseModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[GetAllPromptsItemsPromptsResponse200Format]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptsItemsPromptsResponseQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[GetAllPromptsItemsPromptsResponseResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[GetAllPromptsItemsPromptsResponsePhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[GetAllPromptsItemsPromptsResponseEncodingFormat]
    r"""The format to return the embeddings"""


class GetAllPromptsItemsPromptsResponseModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[GetAllPromptsItemsPromptsResponse200Format] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptsItemsPromptsResponseQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[GetAllPromptsItemsPromptsResponseResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptsItemsPromptsResponsePhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[GetAllPromptsItemsPromptsResponseEncodingFormat] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsItemsPromptsResponseProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
]

GetAllPromptsItemsPromptsResponseRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyType = Literal["image_url"]


class GetAllPrompts2PromptsResponseImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2PromptsResponseImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2PromptsResponse2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyType
    image_url: GetAllPrompts2PromptsResponseImageURLTypedDict


class GetAllPrompts2PromptsResponse2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyType

    image_url: GetAllPrompts2PromptsResponseImageURL


GetAllPrompts2PromptsResponse200ApplicationJSONType = Literal["text"]


class GetAllPrompts2PromptsResponse1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONType
    text: str


class GetAllPrompts2PromptsResponse1(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONType

    text: str


GetAllPromptsContentPromptsResponse2TypedDict = TypeAliasType(
    "GetAllPromptsContentPromptsResponse2TypedDict",
    Union[
        GetAllPrompts2PromptsResponse1TypedDict, GetAllPrompts2PromptsResponse2TypedDict
    ],
)


GetAllPromptsContentPromptsResponse2 = TypeAliasType(
    "GetAllPromptsContentPromptsResponse2",
    Union[GetAllPrompts2PromptsResponse1, GetAllPrompts2PromptsResponse2],
)


GetAllPromptsItemsPromptsResponseContentTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsResponseContentTypedDict",
    Union[str, List[GetAllPromptsContentPromptsResponse2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsResponseContent = TypeAliasType(
    "GetAllPromptsItemsPromptsResponseContent",
    Union[str, List[GetAllPromptsContentPromptsResponse2]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsResponse200ApplicationJSONType = Literal["function"]


class GetAllPromptsItemsPromptsResponseFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsPromptsResponseFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsPromptsResponseToolCallsTypedDict(TypedDict):
    type: GetAllPromptsItemsPromptsResponse200ApplicationJSONType
    function: GetAllPromptsItemsPromptsResponseFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptsItemsPromptsResponseToolCalls(BaseModel):
    type: GetAllPromptsItemsPromptsResponse200ApplicationJSONType

    function: GetAllPromptsItemsPromptsResponseFunction

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptsItemsPromptsResponseMessagesTypedDict(TypedDict):
    role: GetAllPromptsItemsPromptsResponseRole
    r"""The role of the prompt message"""
    content: GetAllPromptsItemsPromptsResponseContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[GetAllPromptsItemsPromptsResponseToolCallsTypedDict]]


class GetAllPromptsItemsPromptsResponseMessages(BaseModel):
    role: GetAllPromptsItemsPromptsResponseRole
    r"""The role of the prompt message"""

    content: GetAllPromptsItemsPromptsResponseContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[GetAllPromptsItemsPromptsResponseToolCalls]] = None


class GetAllPromptsItemsPromptsResponsePromptConfigTypedDict(TypedDict):
    messages: List[GetAllPromptsItemsPromptsResponseMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[GetAllPromptsItemsPromptsResponseModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[
        GetAllPromptsItemsPromptsResponseModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptsItemsPromptsResponseProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptsItemsPromptsResponsePromptConfig(BaseModel):
    messages: List[GetAllPromptsItemsPromptsResponseMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[GetAllPromptsItemsPromptsResponseModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[GetAllPromptsItemsPromptsResponseModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptsItemsPromptsResponseProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptsItemsPromptsResponseMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptsItemsPromptsResponseMetadata(BaseModel):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody3Format = Literal[
    "url", "b64_json", "text", "json_object"
]
r"""Only supported on `image` models."""

GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyQuality = Literal[
    "standard", "hd"
]
r"""Only supported on `image` models."""

GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems3VersionsType = Literal[
    "json_object"
]


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBody2TypedDict(
    TypedDict
):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems3VersionsType


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBody2(
    BaseModel
):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems3VersionsType


GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems3Type = (
    Literal["json_schema"]
)


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyJSONSchemaTypedDict(
    TypedDict
):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyJSONSchema(
    BaseModel
):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBody1TypedDict(
    TypedDict
):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems3Type
    json_schema: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyJSONSchemaTypedDict


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBody1(
    BaseModel
):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems3Type

    json_schema: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyJSONSchema


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyResponseFormatTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyResponseFormatTypedDict",
    Union[
        GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBody2TypedDict,
        GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBody1TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyResponseFormat = (
    TypeAliasType(
        "GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyResponseFormat",
        Union[
            GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBody2,
            GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBody1,
        ],
    )
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyPhotoRealVersion = (
    Literal["v1", "v2"]
)
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyEncodingFormat = Literal[
    "float", "base64"
]
r"""The format to return the embeddings"""


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyModelParametersTypedDict(
    TypedDict
):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody3Format
    ]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyQuality
    ]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[
            GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyResponseFormatTypedDict
        ]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyPhotoRealVersion
    ]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyEncodingFormat
    ]
    r"""The format to return the embeddings"""


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyModelParameters(
    BaseModel
):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody3Format
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyQuality
    ] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[
            GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyResponseFormat
        ],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[
            GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyPhotoRealVersion
        ],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyEncodingFormat
    ] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
]

GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems3VersionsType = Literal[
    "image_url"
]


class GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyImageURLTypedDict(
    TypedDict
):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2PromptsResponse200ApplicationJSONResponseBody2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems3VersionsType
    image_url: (
        GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyImageURLTypedDict
    )


class GetAllPrompts2PromptsResponse200ApplicationJSONResponseBody2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems3VersionsType

    image_url: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyImageURL


GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems3Type = Literal["text"]


class GetAllPrompts2PromptsResponse200ApplicationJSONResponseBody1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems3Type
    text: str


class GetAllPrompts2PromptsResponse200ApplicationJSONResponseBody1(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems3Type

    text: str


GetAllPromptsContentPromptsResponse200ApplicationJSONResponseBody2TypedDict = (
    TypeAliasType(
        "GetAllPromptsContentPromptsResponse200ApplicationJSONResponseBody2TypedDict",
        Union[
            GetAllPrompts2PromptsResponse200ApplicationJSONResponseBody1TypedDict,
            GetAllPrompts2PromptsResponse200ApplicationJSONResponseBody2TypedDict,
        ],
    )
)


GetAllPromptsContentPromptsResponse200ApplicationJSONResponseBody2 = TypeAliasType(
    "GetAllPromptsContentPromptsResponse200ApplicationJSONResponseBody2",
    Union[
        GetAllPrompts2PromptsResponse200ApplicationJSONResponseBody1,
        GetAllPrompts2PromptsResponse200ApplicationJSONResponseBody2,
    ],
)


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyContentTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyContentTypedDict",
    Union[
        str,
        List[
            GetAllPromptsContentPromptsResponse200ApplicationJSONResponseBody2TypedDict
        ],
    ],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyContent = TypeAliasType(
    "GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyContent",
    Union[
        str, List[GetAllPromptsContentPromptsResponse200ApplicationJSONResponseBody2]
    ],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody3Type = Literal[
    "function"
]


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyFunctionTypedDict(
    TypedDict
):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyFunction(
    BaseModel
):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyToolCallsTypedDict(
    TypedDict
):
    type: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody3Type
    function: (
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyFunctionTypedDict
    )
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyToolCalls(
    BaseModel
):
    type: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody3Type

    function: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyFunction

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyMessagesTypedDict(
    TypedDict
):
    role: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyRole
    r"""The role of the prompt message"""
    content: (
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyContentTypedDict
    )
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[
        List[
            GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyToolCallsTypedDict
        ]
    ]


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyMessages(
    BaseModel
):
    role: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyRole
    r"""The role of the prompt message"""

    content: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[
        List[GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyToolCalls]
    ] = None


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyPromptConfigTypedDict(
    TypedDict
):
    messages: List[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyMessagesTypedDict
    ]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyModelType
    ]
    r"""The type of the model"""
    model_parameters: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyProvider
    ]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyPromptConfig(
    BaseModel
):
    messages: List[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyMessages
    ]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyModelType
    ] = None
    r"""The type of the model"""

    model_parameters: Optional[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyModelParameters
    ] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyProvider
    ] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyMetadataTypedDict(
    TypedDict
):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyMetadata(
    BaseModel
):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


class GetAllPromptsItemsPromptsResponseVersionsTypedDict(TypedDict):
    r"""Prompt version model returned from the API"""

    id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyPromptConfigTypedDict
    metadata: (
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyMetadataTypedDict
    )
    commit: str
    timestamp: str
    description: NotRequired[Nullable[str]]


class GetAllPromptsItemsPromptsResponseVersions(BaseModel):
    r"""Prompt version model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: (
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyPromptConfig
    )

    metadata: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyMetadata

    commit: str

    timestamp: str

    description: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsItemsPromptsResponseType = Literal["template"]


class GetAllPromptsItems3TypedDict(TypedDict):
    r"""Prompt template model returned from the API"""

    id: str
    owner: GetAllPromptsItemsPromptsResponseOwnerTypedDict
    domain_id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: GetAllPromptsItemsPromptsResponsePromptConfigTypedDict
    metadata: GetAllPromptsItemsPromptsResponseMetadataTypedDict
    versions: List[GetAllPromptsItemsPromptsResponseVersionsTypedDict]
    type: GetAllPromptsItemsPromptsResponseType
    description: NotRequired[Nullable[str]]
    created: NotRequired[datetime]
    r"""The date and time the resource was created"""
    updated: NotRequired[datetime]
    r"""The date and time the resource was last updated"""


class GetAllPromptsItems3(BaseModel):
    r"""Prompt template model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    owner: GetAllPromptsItemsPromptsResponseOwner

    domain_id: str

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: GetAllPromptsItemsPromptsResponsePromptConfig

    metadata: GetAllPromptsItemsPromptsResponseMetadata

    versions: List[GetAllPromptsItemsPromptsResponseVersions]

    type: GetAllPromptsItemsPromptsResponseType

    description: OptionalNullable[str] = UNSET

    created: Optional[datetime] = None
    r"""The date and time the resource was created"""

    updated: Optional[datetime] = dateutil.parser.isoparse("2024-12-01T21:30:44.576Z")
    r"""The date and time the resource was last updated"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "created", "updated"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsOwnerPrompts2 = Literal["vendor"]

GetAllPromptsItemsPromptsOwnerTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsOwnerTypedDict", Union[str, GetAllPromptsOwnerPrompts2]
)


GetAllPromptsItemsPromptsOwner = TypeAliasType(
    "GetAllPromptsItemsPromptsOwner", Union[str, GetAllPromptsOwnerPrompts2]
)


GetAllPromptsItemsPromptsModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

GetAllPromptsItemsPromptsFormat = Literal["url", "b64_json", "text", "json_object"]
r"""Only supported on `image` models."""

GetAllPromptsItemsPromptsQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

GetAllPromptsResponseFormatPromptsResponse200Type = Literal["json_object"]


class GetAllPromptsResponseFormatPrompts2TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsResponse200Type


class GetAllPromptsResponseFormatPrompts2(BaseModel):
    type: GetAllPromptsResponseFormatPromptsResponse200Type


GetAllPromptsResponseFormatPromptsResponseType = Literal["json_schema"]


class GetAllPromptsResponseFormatPromptsJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptsResponseFormatPromptsJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptsResponseFormatPrompts1TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsResponseType
    json_schema: GetAllPromptsResponseFormatPromptsJSONSchemaTypedDict


class GetAllPromptsResponseFormatPrompts1(BaseModel):
    type: GetAllPromptsResponseFormatPromptsResponseType

    json_schema: GetAllPromptsResponseFormatPromptsJSONSchema


GetAllPromptsItemsPromptsResponseFormatTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsResponseFormatTypedDict",
    Union[
        GetAllPromptsResponseFormatPrompts2TypedDict,
        GetAllPromptsResponseFormatPrompts1TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPromptsResponseFormat = TypeAliasType(
    "GetAllPromptsItemsPromptsResponseFormat",
    Union[GetAllPromptsResponseFormatPrompts2, GetAllPromptsResponseFormatPrompts1],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPromptsPhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

GetAllPromptsItemsPromptsEncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""


class GetAllPromptsItemsPromptsModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[GetAllPromptsItemsPromptsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptsItemsPromptsQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[GetAllPromptsItemsPromptsResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[GetAllPromptsItemsPromptsPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[GetAllPromptsItemsPromptsEncodingFormat]
    r"""The format to return the embeddings"""


class GetAllPromptsItemsPromptsModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[GetAllPromptsItemsPromptsFormat] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptsItemsPromptsQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[GetAllPromptsItemsPromptsResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptsItemsPromptsPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[GetAllPromptsItemsPromptsEncodingFormat] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsItemsPromptsProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
]

GetAllPromptsItemsPromptsRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

GetAllPrompts2PromptsResponse200Type = Literal["image_url"]


class GetAllPrompts2PromptsImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2PromptsImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2Prompts2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsResponse200Type
    image_url: GetAllPrompts2PromptsImageURLTypedDict


class GetAllPrompts2Prompts2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsResponse200Type

    image_url: GetAllPrompts2PromptsImageURL


GetAllPrompts2PromptsResponseType = Literal["text"]


class GetAllPrompts2Prompts1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2PromptsResponseType
    text: str


class GetAllPrompts2Prompts1(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2PromptsResponseType

    text: str


GetAllPromptsContentPrompts2TypedDict = TypeAliasType(
    "GetAllPromptsContentPrompts2TypedDict",
    Union[GetAllPrompts2Prompts1TypedDict, GetAllPrompts2Prompts2TypedDict],
)


GetAllPromptsContentPrompts2 = TypeAliasType(
    "GetAllPromptsContentPrompts2",
    Union[GetAllPrompts2Prompts1, GetAllPrompts2Prompts2],
)


GetAllPromptsItemsPromptsContentTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsContentTypedDict",
    Union[str, List[GetAllPromptsContentPrompts2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsContent = TypeAliasType(
    "GetAllPromptsItemsPromptsContent", Union[str, List[GetAllPromptsContentPrompts2]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyType = Literal[
    "function"
]


class GetAllPromptsItemsPromptsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsPromptsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsPromptsToolCallsTypedDict(TypedDict):
    type: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyType
    function: GetAllPromptsItemsPromptsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptsItemsPromptsToolCalls(BaseModel):
    type: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyType

    function: GetAllPromptsItemsPromptsFunction

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptsItemsPromptsMessagesTypedDict(TypedDict):
    role: GetAllPromptsItemsPromptsRole
    r"""The role of the prompt message"""
    content: GetAllPromptsItemsPromptsContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[GetAllPromptsItemsPromptsToolCallsTypedDict]]


class GetAllPromptsItemsPromptsMessages(BaseModel):
    role: GetAllPromptsItemsPromptsRole
    r"""The role of the prompt message"""

    content: GetAllPromptsItemsPromptsContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[GetAllPromptsItemsPromptsToolCalls]] = None


class GetAllPromptsItemsPromptsPromptConfigTypedDict(TypedDict):
    messages: List[GetAllPromptsItemsPromptsMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[GetAllPromptsItemsPromptsModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[GetAllPromptsItemsPromptsModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptsItemsPromptsProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptsItemsPromptsPromptConfig(BaseModel):
    messages: List[GetAllPromptsItemsPromptsMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[GetAllPromptsItemsPromptsModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[GetAllPromptsItemsPromptsModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptsItemsPromptsProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptsItemsPromptsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptsItemsPromptsMetadata(BaseModel):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


GetAllPromptsItemsPromptsResponse200ModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

GetAllPromptsItemsPromptsResponse200ApplicationJSONFormat = Literal[
    "url", "b64_json", "text", "json_object"
]
r"""Only supported on `image` models."""

GetAllPromptsItemsPromptsResponse200Quality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems2Type = (
    Literal["json_object"]
)


class GetAllPromptsResponseFormatPromptsResponse2002TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems2Type


class GetAllPromptsResponseFormatPromptsResponse2002(BaseModel):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems2Type


GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItemsType = (
    Literal["json_schema"]
)


class GetAllPromptsResponseFormatPromptsResponse200JSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptsResponseFormatPromptsResponse200JSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptsResponseFormatPromptsResponse2001TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItemsType
    json_schema: GetAllPromptsResponseFormatPromptsResponse200JSONSchemaTypedDict


class GetAllPromptsResponseFormatPromptsResponse2001(BaseModel):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItemsType

    json_schema: GetAllPromptsResponseFormatPromptsResponse200JSONSchema


GetAllPromptsItemsPromptsResponse200ResponseFormatTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsResponse200ResponseFormatTypedDict",
    Union[
        GetAllPromptsResponseFormatPromptsResponse2002TypedDict,
        GetAllPromptsResponseFormatPromptsResponse2001TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPromptsResponse200ResponseFormat = TypeAliasType(
    "GetAllPromptsItemsPromptsResponse200ResponseFormat",
    Union[
        GetAllPromptsResponseFormatPromptsResponse2002,
        GetAllPromptsResponseFormatPromptsResponse2001,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPromptsResponse200PhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

GetAllPromptsItemsPromptsResponse200EncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""


class GetAllPromptsItemsPromptsResponse200ModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[GetAllPromptsItemsPromptsResponse200ApplicationJSONFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptsItemsPromptsResponse200Quality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[GetAllPromptsItemsPromptsResponse200ResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[
        GetAllPromptsItemsPromptsResponse200PhotoRealVersion
    ]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[GetAllPromptsItemsPromptsResponse200EncodingFormat]
    r"""The format to return the embeddings"""


class GetAllPromptsItemsPromptsResponse200ModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[GetAllPromptsItemsPromptsResponse200ApplicationJSONFormat] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptsItemsPromptsResponse200Quality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[GetAllPromptsItemsPromptsResponse200ResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptsItemsPromptsResponse200PhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[GetAllPromptsItemsPromptsResponse200EncodingFormat] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsItemsPromptsResponse200Provider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
]

GetAllPromptsItemsPromptsResponse200Role = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems2Type = Literal[
    "image_url"
]


class GetAllPrompts2PromptsResponse200ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2PromptsResponse200ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2PromptsResponse2002TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems2Type
    image_url: GetAllPrompts2PromptsResponse200ImageURLTypedDict


class GetAllPrompts2PromptsResponse2002(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems2Type

    image_url: GetAllPrompts2PromptsResponse200ImageURL


GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItemsType = Literal["text"]


class GetAllPrompts2PromptsResponse2001TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItemsType
    text: str


class GetAllPrompts2PromptsResponse2001(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItemsType

    text: str


GetAllPromptsContentPromptsResponse2002TypedDict = TypeAliasType(
    "GetAllPromptsContentPromptsResponse2002TypedDict",
    Union[
        GetAllPrompts2PromptsResponse2001TypedDict,
        GetAllPrompts2PromptsResponse2002TypedDict,
    ],
)


GetAllPromptsContentPromptsResponse2002 = TypeAliasType(
    "GetAllPromptsContentPromptsResponse2002",
    Union[GetAllPrompts2PromptsResponse2001, GetAllPrompts2PromptsResponse2002],
)


GetAllPromptsItemsPromptsResponse200ContentTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsResponse200ContentTypedDict",
    Union[str, List[GetAllPromptsContentPromptsResponse2002TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsResponse200Content = TypeAliasType(
    "GetAllPromptsItemsPromptsResponse200Content",
    Union[str, List[GetAllPromptsContentPromptsResponse2002]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody2Type = Literal[
    "function"
]


class GetAllPromptsItemsPromptsResponse200FunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsPromptsResponse200Function(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsPromptsResponse200ToolCallsTypedDict(TypedDict):
    type: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody2Type
    function: GetAllPromptsItemsPromptsResponse200FunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptsItemsPromptsResponse200ToolCalls(BaseModel):
    type: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody2Type

    function: GetAllPromptsItemsPromptsResponse200Function

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptsItemsPromptsResponse200MessagesTypedDict(TypedDict):
    role: GetAllPromptsItemsPromptsResponse200Role
    r"""The role of the prompt message"""
    content: GetAllPromptsItemsPromptsResponse200ContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[
        List[GetAllPromptsItemsPromptsResponse200ToolCallsTypedDict]
    ]


class GetAllPromptsItemsPromptsResponse200Messages(BaseModel):
    role: GetAllPromptsItemsPromptsResponse200Role
    r"""The role of the prompt message"""

    content: GetAllPromptsItemsPromptsResponse200Content
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[GetAllPromptsItemsPromptsResponse200ToolCalls]] = None


class GetAllPromptsItemsPromptsResponse200PromptConfigTypedDict(TypedDict):
    messages: List[GetAllPromptsItemsPromptsResponse200MessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[GetAllPromptsItemsPromptsResponse200ModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[
        GetAllPromptsItemsPromptsResponse200ModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptsItemsPromptsResponse200Provider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptsItemsPromptsResponse200PromptConfig(BaseModel):
    messages: List[GetAllPromptsItemsPromptsResponse200Messages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[GetAllPromptsItemsPromptsResponse200ModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[GetAllPromptsItemsPromptsResponse200ModelParameters] = (
        None
    )
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptsItemsPromptsResponse200Provider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptsItemsPromptsResponse200MetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptsItemsPromptsResponse200Metadata(BaseModel):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


class GetAllPromptsItemsPromptsVersionsTypedDict(TypedDict):
    r"""Prompt version model returned from the API"""

    id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: GetAllPromptsItemsPromptsResponse200PromptConfigTypedDict
    metadata: GetAllPromptsItemsPromptsResponse200MetadataTypedDict
    commit: str
    timestamp: str
    description: NotRequired[Nullable[str]]


class GetAllPromptsItemsPromptsVersions(BaseModel):
    r"""Prompt version model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: GetAllPromptsItemsPromptsResponse200PromptConfig

    metadata: GetAllPromptsItemsPromptsResponse200Metadata

    commit: str

    timestamp: str

    description: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsItemsPromptsType = Literal["snippet"]


class GetAllPromptsItems2TypedDict(TypedDict):
    r"""Prompt snippet model returned from the API"""

    id: str
    owner: GetAllPromptsItemsPromptsOwnerTypedDict
    domain_id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: GetAllPromptsItemsPromptsPromptConfigTypedDict
    metadata: GetAllPromptsItemsPromptsMetadataTypedDict
    versions: List[GetAllPromptsItemsPromptsVersionsTypedDict]
    key: str
    type: GetAllPromptsItemsPromptsType
    description: NotRequired[Nullable[str]]
    created: NotRequired[datetime]
    r"""The date and time the resource was created"""
    updated: NotRequired[datetime]
    r"""The date and time the resource was last updated"""


class GetAllPromptsItems2(BaseModel):
    r"""Prompt snippet model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    owner: GetAllPromptsItemsPromptsOwner

    domain_id: str

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: GetAllPromptsItemsPromptsPromptConfig

    metadata: GetAllPromptsItemsPromptsMetadata

    versions: List[GetAllPromptsItemsPromptsVersions]

    key: str

    type: GetAllPromptsItemsPromptsType

    description: OptionalNullable[str] = UNSET

    created: Optional[datetime] = None
    r"""The date and time the resource was created"""

    updated: Optional[datetime] = dateutil.parser.isoparse("2024-12-01T21:30:44.576Z")
    r"""The date and time the resource was last updated"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "created", "updated"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsOwner2 = Literal["vendor"]

GetAllPromptsItemsOwnerTypedDict = TypeAliasType(
    "GetAllPromptsItemsOwnerTypedDict", Union[str, GetAllPromptsOwner2]
)


GetAllPromptsItemsOwner = TypeAliasType(
    "GetAllPromptsItemsOwner", Union[str, GetAllPromptsOwner2]
)


GetAllPromptsItemsModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

GetAllPromptsItemsFormat = Literal["url", "b64_json", "text", "json_object"]
r"""Only supported on `image` models."""

GetAllPromptsItemsQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

GetAllPromptsResponseFormatPromptsType = Literal["json_object"]


class GetAllPromptsResponseFormat2TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsType


class GetAllPromptsResponseFormat2(BaseModel):
    type: GetAllPromptsResponseFormatPromptsType


GetAllPromptsResponseFormatType = Literal["json_schema"]


class GetAllPromptsResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptsResponseFormatJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptsResponseFormat1TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatType
    json_schema: GetAllPromptsResponseFormatJSONSchemaTypedDict


class GetAllPromptsResponseFormat1(BaseModel):
    type: GetAllPromptsResponseFormatType

    json_schema: GetAllPromptsResponseFormatJSONSchema


GetAllPromptsItemsResponseFormatTypedDict = TypeAliasType(
    "GetAllPromptsItemsResponseFormatTypedDict",
    Union[GetAllPromptsResponseFormat2TypedDict, GetAllPromptsResponseFormat1TypedDict],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsResponseFormat = TypeAliasType(
    "GetAllPromptsItemsResponseFormat",
    Union[GetAllPromptsResponseFormat2, GetAllPromptsResponseFormat1],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

GetAllPromptsItemsEncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""


class GetAllPromptsItemsModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[GetAllPromptsItemsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptsItemsQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[Nullable[GetAllPromptsItemsResponseFormatTypedDict]]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[GetAllPromptsItemsPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[GetAllPromptsItemsEncodingFormat]
    r"""The format to return the embeddings"""


class GetAllPromptsItemsModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[GetAllPromptsItemsFormat] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptsItemsQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[GetAllPromptsItemsResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptsItemsPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[GetAllPromptsItemsEncodingFormat] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsItemsProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
]

GetAllPromptsItemsRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

GetAllPrompts2PromptsType = Literal["image_url"]


class GetAllPrompts2ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts22TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsType
    image_url: GetAllPrompts2ImageURLTypedDict


class GetAllPrompts22(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsType

    image_url: GetAllPrompts2ImageURL


GetAllPrompts2Type = Literal["text"]


class GetAllPrompts21TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2Type
    text: str


class GetAllPrompts21(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2Type

    text: str


GetAllPromptsContent2TypedDict = TypeAliasType(
    "GetAllPromptsContent2TypedDict",
    Union[GetAllPrompts21TypedDict, GetAllPrompts22TypedDict],
)


GetAllPromptsContent2 = TypeAliasType(
    "GetAllPromptsContent2", Union[GetAllPrompts21, GetAllPrompts22]
)


GetAllPromptsItemsContentTypedDict = TypeAliasType(
    "GetAllPromptsItemsContentTypedDict",
    Union[str, List[GetAllPromptsContent2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsContent = TypeAliasType(
    "GetAllPromptsItemsContent", Union[str, List[GetAllPromptsContent2]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsResponse200Type = Literal["function"]


class GetAllPromptsItemsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsToolCallsTypedDict(TypedDict):
    type: GetAllPromptsItemsPromptsResponse200Type
    function: GetAllPromptsItemsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptsItemsToolCalls(BaseModel):
    type: GetAllPromptsItemsPromptsResponse200Type

    function: GetAllPromptsItemsFunction

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptsItemsMessagesTypedDict(TypedDict):
    role: GetAllPromptsItemsRole
    r"""The role of the prompt message"""
    content: GetAllPromptsItemsContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[GetAllPromptsItemsToolCallsTypedDict]]


class GetAllPromptsItemsMessages(BaseModel):
    role: GetAllPromptsItemsRole
    r"""The role of the prompt message"""

    content: GetAllPromptsItemsContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[GetAllPromptsItemsToolCalls]] = None


class GetAllPromptsItemsPromptConfigTypedDict(TypedDict):
    messages: List[GetAllPromptsItemsMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[GetAllPromptsItemsModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[GetAllPromptsItemsModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptsItemsProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptsItemsPromptConfig(BaseModel):
    messages: List[GetAllPromptsItemsMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[GetAllPromptsItemsModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[GetAllPromptsItemsModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptsItemsProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptsItemsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptsItemsMetadata(BaseModel):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


GetAllPromptsItemsPromptsResponse200ApplicationJSONModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyFormat = Literal[
    "url", "b64_json", "text", "json_object"
]
r"""Only supported on `image` models."""

GetAllPromptsItemsPromptsResponse200ApplicationJSONQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems1VersionsType = Literal[
    "json_object"
]


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSON2TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems1VersionsType


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSON2(BaseModel):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems1VersionsType


GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems1Type = (
    Literal["json_schema"]
)


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONJSONSchemaTypedDict(
    TypedDict
):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSON1TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems1Type
    json_schema: (
        GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONJSONSchemaTypedDict
    )


class GetAllPromptsResponseFormatPromptsResponse200ApplicationJSON1(BaseModel):
    type: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONResponseBodyItems1Type

    json_schema: GetAllPromptsResponseFormatPromptsResponse200ApplicationJSONJSONSchema


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseFormatTypedDict = (
    TypeAliasType(
        "GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseFormatTypedDict",
        Union[
            GetAllPromptsResponseFormatPromptsResponse200ApplicationJSON2TypedDict,
            GetAllPromptsResponseFormatPromptsResponse200ApplicationJSON1TypedDict,
        ],
    )
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseFormat = TypeAliasType(
    "GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseFormat",
    Union[
        GetAllPromptsResponseFormatPromptsResponse200ApplicationJSON2,
        GetAllPromptsResponseFormatPromptsResponse200ApplicationJSON1,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsItemsPromptsResponse200ApplicationJSONPhotoRealVersion = Literal[
    "v1", "v2"
]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

GetAllPromptsItemsPromptsResponse200ApplicationJSONEncodingFormat = Literal[
    "float", "base64"
]
r"""The format to return the embeddings"""


class GetAllPromptsItemsPromptsResponse200ApplicationJSONModelParametersTypedDict(
    TypedDict
):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyFormat
    ]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptsItemsPromptsResponse200ApplicationJSONQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[
            GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseFormatTypedDict
        ]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONPhotoRealVersion
    ]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONEncodingFormat
    ]
    r"""The format to return the embeddings"""


class GetAllPromptsItemsPromptsResponse200ApplicationJSONModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBodyFormat
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptsItemsPromptsResponse200ApplicationJSONQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[
            GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseFormat
        ],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptsItemsPromptsResponse200ApplicationJSONPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONEncodingFormat
    ] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsItemsPromptsResponse200ApplicationJSONProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
]

GetAllPromptsItemsPromptsResponse200ApplicationJSONRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems1VersionsType = Literal[
    "image_url"
]


class GetAllPrompts2PromptsResponse200ApplicationJSONImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2PromptsResponse200ApplicationJSONImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2PromptsResponse200ApplicationJSON2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems1VersionsType
    image_url: GetAllPrompts2PromptsResponse200ApplicationJSONImageURLTypedDict


class GetAllPrompts2PromptsResponse200ApplicationJSON2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems1VersionsType

    image_url: GetAllPrompts2PromptsResponse200ApplicationJSONImageURL


GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems1Type = Literal["text"]


class GetAllPrompts2PromptsResponse200ApplicationJSON1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems1Type
    text: str


class GetAllPrompts2PromptsResponse200ApplicationJSON1(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2PromptsResponse200ApplicationJSONResponseBodyItems1Type

    text: str


GetAllPromptsContentPromptsResponse200ApplicationJSON2TypedDict = TypeAliasType(
    "GetAllPromptsContentPromptsResponse200ApplicationJSON2TypedDict",
    Union[
        GetAllPrompts2PromptsResponse200ApplicationJSON1TypedDict,
        GetAllPrompts2PromptsResponse200ApplicationJSON2TypedDict,
    ],
)


GetAllPromptsContentPromptsResponse200ApplicationJSON2 = TypeAliasType(
    "GetAllPromptsContentPromptsResponse200ApplicationJSON2",
    Union[
        GetAllPrompts2PromptsResponse200ApplicationJSON1,
        GetAllPrompts2PromptsResponse200ApplicationJSON2,
    ],
)


GetAllPromptsItemsPromptsResponse200ApplicationJSONContentTypedDict = TypeAliasType(
    "GetAllPromptsItemsPromptsResponse200ApplicationJSONContentTypedDict",
    Union[str, List[GetAllPromptsContentPromptsResponse200ApplicationJSON2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsResponse200ApplicationJSONContent = TypeAliasType(
    "GetAllPromptsItemsPromptsResponse200ApplicationJSONContent",
    Union[str, List[GetAllPromptsContentPromptsResponse200ApplicationJSON2]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody1Type = Literal[
    "function"
]


class GetAllPromptsItemsPromptsResponse200ApplicationJSONFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsPromptsResponse200ApplicationJSONFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsItemsPromptsResponse200ApplicationJSONToolCallsTypedDict(TypedDict):
    type: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody1Type
    function: GetAllPromptsItemsPromptsResponse200ApplicationJSONFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptsItemsPromptsResponse200ApplicationJSONToolCalls(BaseModel):
    type: GetAllPromptsItemsPromptsResponse200ApplicationJSONResponseBody1Type

    function: GetAllPromptsItemsPromptsResponse200ApplicationJSONFunction

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptsItemsPromptsResponse200ApplicationJSONMessagesTypedDict(TypedDict):
    role: GetAllPromptsItemsPromptsResponse200ApplicationJSONRole
    r"""The role of the prompt message"""
    content: GetAllPromptsItemsPromptsResponse200ApplicationJSONContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[
        List[GetAllPromptsItemsPromptsResponse200ApplicationJSONToolCallsTypedDict]
    ]


class GetAllPromptsItemsPromptsResponse200ApplicationJSONMessages(BaseModel):
    role: GetAllPromptsItemsPromptsResponse200ApplicationJSONRole
    r"""The role of the prompt message"""

    content: GetAllPromptsItemsPromptsResponse200ApplicationJSONContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[
        List[GetAllPromptsItemsPromptsResponse200ApplicationJSONToolCalls]
    ] = None


class GetAllPromptsItemsPromptsResponse200ApplicationJSONPromptConfigTypedDict(
    TypedDict
):
    messages: List[GetAllPromptsItemsPromptsResponse200ApplicationJSONMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONModelType
    ]
    r"""The type of the model"""
    model_parameters: NotRequired[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptsItemsPromptsResponse200ApplicationJSONProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptsItemsPromptsResponse200ApplicationJSONPromptConfig(BaseModel):
    messages: List[GetAllPromptsItemsPromptsResponse200ApplicationJSONMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONModelType
    ] = None
    r"""The type of the model"""

    model_parameters: Optional[
        GetAllPromptsItemsPromptsResponse200ApplicationJSONModelParameters
    ] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptsItemsPromptsResponse200ApplicationJSONProvider] = (
        None
    )

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptsItemsPromptsResponse200ApplicationJSONMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptsItemsPromptsResponse200ApplicationJSONMetadata(BaseModel):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


class GetAllPromptsItemsVersionsTypedDict(TypedDict):
    r"""Prompt version model returned from the API"""

    id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: (
        GetAllPromptsItemsPromptsResponse200ApplicationJSONPromptConfigTypedDict
    )
    metadata: GetAllPromptsItemsPromptsResponse200ApplicationJSONMetadataTypedDict
    commit: str
    timestamp: str
    description: NotRequired[Nullable[str]]


class GetAllPromptsItemsVersions(BaseModel):
    r"""Prompt version model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: GetAllPromptsItemsPromptsResponse200ApplicationJSONPromptConfig

    metadata: GetAllPromptsItemsPromptsResponse200ApplicationJSONMetadata

    commit: str

    timestamp: str

    description: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsItemsType = Literal["prompt"]


class GetAllPromptsItems1TypedDict(TypedDict):
    r"""Prompt model returned from the API"""

    id: str
    owner: GetAllPromptsItemsOwnerTypedDict
    domain_id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: GetAllPromptsItemsPromptConfigTypedDict
    metadata: GetAllPromptsItemsMetadataTypedDict
    versions: List[GetAllPromptsItemsVersionsTypedDict]
    type: GetAllPromptsItemsType
    description: NotRequired[Nullable[str]]
    created: NotRequired[datetime]
    r"""The date and time the resource was created"""
    updated: NotRequired[datetime]
    r"""The date and time the resource was last updated"""


class GetAllPromptsItems1(BaseModel):
    r"""Prompt model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    owner: GetAllPromptsItemsOwner

    domain_id: str

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: GetAllPromptsItemsPromptConfig

    metadata: GetAllPromptsItemsMetadata

    versions: List[GetAllPromptsItemsVersions]

    type: GetAllPromptsItemsType

    description: OptionalNullable[str] = UNSET

    created: Optional[datetime] = None
    r"""The date and time the resource was created"""

    updated: Optional[datetime] = dateutil.parser.isoparse("2024-12-01T21:30:44.576Z")
    r"""The date and time the resource was last updated"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "created", "updated"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetAllPromptsItemsTypedDict = TypeAliasType(
    "GetAllPromptsItemsTypedDict",
    Union[
        GetAllPromptsItems1TypedDict,
        GetAllPromptsItems3TypedDict,
        GetAllPromptsItems2TypedDict,
    ],
)


GetAllPromptsItems = TypeAliasType(
    "GetAllPromptsItems",
    Union[GetAllPromptsItems1, GetAllPromptsItems3, GetAllPromptsItems2],
)


class GetAllPromptsResponseBodyTypedDict(TypedDict):
    r"""Prompts retrieved."""

    count: float
    page: float
    limit: float
    total_pages: float
    items: List[GetAllPromptsItemsTypedDict]


class GetAllPromptsResponseBody(BaseModel):
    r"""Prompts retrieved."""

    count: float

    page: float

    limit: float

    total_pages: Annotated[float, pydantic.Field(alias="totalPages")]

    items: List[GetAllPromptsItems]
