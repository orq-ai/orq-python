"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .thinkingconfigdisabledschema import (
    ThinkingConfigDisabledSchema,
    ThinkingConfigDisabledSchemaTypedDict,
)
from .thinkingconfigenabledschema import (
    ThinkingConfigEnabledSchema,
    ThinkingConfigEnabledSchemaTypedDict,
)
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import get_discriminator
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


Voice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


ModelConfigurationFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class ModelConfigurationAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: Voice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: ModelConfigurationFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class ModelConfigurationAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: Voice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[ModelConfigurationFormat, pydantic.Field(alias="format")]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


CreateAgentRequestResponseFormatAgentsType = Literal["json_schema",]


class ResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class ResponseFormatJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = False
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "schema", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class JSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreateAgentRequestResponseFormatAgentsType
    json_schema: ResponseFormatJSONSchemaTypedDict


class JSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreateAgentRequestResponseFormatAgentsType

    json_schema: ResponseFormatJSONSchema


CreateAgentRequestResponseFormatType = Literal["json_object",]


class JSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreateAgentRequestResponseFormatType


class JSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreateAgentRequestResponseFormatType


ResponseFormatType = Literal["text",]


class TextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: ResponseFormatType


class Text(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: ResponseFormatType


ResponseFormatTypedDict = TypeAliasType(
    "ResponseFormatTypedDict",
    Union[TextTypedDict, JSONObjectTypedDict, JSONSchemaTypedDict],
)
r"""An object specifying the format that the model must output"""


ResponseFormat = Annotated[
    Union[
        Annotated[Text, Tag("text")],
        Annotated[JSONObject, Tag("json_object")],
        Annotated[JSONSchema, Tag("json_schema")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An object specifying the format that the model must output"""


ReasoningEffort = Literal[
    "none",
    "minimal",
    "low",
    "medium",
    "high",
    "xhigh",
]
r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

- `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
- All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
- The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
- `xhigh` is currently only supported for `gpt-5.1-codex-max`.

Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
"""


StopTypedDict = TypeAliasType("StopTypedDict", Union[str, List[str]])
r"""Up to 4 sequences where the API will stop generating further tokens."""


Stop = TypeAliasType("Stop", Union[str, List[str]])
r"""Up to 4 sequences where the API will stop generating further tokens."""


class StreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class StreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ThinkingTypedDict = TypeAliasType(
    "ThinkingTypedDict",
    Union[ThinkingConfigDisabledSchemaTypedDict, ThinkingConfigEnabledSchemaTypedDict],
)


Thinking = Annotated[
    Union[
        Annotated[ThinkingConfigDisabledSchema, Tag("disabled")],
        Annotated[ThinkingConfigEnabledSchema, Tag("enabled")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


ToolChoiceType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class ToolChoiceFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to call."""


class ToolChoiceFunction(BaseModel):
    name: str
    r"""The name of the function to call."""


class ToolChoice2TypedDict(TypedDict):
    function: ToolChoiceFunctionTypedDict
    type: NotRequired[ToolChoiceType]
    r"""The type of the tool. Currently, only function is supported."""


class ToolChoice2(BaseModel):
    function: ToolChoiceFunction

    type: Optional[ToolChoiceType] = None
    r"""The type of the tool. Currently, only function is supported."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ToolChoice1 = Literal[
    "none",
    "auto",
    "required",
]


ToolChoiceTypedDict = TypeAliasType(
    "ToolChoiceTypedDict", Union[ToolChoice2TypedDict, ToolChoice1]
)
r"""Controls which (if any) tool is called by the model."""


ToolChoice = TypeAliasType("ToolChoice", Union[ToolChoice2, ToolChoice1])
r"""Controls which (if any) tool is called by the model."""


Modalities = Literal[
    "text",
    "audio",
]


ID1 = Literal[
    "orq_pii_detection",
    "orq_sexual_moderation",
    "orq_harmful_moderation",
]
r"""The key of the guardrail."""


IDTypedDict = TypeAliasType("IDTypedDict", Union[ID1, str])


ID = TypeAliasType("ID", Union[ID1, str])


CreateAgentRequestModelConfigurationExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateAgentRequestModelConfigurationGuardrailsTypedDict(TypedDict):
    id: IDTypedDict
    execute_on: CreateAgentRequestModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateAgentRequestModelConfigurationGuardrails(BaseModel):
    id: ID

    execute_on: CreateAgentRequestModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class FallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class Fallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


class RetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class Retry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ModelConfigurationType = Literal["exact_match",]


class CacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: ModelConfigurationType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class Cache(BaseModel):
    r"""Cache configuration for the request."""

    type: ModelConfigurationType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


LoadBalancerType = Literal["weight_based",]


class CreateAgentRequestLoadBalancerModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class CreateAgentRequestLoadBalancerModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class LoadBalancer1TypedDict(TypedDict):
    type: LoadBalancerType
    models: List[CreateAgentRequestLoadBalancerModelsTypedDict]


class LoadBalancer1(BaseModel):
    type: LoadBalancerType

    models: List[CreateAgentRequestLoadBalancerModels]


LoadBalancerTypedDict = LoadBalancer1TypedDict
r"""Load balancer configuration for the request."""


LoadBalancer = LoadBalancer1
r"""Load balancer configuration for the request."""


class TimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class Timeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class ParametersTypedDict(TypedDict):
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""

    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    audio: NotRequired[Nullable[ModelConfigurationAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[ResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[ReasoningEffort]
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[StopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[Nullable[StreamOptionsTypedDict]]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[ThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[ToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[Nullable[List[Modalities]]]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    guardrails: NotRequired[
        List[CreateAgentRequestModelConfigurationGuardrailsTypedDict]
    ]
    r"""A list of guardrails to apply to the request."""
    fallbacks: NotRequired[List[FallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    retry: NotRequired[RetryTypedDict]
    r"""Retry configuration for the request"""
    cache: NotRequired[CacheTypedDict]
    r"""Cache configuration for the request."""
    load_balancer: NotRequired[LoadBalancerTypedDict]
    r"""Load balancer configuration for the request."""
    timeout: NotRequired[TimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


class Parameters(BaseModel):
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    audio: OptionalNullable[ModelConfigurationAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[ResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[ReasoningEffort] = None
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[Stop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[StreamOptions] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[Thinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[ToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[Modalities]] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    guardrails: Optional[List[CreateAgentRequestModelConfigurationGuardrails]] = None
    r"""A list of guardrails to apply to the request."""

    fallbacks: Optional[List[Fallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    retry: Optional[Retry] = None
    r"""Retry configuration for the request"""

    cache: Optional[Cache] = None
    r"""Cache configuration for the request."""

    load_balancer: Optional[LoadBalancer] = None
    r"""Load balancer configuration for the request."""

    timeout: Optional[Timeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "name",
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "response_format",
                "reasoning_effort",
                "verbosity",
                "seed",
                "stop",
                "stream_options",
                "thinking",
                "temperature",
                "top_p",
                "top_k",
                "tool_choice",
                "parallel_tool_calls",
                "modalities",
                "guardrails",
                "fallbacks",
                "retry",
                "cache",
                "load_balancer",
                "timeout",
            ]
        )
        nullable_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "seed",
                "stop",
                "stream_options",
                "temperature",
                "top_p",
                "top_k",
                "modalities",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class ModelConfigurationRetryTypedDict(TypedDict):
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class ModelConfigurationRetry(BaseModel):
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class ModelConfiguration2TypedDict(TypedDict):
    r"""

    Model configuration with parameters and retry settings.
    """

    id: str
    r"""A model ID string (e.g., `openai/gpt-4o` or `anthropic/claude-haiku-4-5-20251001`). Only models that support tool calling can be used with agents."""
    parameters: NotRequired[ParametersTypedDict]
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""
    retry: NotRequired[ModelConfigurationRetryTypedDict]
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""


class ModelConfiguration2(BaseModel):
    r"""

    Model configuration with parameters and retry settings.
    """

    id: str
    r"""A model ID string (e.g., `openai/gpt-4o` or `anthropic/claude-haiku-4-5-20251001`). Only models that support tool calling can be used with agents."""

    parameters: Optional[Parameters] = None
    r"""Model behavior parameters that control how the model generates responses. Common parameters: `temperature` (0-1, randomness), `max_completion_tokens` (max output length), `top_p` (sampling diversity). Advanced: `frequency_penalty`, `presence_penalty`, `response_format` (JSON/structured), `reasoning_effort`, `seed` (reproducibility). Support varies by model - consult AI Gateway documentation."""

    retry: Optional[ModelConfigurationRetry] = None
    r"""Retry configuration for model requests. Retries are triggered for specific HTTP status codes (e.g., 500, 429, 502, 503, 504). Supports configurable retry count (1-5) and custom status codes."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["parameters", "retry"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ModelConfigurationTypedDict = TypeAliasType(
    "ModelConfigurationTypedDict", Union[ModelConfiguration2TypedDict, str]
)
r"""Model configuration for agent execution. Can be a simple model ID string or a configuration object with optional behavior parameters and retry settings."""


ModelConfiguration = TypeAliasType(
    "ModelConfiguration", Union[ModelConfiguration2, str]
)
r"""Model configuration for agent execution. Can be a simple model ID string or a configuration object with optional behavior parameters and retry settings."""


FallbackModelConfigurationVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


FallbackModelConfigurationFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class FallbackModelConfigurationAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: FallbackModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: FallbackModelConfigurationFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class FallbackModelConfigurationAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: FallbackModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[FallbackModelConfigurationFormat, pydantic.Field(alias="format")]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


CreateAgentRequestResponseFormatAgentsRequestRequestBodyFallbackModelsType = Literal[
    "json_schema",
]


class CreateAgentRequestResponseFormatAgentsJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class CreateAgentRequestResponseFormatAgentsJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = False
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "schema", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestResponseFormatJSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreateAgentRequestResponseFormatAgentsRequestRequestBodyFallbackModelsType
    json_schema: CreateAgentRequestResponseFormatAgentsJSONSchemaTypedDict


class CreateAgentRequestResponseFormatJSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreateAgentRequestResponseFormatAgentsRequestRequestBodyFallbackModelsType

    json_schema: CreateAgentRequestResponseFormatAgentsJSONSchema


CreateAgentRequestResponseFormatAgentsRequestRequestBodyType = Literal["json_object",]


class ResponseFormatJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreateAgentRequestResponseFormatAgentsRequestRequestBodyType


class ResponseFormatJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreateAgentRequestResponseFormatAgentsRequestRequestBodyType


CreateAgentRequestResponseFormatAgentsRequestType = Literal["text",]


class ResponseFormatTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: CreateAgentRequestResponseFormatAgentsRequestType


class ResponseFormatText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: CreateAgentRequestResponseFormatAgentsRequestType


FallbackModelConfigurationResponseFormatTypedDict = TypeAliasType(
    "FallbackModelConfigurationResponseFormatTypedDict",
    Union[
        ResponseFormatTextTypedDict,
        ResponseFormatJSONObjectTypedDict,
        CreateAgentRequestResponseFormatJSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


FallbackModelConfigurationResponseFormat = Annotated[
    Union[
        Annotated[ResponseFormatText, Tag("text")],
        Annotated[ResponseFormatJSONObject, Tag("json_object")],
        Annotated[CreateAgentRequestResponseFormatJSONSchema, Tag("json_schema")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An object specifying the format that the model must output"""


FallbackModelConfigurationReasoningEffort = Literal[
    "none",
    "minimal",
    "low",
    "medium",
    "high",
    "xhigh",
]
r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

- `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
- All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
- The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
- `xhigh` is currently only supported for `gpt-5.1-codex-max`.

Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
"""


FallbackModelConfigurationStopTypedDict = TypeAliasType(
    "FallbackModelConfigurationStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


FallbackModelConfigurationStop = TypeAliasType(
    "FallbackModelConfigurationStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class FallbackModelConfigurationStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class FallbackModelConfigurationStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


FallbackModelConfigurationThinkingTypedDict = TypeAliasType(
    "FallbackModelConfigurationThinkingTypedDict",
    Union[ThinkingConfigDisabledSchemaTypedDict, ThinkingConfigEnabledSchemaTypedDict],
)


FallbackModelConfigurationThinking = Annotated[
    Union[
        Annotated[ThinkingConfigDisabledSchema, Tag("disabled")],
        Annotated[ThinkingConfigEnabledSchema, Tag("enabled")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


CreateAgentRequestToolChoiceType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class CreateAgentRequestToolChoiceFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to call."""


class CreateAgentRequestToolChoiceFunction(BaseModel):
    name: str
    r"""The name of the function to call."""


class CreateAgentRequestToolChoice2TypedDict(TypedDict):
    function: CreateAgentRequestToolChoiceFunctionTypedDict
    type: NotRequired[CreateAgentRequestToolChoiceType]
    r"""The type of the tool. Currently, only function is supported."""


class CreateAgentRequestToolChoice2(BaseModel):
    function: CreateAgentRequestToolChoiceFunction

    type: Optional[CreateAgentRequestToolChoiceType] = None
    r"""The type of the tool. Currently, only function is supported."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestToolChoice1 = Literal[
    "none",
    "auto",
    "required",
]


FallbackModelConfigurationToolChoiceTypedDict = TypeAliasType(
    "FallbackModelConfigurationToolChoiceTypedDict",
    Union[CreateAgentRequestToolChoice2TypedDict, CreateAgentRequestToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


FallbackModelConfigurationToolChoice = TypeAliasType(
    "FallbackModelConfigurationToolChoice",
    Union[CreateAgentRequestToolChoice2, CreateAgentRequestToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


FallbackModelConfigurationModalities = Literal[
    "text",
    "audio",
]


CreateAgentRequestID1 = Literal[
    "orq_pii_detection",
    "orq_sexual_moderation",
    "orq_harmful_moderation",
]
r"""The key of the guardrail."""


FallbackModelConfigurationIDTypedDict = TypeAliasType(
    "FallbackModelConfigurationIDTypedDict", Union[CreateAgentRequestID1, str]
)


FallbackModelConfigurationID = TypeAliasType(
    "FallbackModelConfigurationID", Union[CreateAgentRequestID1, str]
)


FallbackModelConfigurationExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class FallbackModelConfigurationGuardrailsTypedDict(TypedDict):
    id: FallbackModelConfigurationIDTypedDict
    execute_on: FallbackModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class FallbackModelConfigurationGuardrails(BaseModel):
    id: FallbackModelConfigurationID

    execute_on: FallbackModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class FallbackModelConfigurationFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class FallbackModelConfigurationFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


class FallbackModelConfigurationRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class FallbackModelConfigurationRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


FallbackModelConfigurationType = Literal["exact_match",]


class FallbackModelConfigurationCacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: FallbackModelConfigurationType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class FallbackModelConfigurationCache(BaseModel):
    r"""Cache configuration for the request."""

    type: FallbackModelConfigurationType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestLoadBalancerType = Literal["weight_based",]


class LoadBalancerModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class LoadBalancerModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestLoadBalancer1TypedDict(TypedDict):
    type: CreateAgentRequestLoadBalancerType
    models: List[LoadBalancerModelsTypedDict]


class CreateAgentRequestLoadBalancer1(BaseModel):
    type: CreateAgentRequestLoadBalancerType

    models: List[LoadBalancerModels]


FallbackModelConfigurationLoadBalancerTypedDict = (
    CreateAgentRequestLoadBalancer1TypedDict
)
r"""Load balancer configuration for the request."""


FallbackModelConfigurationLoadBalancer = CreateAgentRequestLoadBalancer1
r"""Load balancer configuration for the request."""


class FallbackModelConfigurationTimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class FallbackModelConfigurationTimeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class FallbackModelConfigurationParametersTypedDict(TypedDict):
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    audio: NotRequired[Nullable[FallbackModelConfigurationAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[FallbackModelConfigurationResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[FallbackModelConfigurationReasoningEffort]
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[FallbackModelConfigurationStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[
        Nullable[FallbackModelConfigurationStreamOptionsTypedDict]
    ]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[FallbackModelConfigurationThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[FallbackModelConfigurationToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[Nullable[List[FallbackModelConfigurationModalities]]]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    guardrails: NotRequired[List[FallbackModelConfigurationGuardrailsTypedDict]]
    r"""A list of guardrails to apply to the request."""
    fallbacks: NotRequired[List[FallbackModelConfigurationFallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    retry: NotRequired[FallbackModelConfigurationRetryTypedDict]
    r"""Retry configuration for the request"""
    cache: NotRequired[FallbackModelConfigurationCacheTypedDict]
    r"""Cache configuration for the request."""
    load_balancer: NotRequired[FallbackModelConfigurationLoadBalancerTypedDict]
    r"""Load balancer configuration for the request."""
    timeout: NotRequired[FallbackModelConfigurationTimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


class FallbackModelConfigurationParameters(BaseModel):
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    audio: OptionalNullable[FallbackModelConfigurationAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[FallbackModelConfigurationResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[FallbackModelConfigurationReasoningEffort] = None
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[FallbackModelConfigurationStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[FallbackModelConfigurationStreamOptions] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[FallbackModelConfigurationThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[FallbackModelConfigurationToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[FallbackModelConfigurationModalities]] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    guardrails: Optional[List[FallbackModelConfigurationGuardrails]] = None
    r"""A list of guardrails to apply to the request."""

    fallbacks: Optional[List[FallbackModelConfigurationFallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    retry: Optional[FallbackModelConfigurationRetry] = None
    r"""Retry configuration for the request"""

    cache: Optional[FallbackModelConfigurationCache] = None
    r"""Cache configuration for the request."""

    load_balancer: Optional[FallbackModelConfigurationLoadBalancer] = None
    r"""Load balancer configuration for the request."""

    timeout: Optional[FallbackModelConfigurationTimeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "name",
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "response_format",
                "reasoning_effort",
                "verbosity",
                "seed",
                "stop",
                "stream_options",
                "thinking",
                "temperature",
                "top_p",
                "top_k",
                "tool_choice",
                "parallel_tool_calls",
                "modalities",
                "guardrails",
                "fallbacks",
                "retry",
                "cache",
                "load_balancer",
                "timeout",
            ]
        )
        nullable_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "seed",
                "stop",
                "stream_options",
                "temperature",
                "top_p",
                "top_k",
                "modalities",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateAgentRequestFallbackModelConfigurationRetryTypedDict(TypedDict):
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class CreateAgentRequestFallbackModelConfigurationRetry(BaseModel):
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class FallbackModelConfiguration2TypedDict(TypedDict):
    r"""Fallback model configuration with optional parameters and retry settings."""

    id: str
    r"""A fallback model ID string. Must support tool calling."""
    parameters: NotRequired[FallbackModelConfigurationParametersTypedDict]
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""
    retry: NotRequired[CreateAgentRequestFallbackModelConfigurationRetryTypedDict]
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""


class FallbackModelConfiguration2(BaseModel):
    r"""Fallback model configuration with optional parameters and retry settings."""

    id: str
    r"""A fallback model ID string. Must support tool calling."""

    parameters: Optional[FallbackModelConfigurationParameters] = None
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    retry: Optional[CreateAgentRequestFallbackModelConfigurationRetry] = None
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["parameters", "retry"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


FallbackModelConfigurationTypedDict = TypeAliasType(
    "FallbackModelConfigurationTypedDict",
    Union[FallbackModelConfiguration2TypedDict, str],
)
r"""Fallback model for automatic failover when primary model request fails. Supports optional parameter overrides. Can be a simple model ID string or a configuration object with model-specific parameters. Fallbacks are tried in order."""


FallbackModelConfiguration = TypeAliasType(
    "FallbackModelConfiguration", Union[FallbackModelConfiguration2, str]
)
r"""Fallback model for automatic failover when primary model request fails. Supports optional parameter overrides. Can be a simple model ID string or a configuration object with model-specific parameters. Fallbacks are tried in order."""


CreateAgentRequestToolApprovalRequired = Literal[
    "all",
    "respect_tool",
    "none",
]
r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools16Type = (
    Literal["mcp",]
)
r"""MCP tool type"""


class MCPToolTypedDict(TypedDict):
    r"""Executes tools from Model Context Protocol (MCP) servers. Specify the parent MCP tool using \"key\" or \"id\", and the specific nested tool using \"tool_id\"."""

    tool_id: str
    r"""The ID of the specific nested tool within the MCP server"""
    type: NotRequired[
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools16Type
    ]
    r"""MCP tool type"""
    key: NotRequired[str]
    r"""The key of the parent MCP tool"""
    id: NotRequired[str]
    r"""The ID of the parent MCP tool"""
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class MCPTool(BaseModel):
    r"""Executes tools from Model Context Protocol (MCP) servers. Specify the parent MCP tool using \"key\" or \"id\", and the specific nested tool using \"tool_id\"."""

    tool_id: str
    r"""The ID of the specific nested tool within the MCP server"""

    type: Optional[
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools16Type
    ] = "mcp"
    r"""MCP tool type"""

    key: Optional[str] = None
    r"""The key of the parent MCP tool"""

    id: Optional[str] = None
    r"""The ID of the parent MCP tool"""

    requires_approval: Optional[bool] = False
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type", "key", "id", "requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools15Type = (
    Literal["json_schema",]
)
r"""JSON Schema tool type"""


class JSONSchemaToolTypedDict(TypedDict):
    r"""Enforces structured output format using JSON Schema. Must reference a pre-created JSON Schema tool by key or id."""

    type: NotRequired[
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools15Type
    ]
    r"""JSON Schema tool type"""
    key: NotRequired[str]
    r"""The key of the pre-created JSON Schema tool"""
    id: NotRequired[str]
    r"""The ID of the pre-created JSON Schema tool"""
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class JSONSchemaTool(BaseModel):
    r"""Enforces structured output format using JSON Schema. Must reference a pre-created JSON Schema tool by key or id."""

    type: Optional[
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools15Type
    ] = "json_schema"
    r"""JSON Schema tool type"""

    key: Optional[str] = None
    r"""The key of the pre-created JSON Schema tool"""

    id: Optional[str] = None
    r"""The ID of the pre-created JSON Schema tool"""

    requires_approval: Optional[bool] = False
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type", "key", "id", "requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools14Type = (
    Literal["function",]
)
r"""Function tool type"""


class FunctionToolTypedDict(TypedDict):
    r"""Calls custom function tools defined in the agent configuration. Must reference a pre-created function tool by key or id."""

    type: NotRequired[
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools14Type
    ]
    r"""Function tool type"""
    key: NotRequired[str]
    r"""The key of the pre-created function tool"""
    id: NotRequired[str]
    r"""The ID of the pre-created function tool"""
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class FunctionTool(BaseModel):
    r"""Calls custom function tools defined in the agent configuration. Must reference a pre-created function tool by key or id."""

    type: Optional[
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools14Type
    ] = "function"
    r"""Function tool type"""

    key: Optional[str] = None
    r"""The key of the pre-created function tool"""

    id: Optional[str] = None
    r"""The ID of the pre-created function tool"""

    requires_approval: Optional[bool] = False
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type", "key", "id", "requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools13Type = (
    Literal["code",]
)
r"""Code execution tool type"""


class CodeExecutionToolTypedDict(TypedDict):
    r"""Executes code snippets in a sandboxed environment. Must reference a pre-created code tool by key or id."""

    type: NotRequired[
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools13Type
    ]
    r"""Code execution tool type"""
    key: NotRequired[str]
    r"""The key of the pre-created code tool"""
    id: NotRequired[str]
    r"""The ID of the pre-created code tool"""
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class CodeExecutionTool(BaseModel):
    r"""Executes code snippets in a sandboxed environment. Must reference a pre-created code tool by key or id."""

    type: Optional[
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools13Type
    ] = "code"
    r"""Code execution tool type"""

    key: Optional[str] = None
    r"""The key of the pre-created code tool"""

    id: Optional[str] = None
    r"""The ID of the pre-created code tool"""

    requires_approval: Optional[bool] = False
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type", "key", "id", "requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools12Type = (
    Literal["http",]
)
r"""HTTP tool type"""


class HTTPToolTypedDict(TypedDict):
    r"""Executes HTTP requests to interact with external APIs and web services. Must reference a pre-created HTTP tool by key or id."""

    type: NotRequired[
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools12Type
    ]
    r"""HTTP tool type"""
    key: NotRequired[str]
    r"""The key of the pre-created HTTP tool"""
    id: NotRequired[str]
    r"""The ID of the pre-created HTTP tool"""
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class HTTPTool(BaseModel):
    r"""Executes HTTP requests to interact with external APIs and web services. Must reference a pre-created HTTP tool by key or id."""

    type: Optional[
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools12Type
    ] = "http"
    r"""HTTP tool type"""

    key: Optional[str] = None
    r"""The key of the pre-created HTTP tool"""

    id: Optional[str] = None
    r"""The ID of the pre-created HTTP tool"""

    requires_approval: Optional[bool] = False
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type", "key", "id", "requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools11Type = (
    Literal["current_date",]
)


class CurrentDateToolTypedDict(TypedDict):
    r"""Returns the current date and time"""

    type: (
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools11Type
    )
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class CurrentDateTool(BaseModel):
    r"""Returns the current date and time"""

    type: (
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools11Type
    )

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools10Type = (
    Literal["query_knowledge_base",]
)


class QueryKnowledgeBaseToolTypedDict(TypedDict):
    r"""Queries knowledge bases for information"""

    type: (
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools10Type
    )
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class QueryKnowledgeBaseTool(BaseModel):
    r"""Queries knowledge bases for information"""

    type: (
        CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools10Type
    )

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools9Type = (
    Literal["retrieve_knowledge_bases",]
)


class RetrieveKnowledgeBasesToolTypedDict(TypedDict):
    r"""Lists available knowledge bases"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools9Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class RetrieveKnowledgeBasesTool(BaseModel):
    r"""Lists available knowledge bases"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools9Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools8Type = (
    Literal["delete_memory_document",]
)


class DeleteMemoryDocumentToolTypedDict(TypedDict):
    r"""Deletes documents from memory stores"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools8Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class DeleteMemoryDocumentTool(BaseModel):
    r"""Deletes documents from memory stores"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsTools8Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsToolsType = Literal[
    "retrieve_memory_stores",
]


class RetrieveMemoryStoresToolTypedDict(TypedDict):
    r"""Lists available memory stores"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsToolsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class RetrieveMemoryStoresTool(BaseModel):
    r"""Lists available memory stores"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsToolsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsType = Literal[
    "write_memory_store",
]


class WriteMemoryStoreToolTypedDict(TypedDict):
    r"""Writes information to agent memory stores"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class WriteMemoryStoreTool(BaseModel):
    r"""Writes information to agent memory stores"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodySettingsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodyType = Literal[
    "query_memory_store",
]


class QueryMemoryStoreToolTypedDict(TypedDict):
    r"""Queries agent memory stores for context"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodyType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class QueryMemoryStoreTool(BaseModel):
    r"""Queries agent memory stores for context"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestRequestBodyType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsRequestType = Literal["retrieve_agents",]


class RetrieveAgentsToolTypedDict(TypedDict):
    r"""Retrieves available agents in the system"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class RetrieveAgentsTool(BaseModel):
    r"""Retrieves available agents in the system"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsRequestType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDAgentsType = Literal["call_sub_agent",]


class CallSubAgentToolTypedDict(TypedDict):
    r"""Delegates tasks to specialized sub-agents"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class CallSubAgentTool(BaseModel):
    r"""Delegates tasks to specialized sub-agents"""

    type: CreateAgentRequestAgentToolInputCRUDAgentsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentToolInputCRUDType = Literal["web_scraper",]


class WebScraperToolTypedDict(TypedDict):
    r"""Scrapes and extracts content from web pages"""

    type: CreateAgentRequestAgentToolInputCRUDType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class WebScraperTool(BaseModel):
    r"""Scrapes and extracts content from web pages"""

    type: CreateAgentRequestAgentToolInputCRUDType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


AgentToolInputCRUDType = Literal["google_search",]


class GoogleSearchToolTypedDict(TypedDict):
    r"""Performs Google searches to retrieve web content"""

    type: AgentToolInputCRUDType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class GoogleSearchTool(BaseModel):
    r"""Performs Google searches to retrieve web content"""

    type: AgentToolInputCRUDType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["requires_approval"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


AgentToolInputCRUDTypedDict = TypeAliasType(
    "AgentToolInputCRUDTypedDict",
    Union[
        GoogleSearchToolTypedDict,
        WebScraperToolTypedDict,
        CallSubAgentToolTypedDict,
        RetrieveAgentsToolTypedDict,
        QueryMemoryStoreToolTypedDict,
        WriteMemoryStoreToolTypedDict,
        RetrieveMemoryStoresToolTypedDict,
        DeleteMemoryDocumentToolTypedDict,
        RetrieveKnowledgeBasesToolTypedDict,
        QueryKnowledgeBaseToolTypedDict,
        CurrentDateToolTypedDict,
        HTTPToolTypedDict,
        CodeExecutionToolTypedDict,
        FunctionToolTypedDict,
        JSONSchemaToolTypedDict,
        MCPToolTypedDict,
    ],
)
r"""Tool configuration for agent create/update operations. Built-in tools only require a type, while custom tools (HTTP, Code, Function, JSON Schema, MCP) must reference pre-created tools by key or id."""


AgentToolInputCRUD = TypeAliasType(
    "AgentToolInputCRUD",
    Union[
        GoogleSearchTool,
        WebScraperTool,
        CallSubAgentTool,
        RetrieveAgentsTool,
        QueryMemoryStoreTool,
        WriteMemoryStoreTool,
        RetrieveMemoryStoresTool,
        DeleteMemoryDocumentTool,
        RetrieveKnowledgeBasesTool,
        QueryKnowledgeBaseTool,
        CurrentDateTool,
        HTTPTool,
        CodeExecutionTool,
        FunctionTool,
        JSONSchemaTool,
        MCPTool,
    ],
)
r"""Tool configuration for agent create/update operations. Built-in tools only require a type, while custom tools (HTTP, Code, Function, JSON Schema, MCP) must reference pre-created tools by key or id."""


CreateAgentRequestExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class CreateAgentRequestEvaluatorsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: CreateAgentRequestExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class CreateAgentRequestEvaluators(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: CreateAgentRequestExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["sample_rate"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentsExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class CreateAgentRequestGuardrailsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: CreateAgentRequestAgentsExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class CreateAgentRequestGuardrails(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: CreateAgentRequestAgentsExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["sample_rate"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestSettingsTypedDict(TypedDict):
    r"""Configuration settings for the agent's behavior"""

    max_iterations: NotRequired[int]
    r"""Maximum iterations(llm calls) before the agent will stop executing."""
    max_execution_time: NotRequired[int]
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""
    max_cost: NotRequired[float]
    r"""Maximum cost in USD for the agent execution. When the accumulated cost exceeds this limit, the agent will stop executing. Set to 0 for unlimited. Only supported in v3 responses"""
    tool_approval_required: NotRequired[CreateAgentRequestToolApprovalRequired]
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""
    tools: NotRequired[List[AgentToolInputCRUDTypedDict]]
    r"""Tools available to the agent. Built-in tools only need a type, while custom tools (http, code, function) must reference pre-created tools by key or id."""
    evaluators: NotRequired[List[CreateAgentRequestEvaluatorsTypedDict]]
    r"""Configuration for an evaluator applied to the agent"""
    guardrails: NotRequired[List[CreateAgentRequestGuardrailsTypedDict]]
    r"""Configuration for a guardrail applied to the agent"""


class CreateAgentRequestSettings(BaseModel):
    r"""Configuration settings for the agent's behavior"""

    max_iterations: Optional[int] = 100
    r"""Maximum iterations(llm calls) before the agent will stop executing."""

    max_execution_time: Optional[int] = 600
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""

    max_cost: Optional[float] = 0
    r"""Maximum cost in USD for the agent execution. When the accumulated cost exceeds this limit, the agent will stop executing. Set to 0 for unlimited. Only supported in v3 responses"""

    tool_approval_required: Optional[CreateAgentRequestToolApprovalRequired] = (
        "respect_tool"
    )
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""

    tools: Optional[List[AgentToolInputCRUD]] = None
    r"""Tools available to the agent. Built-in tools only need a type, while custom tools (http, code, function) must reference pre-created tools by key or id."""

    evaluators: Optional[List[CreateAgentRequestEvaluators]] = None
    r"""Configuration for an evaluator applied to the agent"""

    guardrails: Optional[List[CreateAgentRequestGuardrails]] = None
    r"""Configuration for a guardrail applied to the agent"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "max_iterations",
                "max_execution_time",
                "max_cost",
                "tool_approval_required",
                "tools",
                "evaluators",
                "guardrails",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class KnowledgeBasesTypedDict(TypedDict):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


class KnowledgeBases(BaseModel):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


class TeamOfAgentsTypedDict(TypedDict):
    key: str
    r"""The unique key of the agent within the workspace"""
    role: NotRequired[str]
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""


class TeamOfAgents(BaseModel):
    key: str
    r"""The unique key of the agent within the workspace"""

    role: Optional[str] = None
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["role"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


Source = Literal[
    "internal",
    "external",
    "experiment",
]


class CreateAgentRequestRequestBodyTypedDict(TypedDict):
    key: str
    r"""Unique identifier for the agent within the workspace"""
    role: str
    r"""The role or function of the agent"""
    description: str
    r"""A brief description of what the agent does"""
    instructions: str
    r"""Detailed instructions that guide the agent's behavior"""
    path: str
    r"""The path where the agent will be stored in the project structure. The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """
    model: ModelConfigurationTypedDict
    r"""Model configuration for agent execution. Can be a simple model ID string or a configuration object with optional behavior parameters and retry settings."""
    settings: CreateAgentRequestSettingsTypedDict
    r"""Configuration settings for the agent's behavior"""
    display_name: NotRequired[str]
    r"""agent display name within the workspace"""
    system_prompt: NotRequired[str]
    r"""A custom system prompt template for the agent. If omitted, the default template is used."""
    fallback_models: NotRequired[List[FallbackModelConfigurationTypedDict]]
    r"""Optional array of fallback models used when the primary model fails. Fallbacks are attempted in order. All models must support tool calling."""
    memory_stores: NotRequired[List[str]]
    r"""Optional array of memory store identifiers for the agent to access. Accepts both memory store IDs and keys."""
    knowledge_bases: NotRequired[List[KnowledgeBasesTypedDict]]
    r"""Optional array of knowledge base configurations for the agent to access"""
    team_of_agents: NotRequired[List[TeamOfAgentsTypedDict]]
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""
    variables: NotRequired[Dict[str, Any]]
    source: NotRequired[Source]


class CreateAgentRequestRequestBody(BaseModel):
    key: str
    r"""Unique identifier for the agent within the workspace"""

    role: str
    r"""The role or function of the agent"""

    description: str
    r"""A brief description of what the agent does"""

    instructions: str
    r"""Detailed instructions that guide the agent's behavior"""

    path: str
    r"""The path where the agent will be stored in the project structure. The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """

    model: ModelConfiguration
    r"""Model configuration for agent execution. Can be a simple model ID string or a configuration object with optional behavior parameters and retry settings."""

    settings: CreateAgentRequestSettings
    r"""Configuration settings for the agent's behavior"""

    display_name: Optional[str] = None
    r"""agent display name within the workspace"""

    system_prompt: Optional[str] = None
    r"""A custom system prompt template for the agent. If omitted, the default template is used."""

    fallback_models: Optional[List[FallbackModelConfiguration]] = None
    r"""Optional array of fallback models used when the primary model fails. Fallbacks are attempted in order. All models must support tool calling."""

    memory_stores: Optional[List[str]] = None
    r"""Optional array of memory store identifiers for the agent to access. Accepts both memory store IDs and keys."""

    knowledge_bases: Optional[List[KnowledgeBases]] = None
    r"""Optional array of knowledge base configurations for the agent to access"""

    team_of_agents: Optional[List[TeamOfAgents]] = None
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""

    variables: Optional[Dict[str, Any]] = None

    source: Optional[Source] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "display_name",
                "system_prompt",
                "fallback_models",
                "memory_stores",
                "knowledge_bases",
                "team_of_agents",
                "variables",
                "source",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestStatus = Literal[
    "live",
    "draft",
    "pending",
    "published",
]
r"""The status of the agent. `Live` is the latest version of the agent. `Draft` is a version that is not yet published. `Pending` is a version that is pending approval. `Published` is a version that was live and has been replaced by a new version."""


CreateAgentRequestAgentsToolApprovalRequired = Literal[
    "all",
    "respect_tool",
    "none",
]
r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""


class CreateAgentRequestConditionsTypedDict(TypedDict):
    condition: str
    r"""The argument of the tool call to evaluate"""
    operator: str
    r"""The operator to use"""
    value: str
    r"""The value to compare against"""


class CreateAgentRequestConditions(BaseModel):
    condition: str
    r"""The argument of the tool call to evaluate"""

    operator: str
    r"""The operator to use"""

    value: str
    r"""The value to compare against"""


class CreateAgentRequestToolsTypedDict(TypedDict):
    id: str
    r"""The id of the resource"""
    action_type: str
    key: NotRequired[str]
    r"""Optional tool key for custom tools"""
    display_name: NotRequired[str]
    description: NotRequired[str]
    r"""Optional tool description"""
    requires_approval: NotRequired[bool]
    tool_id: NotRequired[str]
    r"""Nested tool ID for MCP tools (identifies specific tool within MCP server)"""
    conditions: NotRequired[List[CreateAgentRequestConditionsTypedDict]]
    timeout: NotRequired[float]
    r"""Tool execution timeout in seconds (default: 2 minutes, max: 10 minutes)"""


class CreateAgentRequestTools(BaseModel):
    id: str
    r"""The id of the resource"""

    action_type: str

    key: Optional[str] = None
    r"""Optional tool key for custom tools"""

    display_name: Optional[str] = None

    description: Optional[str] = None
    r"""Optional tool description"""

    requires_approval: Optional[bool] = False

    tool_id: Optional[str] = None
    r"""Nested tool ID for MCP tools (identifies specific tool within MCP server)"""

    conditions: Optional[List[CreateAgentRequestConditions]] = None

    timeout: Optional[float] = 120
    r"""Tool execution timeout in seconds (default: 2 minutes, max: 10 minutes)"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "key",
                "display_name",
                "description",
                "requires_approval",
                "tool_id",
                "conditions",
                "timeout",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentsResponseExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class CreateAgentRequestAgentsEvaluatorsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: CreateAgentRequestAgentsResponseExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class CreateAgentRequestAgentsEvaluators(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: CreateAgentRequestAgentsResponseExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["sample_rate"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestAgentsResponse201ExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class CreateAgentRequestAgentsGuardrailsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: CreateAgentRequestAgentsResponse201ExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class CreateAgentRequestAgentsGuardrails(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: CreateAgentRequestAgentsResponse201ExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["sample_rate"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestAgentsSettingsTypedDict(TypedDict):
    max_iterations: NotRequired[int]
    r"""Maximum iterations(llm calls) before the agent will stop executing."""
    max_execution_time: NotRequired[int]
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""
    max_cost: NotRequired[float]
    r"""Maximum cost in USD for the agent execution. When the accumulated cost exceeds this limit, the agent will stop executing. Set to 0 for unlimited. Only supported in v3 responses"""
    tool_approval_required: NotRequired[CreateAgentRequestAgentsToolApprovalRequired]
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""
    tools: NotRequired[List[CreateAgentRequestToolsTypedDict]]
    evaluators: NotRequired[List[CreateAgentRequestAgentsEvaluatorsTypedDict]]
    r"""Configuration for an evaluator applied to the agent"""
    guardrails: NotRequired[List[CreateAgentRequestAgentsGuardrailsTypedDict]]
    r"""Configuration for a guardrail applied to the agent"""


class CreateAgentRequestAgentsSettings(BaseModel):
    max_iterations: Optional[int] = 100
    r"""Maximum iterations(llm calls) before the agent will stop executing."""

    max_execution_time: Optional[int] = 600
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""

    max_cost: Optional[float] = 0
    r"""Maximum cost in USD for the agent execution. When the accumulated cost exceeds this limit, the agent will stop executing. Set to 0 for unlimited. Only supported in v3 responses"""

    tool_approval_required: Optional[CreateAgentRequestAgentsToolApprovalRequired] = (
        "respect_tool"
    )
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""

    tools: Optional[List[CreateAgentRequestTools]] = None

    evaluators: Optional[List[CreateAgentRequestAgentsEvaluators]] = None
    r"""Configuration for an evaluator applied to the agent"""

    guardrails: Optional[List[CreateAgentRequestAgentsGuardrails]] = None
    r"""Configuration for a guardrail applied to the agent"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "max_iterations",
                "max_execution_time",
                "max_cost",
                "tool_approval_required",
                "tools",
                "evaluators",
                "guardrails",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


CreateAgentRequestFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class CreateAgentRequestAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: CreateAgentRequestVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: CreateAgentRequestFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class CreateAgentRequestAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: CreateAgentRequestVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[CreateAgentRequestFormat, pydantic.Field(alias="format")]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONType = Literal[
    "json_schema",
]


class CreateAgentRequestResponseFormatAgentsResponseJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class CreateAgentRequestResponseFormatAgentsResponseJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = False
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "schema", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestResponseFormatAgentsResponse201JSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONType
    json_schema: CreateAgentRequestResponseFormatAgentsResponseJSONSchemaTypedDict


class CreateAgentRequestResponseFormatAgentsResponse201JSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONType

    json_schema: CreateAgentRequestResponseFormatAgentsResponseJSONSchema


CreateAgentRequestResponseFormatAgentsResponse201Type = Literal["json_object",]


class CreateAgentRequestResponseFormatJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreateAgentRequestResponseFormatAgentsResponse201Type


class CreateAgentRequestResponseFormatJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreateAgentRequestResponseFormatAgentsResponse201Type


CreateAgentRequestResponseFormatAgentsResponseType = Literal["text",]


class CreateAgentRequestResponseFormatTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: CreateAgentRequestResponseFormatAgentsResponseType


class CreateAgentRequestResponseFormatText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: CreateAgentRequestResponseFormatAgentsResponseType


CreateAgentRequestResponseFormatTypedDict = TypeAliasType(
    "CreateAgentRequestResponseFormatTypedDict",
    Union[
        CreateAgentRequestResponseFormatTextTypedDict,
        CreateAgentRequestResponseFormatJSONObjectTypedDict,
        CreateAgentRequestResponseFormatAgentsResponse201JSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


CreateAgentRequestResponseFormat = Annotated[
    Union[
        Annotated[CreateAgentRequestResponseFormatText, Tag("text")],
        Annotated[CreateAgentRequestResponseFormatJSONObject, Tag("json_object")],
        Annotated[
            CreateAgentRequestResponseFormatAgentsResponse201JSONSchema,
            Tag("json_schema"),
        ],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An object specifying the format that the model must output"""


CreateAgentRequestReasoningEffort = Literal[
    "none",
    "minimal",
    "low",
    "medium",
    "high",
    "xhigh",
]
r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

- `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
- All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
- The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
- `xhigh` is currently only supported for `gpt-5.1-codex-max`.

Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
"""


CreateAgentRequestStopTypedDict = TypeAliasType(
    "CreateAgentRequestStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


CreateAgentRequestStop = TypeAliasType("CreateAgentRequestStop", Union[str, List[str]])
r"""Up to 4 sequences where the API will stop generating further tokens."""


class CreateAgentRequestStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class CreateAgentRequestStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestThinkingTypedDict = TypeAliasType(
    "CreateAgentRequestThinkingTypedDict",
    Union[ThinkingConfigDisabledSchemaTypedDict, ThinkingConfigEnabledSchemaTypedDict],
)


CreateAgentRequestThinking = Annotated[
    Union[
        Annotated[ThinkingConfigDisabledSchema, Tag("disabled")],
        Annotated[ThinkingConfigEnabledSchema, Tag("enabled")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


CreateAgentRequestToolChoiceAgentsType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class CreateAgentRequestToolChoiceAgentsFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to call."""


class CreateAgentRequestToolChoiceAgentsFunction(BaseModel):
    name: str
    r"""The name of the function to call."""


class CreateAgentRequestToolChoiceAgents2TypedDict(TypedDict):
    function: CreateAgentRequestToolChoiceAgentsFunctionTypedDict
    type: NotRequired[CreateAgentRequestToolChoiceAgentsType]
    r"""The type of the tool. Currently, only function is supported."""


class CreateAgentRequestToolChoiceAgents2(BaseModel):
    function: CreateAgentRequestToolChoiceAgentsFunction

    type: Optional[CreateAgentRequestToolChoiceAgentsType] = None
    r"""The type of the tool. Currently, only function is supported."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestToolChoiceAgents1 = Literal[
    "none",
    "auto",
    "required",
]


CreateAgentRequestToolChoiceTypedDict = TypeAliasType(
    "CreateAgentRequestToolChoiceTypedDict",
    Union[
        CreateAgentRequestToolChoiceAgents2TypedDict,
        CreateAgentRequestToolChoiceAgents1,
    ],
)
r"""Controls which (if any) tool is called by the model."""


CreateAgentRequestToolChoice = TypeAliasType(
    "CreateAgentRequestToolChoice",
    Union[CreateAgentRequestToolChoiceAgents2, CreateAgentRequestToolChoiceAgents1],
)
r"""Controls which (if any) tool is called by the model."""


CreateAgentRequestModalities = Literal[
    "text",
    "audio",
]


CreateAgentRequestIDAgents1 = Literal[
    "orq_pii_detection",
    "orq_sexual_moderation",
    "orq_harmful_moderation",
]
r"""The key of the guardrail."""


CreateAgentRequestIDTypedDict = TypeAliasType(
    "CreateAgentRequestIDTypedDict", Union[CreateAgentRequestIDAgents1, str]
)


CreateAgentRequestID = TypeAliasType(
    "CreateAgentRequestID", Union[CreateAgentRequestIDAgents1, str]
)


CreateAgentRequestAgentsResponse201ApplicationJSONExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateAgentRequestAgentsResponseGuardrailsTypedDict(TypedDict):
    id: CreateAgentRequestIDTypedDict
    execute_on: CreateAgentRequestAgentsResponse201ApplicationJSONExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateAgentRequestAgentsResponseGuardrails(BaseModel):
    id: CreateAgentRequestID

    execute_on: CreateAgentRequestAgentsResponse201ApplicationJSONExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateAgentRequestFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class CreateAgentRequestFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


class CreateAgentRequestAgentsRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class CreateAgentRequestAgentsRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestType = Literal["exact_match",]


class CreateAgentRequestCacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: CreateAgentRequestType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class CreateAgentRequestCache(BaseModel):
    r"""Cache configuration for the request."""

    type: CreateAgentRequestType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestLoadBalancerAgentsType = Literal["weight_based",]


class CreateAgentRequestLoadBalancerAgentsModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class CreateAgentRequestLoadBalancerAgentsModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestLoadBalancerAgents1TypedDict(TypedDict):
    type: CreateAgentRequestLoadBalancerAgentsType
    models: List[CreateAgentRequestLoadBalancerAgentsModelsTypedDict]


class CreateAgentRequestLoadBalancerAgents1(BaseModel):
    type: CreateAgentRequestLoadBalancerAgentsType

    models: List[CreateAgentRequestLoadBalancerAgentsModels]


CreateAgentRequestLoadBalancerTypedDict = CreateAgentRequestLoadBalancerAgents1TypedDict
r"""Load balancer configuration for the request."""


CreateAgentRequestLoadBalancer = CreateAgentRequestLoadBalancerAgents1
r"""Load balancer configuration for the request."""


class CreateAgentRequestTimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class CreateAgentRequestTimeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class CreateAgentRequestParametersTypedDict(TypedDict):
    r"""Model behavior parameters (snake_case) stored as part of the agent configuration. These become the default parameters used when the agent is executed. Commonly used: temperature (0-1, controls randomness), max_completion_tokens (response length), top_p (nucleus sampling). Advanced: frequency_penalty, presence_penalty, response_format (JSON/structured output), reasoning_effort (for o1/thinking models), seed (reproducibility), stop sequences. Model-specific support varies. Runtime parameters in agent execution requests can override these defaults."""

    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    audio: NotRequired[Nullable[CreateAgentRequestAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[CreateAgentRequestResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[CreateAgentRequestReasoningEffort]
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[CreateAgentRequestStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[Nullable[CreateAgentRequestStreamOptionsTypedDict]]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[CreateAgentRequestThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[CreateAgentRequestToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[Nullable[List[CreateAgentRequestModalities]]]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    guardrails: NotRequired[List[CreateAgentRequestAgentsResponseGuardrailsTypedDict]]
    r"""A list of guardrails to apply to the request."""
    fallbacks: NotRequired[List[CreateAgentRequestFallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    retry: NotRequired[CreateAgentRequestAgentsRetryTypedDict]
    r"""Retry configuration for the request"""
    cache: NotRequired[CreateAgentRequestCacheTypedDict]
    r"""Cache configuration for the request."""
    load_balancer: NotRequired[CreateAgentRequestLoadBalancerTypedDict]
    r"""Load balancer configuration for the request."""
    timeout: NotRequired[CreateAgentRequestTimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


class CreateAgentRequestParameters(BaseModel):
    r"""Model behavior parameters (snake_case) stored as part of the agent configuration. These become the default parameters used when the agent is executed. Commonly used: temperature (0-1, controls randomness), max_completion_tokens (response length), top_p (nucleus sampling). Advanced: frequency_penalty, presence_penalty, response_format (JSON/structured output), reasoning_effort (for o1/thinking models), seed (reproducibility), stop sequences. Model-specific support varies. Runtime parameters in agent execution requests can override these defaults."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    audio: OptionalNullable[CreateAgentRequestAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[CreateAgentRequestResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[CreateAgentRequestReasoningEffort] = None
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[CreateAgentRequestStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[CreateAgentRequestStreamOptions] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[CreateAgentRequestThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[CreateAgentRequestToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[CreateAgentRequestModalities]] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    guardrails: Optional[List[CreateAgentRequestAgentsResponseGuardrails]] = None
    r"""A list of guardrails to apply to the request."""

    fallbacks: Optional[List[CreateAgentRequestFallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    retry: Optional[CreateAgentRequestAgentsRetry] = None
    r"""Retry configuration for the request"""

    cache: Optional[CreateAgentRequestCache] = None
    r"""Cache configuration for the request."""

    load_balancer: Optional[CreateAgentRequestLoadBalancer] = None
    r"""Load balancer configuration for the request."""

    timeout: Optional[CreateAgentRequestTimeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "name",
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "response_format",
                "reasoning_effort",
                "verbosity",
                "seed",
                "stop",
                "stream_options",
                "thinking",
                "temperature",
                "top_p",
                "top_k",
                "tool_choice",
                "parallel_tool_calls",
                "modalities",
                "guardrails",
                "fallbacks",
                "retry",
                "cache",
                "load_balancer",
                "timeout",
            ]
        )
        nullable_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "seed",
                "stop",
                "stream_options",
                "temperature",
                "top_p",
                "top_k",
                "modalities",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateAgentRequestRetryTypedDict(TypedDict):
    r"""Retry configuration for model requests. Allows customizing retry count (1-5) and HTTP status codes that trigger retries. Default codes: [429]. Common codes: 500 (internal error), 429 (rate limit), 502/503/504 (gateway errors)."""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class CreateAgentRequestRetry(BaseModel):
    r"""Retry configuration for model requests. Allows customizing retry count (1-5) and HTTP status codes that trigger retries. Default codes: [429]. Common codes: 500 (internal error), 429 (rate limit), 502/503/504 (gateway errors)."""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestFallbackModelConfigurationVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


CreateAgentRequestFallbackModelConfigurationFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class CreateAgentRequestFallbackModelConfigurationAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: CreateAgentRequestFallbackModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: CreateAgentRequestFallbackModelConfigurationFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class CreateAgentRequestFallbackModelConfigurationAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: CreateAgentRequestFallbackModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[
        CreateAgentRequestFallbackModelConfigurationFormat,
        pydantic.Field(alias="format"),
    ]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyModelFallbackModelsType = Literal[
    "json_schema",
]


class CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONJSONSchemaTypedDict(
    TypedDict
):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONJSONSchema(
    BaseModel
):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = False
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "schema", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyJSONSchemaTypedDict(
    TypedDict
):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyModelFallbackModelsType
    json_schema: CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONJSONSchemaTypedDict


class CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyJSONSchema(
    BaseModel
):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyModelFallbackModelsType

    json_schema: (
        CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONJSONSchema
    )


CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyModelType = Literal[
    "json_object",
]


class CreateAgentRequestResponseFormatAgentsJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyModelType


class CreateAgentRequestResponseFormatAgentsJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyModelType


CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyType = (
    Literal["text",]
)


class CreateAgentRequestResponseFormatAgentsTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: (
        CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyType
    )


class CreateAgentRequestResponseFormatAgentsText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: (
        CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyType
    )


CreateAgentRequestFallbackModelConfigurationResponseFormatTypedDict = TypeAliasType(
    "CreateAgentRequestFallbackModelConfigurationResponseFormatTypedDict",
    Union[
        CreateAgentRequestResponseFormatAgentsTextTypedDict,
        CreateAgentRequestResponseFormatAgentsJSONObjectTypedDict,
        CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyJSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


CreateAgentRequestFallbackModelConfigurationResponseFormat = Annotated[
    Union[
        Annotated[CreateAgentRequestResponseFormatAgentsText, Tag("text")],
        Annotated[CreateAgentRequestResponseFormatAgentsJSONObject, Tag("json_object")],
        Annotated[
            CreateAgentRequestResponseFormatAgentsResponse201ApplicationJSONResponseBodyJSONSchema,
            Tag("json_schema"),
        ],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An object specifying the format that the model must output"""


CreateAgentRequestFallbackModelConfigurationReasoningEffort = Literal[
    "none",
    "minimal",
    "low",
    "medium",
    "high",
    "xhigh",
]
r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

- `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
- All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
- The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
- `xhigh` is currently only supported for `gpt-5.1-codex-max`.

Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
"""


CreateAgentRequestFallbackModelConfigurationStopTypedDict = TypeAliasType(
    "CreateAgentRequestFallbackModelConfigurationStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


CreateAgentRequestFallbackModelConfigurationStop = TypeAliasType(
    "CreateAgentRequestFallbackModelConfigurationStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class CreateAgentRequestFallbackModelConfigurationStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class CreateAgentRequestFallbackModelConfigurationStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestFallbackModelConfigurationThinkingTypedDict = TypeAliasType(
    "CreateAgentRequestFallbackModelConfigurationThinkingTypedDict",
    Union[ThinkingConfigDisabledSchemaTypedDict, ThinkingConfigEnabledSchemaTypedDict],
)


CreateAgentRequestFallbackModelConfigurationThinking = Annotated[
    Union[
        Annotated[ThinkingConfigDisabledSchema, Tag("disabled")],
        Annotated[ThinkingConfigEnabledSchema, Tag("enabled")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


CreateAgentRequestToolChoiceAgentsResponseType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class CreateAgentRequestToolChoiceAgentsResponseFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to call."""


class CreateAgentRequestToolChoiceAgentsResponseFunction(BaseModel):
    name: str
    r"""The name of the function to call."""


class CreateAgentRequestToolChoiceAgentsResponse2TypedDict(TypedDict):
    function: CreateAgentRequestToolChoiceAgentsResponseFunctionTypedDict
    type: NotRequired[CreateAgentRequestToolChoiceAgentsResponseType]
    r"""The type of the tool. Currently, only function is supported."""


class CreateAgentRequestToolChoiceAgentsResponse2(BaseModel):
    function: CreateAgentRequestToolChoiceAgentsResponseFunction

    type: Optional[CreateAgentRequestToolChoiceAgentsResponseType] = None
    r"""The type of the tool. Currently, only function is supported."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestToolChoiceAgentsResponse1 = Literal[
    "none",
    "auto",
    "required",
]


CreateAgentRequestFallbackModelConfigurationToolChoiceTypedDict = TypeAliasType(
    "CreateAgentRequestFallbackModelConfigurationToolChoiceTypedDict",
    Union[
        CreateAgentRequestToolChoiceAgentsResponse2TypedDict,
        CreateAgentRequestToolChoiceAgentsResponse1,
    ],
)
r"""Controls which (if any) tool is called by the model."""


CreateAgentRequestFallbackModelConfigurationToolChoice = TypeAliasType(
    "CreateAgentRequestFallbackModelConfigurationToolChoice",
    Union[
        CreateAgentRequestToolChoiceAgentsResponse2,
        CreateAgentRequestToolChoiceAgentsResponse1,
    ],
)
r"""Controls which (if any) tool is called by the model."""


CreateAgentRequestFallbackModelConfigurationModalities = Literal[
    "text",
    "audio",
]


CreateAgentRequestIDAgentsResponse1 = Literal[
    "orq_pii_detection",
    "orq_sexual_moderation",
    "orq_harmful_moderation",
]
r"""The key of the guardrail."""


CreateAgentRequestFallbackModelConfigurationIDTypedDict = TypeAliasType(
    "CreateAgentRequestFallbackModelConfigurationIDTypedDict",
    Union[CreateAgentRequestIDAgentsResponse1, str],
)


CreateAgentRequestFallbackModelConfigurationID = TypeAliasType(
    "CreateAgentRequestFallbackModelConfigurationID",
    Union[CreateAgentRequestIDAgentsResponse1, str],
)


CreateAgentRequestFallbackModelConfigurationExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateAgentRequestFallbackModelConfigurationGuardrailsTypedDict(TypedDict):
    id: CreateAgentRequestFallbackModelConfigurationIDTypedDict
    execute_on: CreateAgentRequestFallbackModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateAgentRequestFallbackModelConfigurationGuardrails(BaseModel):
    id: CreateAgentRequestFallbackModelConfigurationID

    execute_on: CreateAgentRequestFallbackModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateAgentRequestFallbackModelConfigurationFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class CreateAgentRequestFallbackModelConfigurationFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


class CreateAgentRequestFallbackModelConfigurationAgentsResponseRetryTypedDict(
    TypedDict
):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class CreateAgentRequestFallbackModelConfigurationAgentsResponseRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestFallbackModelConfigurationType = Literal["exact_match",]


class CreateAgentRequestFallbackModelConfigurationCacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: CreateAgentRequestFallbackModelConfigurationType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class CreateAgentRequestFallbackModelConfigurationCache(BaseModel):
    r"""Cache configuration for the request."""

    type: CreateAgentRequestFallbackModelConfigurationType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestLoadBalancerAgentsResponseType = Literal["weight_based",]


class CreateAgentRequestLoadBalancerAgentsResponseModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class CreateAgentRequestLoadBalancerAgentsResponseModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestLoadBalancerAgentsResponse1TypedDict(TypedDict):
    type: CreateAgentRequestLoadBalancerAgentsResponseType
    models: List[CreateAgentRequestLoadBalancerAgentsResponseModelsTypedDict]


class CreateAgentRequestLoadBalancerAgentsResponse1(BaseModel):
    type: CreateAgentRequestLoadBalancerAgentsResponseType

    models: List[CreateAgentRequestLoadBalancerAgentsResponseModels]


CreateAgentRequestFallbackModelConfigurationLoadBalancerTypedDict = (
    CreateAgentRequestLoadBalancerAgentsResponse1TypedDict
)
r"""Load balancer configuration for the request."""


CreateAgentRequestFallbackModelConfigurationLoadBalancer = (
    CreateAgentRequestLoadBalancerAgentsResponse1
)
r"""Load balancer configuration for the request."""


class CreateAgentRequestFallbackModelConfigurationTimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class CreateAgentRequestFallbackModelConfigurationTimeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class CreateAgentRequestFallbackModelConfigurationParametersTypedDict(TypedDict):
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    audio: NotRequired[
        Nullable[CreateAgentRequestFallbackModelConfigurationAudioTypedDict]
    ]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[
        CreateAgentRequestFallbackModelConfigurationResponseFormatTypedDict
    ]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[
        CreateAgentRequestFallbackModelConfigurationReasoningEffort
    ]
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[
        Nullable[CreateAgentRequestFallbackModelConfigurationStopTypedDict]
    ]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[
        Nullable[CreateAgentRequestFallbackModelConfigurationStreamOptionsTypedDict]
    ]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[CreateAgentRequestFallbackModelConfigurationThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[
        CreateAgentRequestFallbackModelConfigurationToolChoiceTypedDict
    ]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[
        Nullable[List[CreateAgentRequestFallbackModelConfigurationModalities]]
    ]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    guardrails: NotRequired[
        List[CreateAgentRequestFallbackModelConfigurationGuardrailsTypedDict]
    ]
    r"""A list of guardrails to apply to the request."""
    fallbacks: NotRequired[
        List[CreateAgentRequestFallbackModelConfigurationFallbacksTypedDict]
    ]
    r"""Array of fallback models to use if primary model fails"""
    retry: NotRequired[
        CreateAgentRequestFallbackModelConfigurationAgentsResponseRetryTypedDict
    ]
    r"""Retry configuration for the request"""
    cache: NotRequired[CreateAgentRequestFallbackModelConfigurationCacheTypedDict]
    r"""Cache configuration for the request."""
    load_balancer: NotRequired[
        CreateAgentRequestFallbackModelConfigurationLoadBalancerTypedDict
    ]
    r"""Load balancer configuration for the request."""
    timeout: NotRequired[CreateAgentRequestFallbackModelConfigurationTimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


class CreateAgentRequestFallbackModelConfigurationParameters(BaseModel):
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    audio: OptionalNullable[CreateAgentRequestFallbackModelConfigurationAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[
        CreateAgentRequestFallbackModelConfigurationResponseFormat
    ] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[
        CreateAgentRequestFallbackModelConfigurationReasoningEffort
    ] = None
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[CreateAgentRequestFallbackModelConfigurationStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[
        CreateAgentRequestFallbackModelConfigurationStreamOptions
    ] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[CreateAgentRequestFallbackModelConfigurationThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[CreateAgentRequestFallbackModelConfigurationToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[
        List[CreateAgentRequestFallbackModelConfigurationModalities]
    ] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    guardrails: Optional[
        List[CreateAgentRequestFallbackModelConfigurationGuardrails]
    ] = None
    r"""A list of guardrails to apply to the request."""

    fallbacks: Optional[List[CreateAgentRequestFallbackModelConfigurationFallbacks]] = (
        None
    )
    r"""Array of fallback models to use if primary model fails"""

    retry: Optional[CreateAgentRequestFallbackModelConfigurationAgentsResponseRetry] = (
        None
    )
    r"""Retry configuration for the request"""

    cache: Optional[CreateAgentRequestFallbackModelConfigurationCache] = None
    r"""Cache configuration for the request."""

    load_balancer: Optional[
        CreateAgentRequestFallbackModelConfigurationLoadBalancer
    ] = None
    r"""Load balancer configuration for the request."""

    timeout: Optional[CreateAgentRequestFallbackModelConfigurationTimeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "name",
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "response_format",
                "reasoning_effort",
                "verbosity",
                "seed",
                "stop",
                "stream_options",
                "thinking",
                "temperature",
                "top_p",
                "top_k",
                "tool_choice",
                "parallel_tool_calls",
                "modalities",
                "guardrails",
                "fallbacks",
                "retry",
                "cache",
                "load_balancer",
                "timeout",
            ]
        )
        nullable_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "seed",
                "stop",
                "stream_options",
                "temperature",
                "top_p",
                "top_k",
                "modalities",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateAgentRequestFallbackModelConfigurationAgentsRetryTypedDict(TypedDict):
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class CreateAgentRequestFallbackModelConfigurationAgentsRetry(BaseModel):
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestFallbackModelConfiguration2TypedDict(TypedDict):
    r"""Fallback model configuration with optional parameters and retry settings."""

    id: str
    r"""A fallback model ID string. Must support tool calling."""
    parameters: NotRequired[
        CreateAgentRequestFallbackModelConfigurationParametersTypedDict
    ]
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""
    retry: NotRequired[CreateAgentRequestFallbackModelConfigurationAgentsRetryTypedDict]
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""


class CreateAgentRequestFallbackModelConfiguration2(BaseModel):
    r"""Fallback model configuration with optional parameters and retry settings."""

    id: str
    r"""A fallback model ID string. Must support tool calling."""

    parameters: Optional[CreateAgentRequestFallbackModelConfigurationParameters] = None
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    retry: Optional[CreateAgentRequestFallbackModelConfigurationAgentsRetry] = None
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["parameters", "retry"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateAgentRequestFallbackModelConfigurationTypedDict = TypeAliasType(
    "CreateAgentRequestFallbackModelConfigurationTypedDict",
    Union[CreateAgentRequestFallbackModelConfiguration2TypedDict, str],
)
r"""Fallback model for automatic failover when primary model request fails. Supports optional parameter overrides. Can be a simple model ID string or a configuration object with model-specific parameters. Fallbacks are tried in order."""


CreateAgentRequestFallbackModelConfiguration = TypeAliasType(
    "CreateAgentRequestFallbackModelConfiguration",
    Union[CreateAgentRequestFallbackModelConfiguration2, str],
)
r"""Fallback model for automatic failover when primary model request fails. Supports optional parameter overrides. Can be a simple model ID string or a configuration object with model-specific parameters. Fallbacks are tried in order."""


class ModelTypedDict(TypedDict):
    id: str
    r"""The database ID of the primary model"""
    integration_id: NotRequired[Nullable[str]]
    r"""Optional integration ID for custom model configurations"""
    parameters: NotRequired[CreateAgentRequestParametersTypedDict]
    r"""Model behavior parameters (snake_case) stored as part of the agent configuration. These become the default parameters used when the agent is executed. Commonly used: temperature (0-1, controls randomness), max_completion_tokens (response length), top_p (nucleus sampling). Advanced: frequency_penalty, presence_penalty, response_format (JSON/structured output), reasoning_effort (for o1/thinking models), seed (reproducibility), stop sequences. Model-specific support varies. Runtime parameters in agent execution requests can override these defaults."""
    retry: NotRequired[CreateAgentRequestRetryTypedDict]
    r"""Retry configuration for model requests. Allows customizing retry count (1-5) and HTTP status codes that trigger retries. Default codes: [429]. Common codes: 500 (internal error), 429 (rate limit), 502/503/504 (gateway errors)."""
    fallback_models: NotRequired[
        Nullable[List[CreateAgentRequestFallbackModelConfigurationTypedDict]]
    ]
    r"""Optional array of fallback models (string IDs or config objects) that will be used automatically in order if the primary model fails"""


class Model(BaseModel):
    id: str
    r"""The database ID of the primary model"""

    integration_id: OptionalNullable[str] = UNSET
    r"""Optional integration ID for custom model configurations"""

    parameters: Optional[CreateAgentRequestParameters] = None
    r"""Model behavior parameters (snake_case) stored as part of the agent configuration. These become the default parameters used when the agent is executed. Commonly used: temperature (0-1, controls randomness), max_completion_tokens (response length), top_p (nucleus sampling). Advanced: frequency_penalty, presence_penalty, response_format (JSON/structured output), reasoning_effort (for o1/thinking models), seed (reproducibility), stop sequences. Model-specific support varies. Runtime parameters in agent execution requests can override these defaults."""

    retry: Optional[CreateAgentRequestRetry] = None
    r"""Retry configuration for model requests. Allows customizing retry count (1-5) and HTTP status codes that trigger retries. Default codes: [429]. Common codes: 500 (internal error), 429 (rate limit), 502/503/504 (gateway errors)."""

    fallback_models: OptionalNullable[
        List[CreateAgentRequestFallbackModelConfiguration]
    ] = UNSET
    r"""Optional array of fallback models (string IDs or config objects) that will be used automatically in order if the primary model fails"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["integration_id", "parameters", "retry", "fallback_models"]
        )
        nullable_fields = set(["integration_id", "fallback_models"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateAgentRequestTeamOfAgentsTypedDict(TypedDict):
    key: str
    r"""The unique key of the agent within the workspace"""
    role: NotRequired[str]
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""


class CreateAgentRequestTeamOfAgents(BaseModel):
    key: str
    r"""The unique key of the agent within the workspace"""

    role: Optional[str] = None
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["role"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestMetricsTypedDict(TypedDict):
    total_cost: NotRequired[float]


class CreateAgentRequestMetrics(BaseModel):
    total_cost: Optional[float] = 0

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["total_cost"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateAgentRequestKnowledgeBasesTypedDict(TypedDict):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


class CreateAgentRequestKnowledgeBases(BaseModel):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


CreateAgentRequestSource = Literal[
    "internal",
    "external",
    "experiment",
]


class CreateAgentRequestResponseBodyTypedDict(TypedDict):
    r"""Agent successfully created and ready for use. Returns the complete agent manifest including the generated ID, configuration, and all settings."""

    id: str
    key: str
    r"""Unique identifier for the agent within the workspace"""
    display_name: str
    project_id: str
    role: str
    description: str
    instructions: str
    status: CreateAgentRequestStatus
    r"""The status of the agent. `Live` is the latest version of the agent. `Draft` is a version that is not yet published. `Pending` is a version that is pending approval. `Published` is a version that was live and has been replaced by a new version."""
    model: ModelTypedDict
    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """
    memory_stores: List[str]
    r"""Array of memory store identifiers. Accepts both memory store IDs and keys."""
    team_of_agents: List[CreateAgentRequestTeamOfAgentsTypedDict]
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""
    created_by_id: NotRequired[Nullable[str]]
    updated_by_id: NotRequired[Nullable[str]]
    created: NotRequired[str]
    updated: NotRequired[str]
    system_prompt: NotRequired[str]
    settings: NotRequired[CreateAgentRequestAgentsSettingsTypedDict]
    version_hash: NotRequired[str]
    metrics: NotRequired[CreateAgentRequestMetricsTypedDict]
    variables: NotRequired[Dict[str, Any]]
    r"""Extracted variables from agent instructions"""
    knowledge_bases: NotRequired[List[CreateAgentRequestKnowledgeBasesTypedDict]]
    r"""Agent knowledge bases reference"""
    source: NotRequired[CreateAgentRequestSource]


class CreateAgentRequestResponseBody(BaseModel):
    r"""Agent successfully created and ready for use. Returns the complete agent manifest including the generated ID, configuration, and all settings."""

    id: Annotated[str, pydantic.Field(alias="_id")]

    key: str
    r"""Unique identifier for the agent within the workspace"""

    display_name: str

    project_id: str

    role: str

    description: str

    instructions: str

    status: CreateAgentRequestStatus
    r"""The status of the agent. `Live` is the latest version of the agent. `Draft` is a version that is not yet published. `Pending` is a version that is pending approval. `Published` is a version that was live and has been replaced by a new version."""

    model: Model

    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """

    memory_stores: List[str]
    r"""Array of memory store identifiers. Accepts both memory store IDs and keys."""

    team_of_agents: List[CreateAgentRequestTeamOfAgents]
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""

    created_by_id: OptionalNullable[str] = UNSET

    updated_by_id: OptionalNullable[str] = UNSET

    created: Optional[str] = None

    updated: Optional[str] = None

    system_prompt: Optional[str] = None

    settings: Optional[CreateAgentRequestAgentsSettings] = None

    version_hash: Optional[str] = None

    metrics: Optional[CreateAgentRequestMetrics] = None

    variables: Optional[Dict[str, Any]] = None
    r"""Extracted variables from agent instructions"""

    knowledge_bases: Optional[List[CreateAgentRequestKnowledgeBases]] = None
    r"""Agent knowledge bases reference"""

    source: Optional[CreateAgentRequestSource] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "created_by_id",
                "updated_by_id",
                "created",
                "updated",
                "system_prompt",
                "settings",
                "version_hash",
                "metrics",
                "variables",
                "knowledge_bases",
                "source",
            ]
        )
        nullable_fields = set(["created_by_id", "updated_by_id"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m
