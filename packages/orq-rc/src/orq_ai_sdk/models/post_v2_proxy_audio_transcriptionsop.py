"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
import httpx
import io
from orq_ai_sdk.models import OrqError
from orq_ai_sdk.types import BaseModel, Nullable, UNSET_SENTINEL
from orq_ai_sdk.utils import FieldMetadata, MultipartFormMetadata
import pydantic
from pydantic import model_serializer
from typing import IO, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


PostV2ProxyAudioTranscriptionsResponseFormat = Literal[
    "json", "text", "srt", "verbose_json", "vtt"
]
r"""The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt."""

TimestampsGranularity = Literal["none", "word", "character"]
r"""The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word."""

TimestampGranularities = Literal["word", "segment"]


class PostV2ProxyAudioTranscriptionsFileTypedDict(TypedDict):
    file_name: str
    content: Union[bytes, IO[bytes], io.BufferedReader]
    content_type: NotRequired[str]


class PostV2ProxyAudioTranscriptionsFile(BaseModel):
    file_name: Annotated[
        str, pydantic.Field(alias="fileName"), FieldMetadata(multipart=True)
    ]

    content: Annotated[
        Union[bytes, IO[bytes], io.BufferedReader],
        pydantic.Field(alias=""),
        FieldMetadata(multipart=MultipartFormMetadata(content=True)),
    ]

    content_type: Annotated[
        Optional[str],
        pydantic.Field(alias="Content-Type"),
        FieldMetadata(multipart=True),
    ] = None


class PostV2ProxyAudioTranscriptionsRequestBodyTypedDict(TypedDict):
    r"""Transcribes audio into the input language."""

    file: PostV2ProxyAudioTranscriptionsFileTypedDict
    r"""The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm."""
    model: NotRequired[str]
    r"""ID of the model to use"""
    prompt: NotRequired[str]
    r"""An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language."""
    enable_logging: NotRequired[bool]
    r"""When enable_logging is set to false, zero retention mode is used. This disables history features like request stitching and is only available to enterprise customers."""
    diarize: NotRequired[bool]
    r"""Whether to annotate which speaker is currently talking in the uploaded file."""
    response_format: NotRequired[PostV2ProxyAudioTranscriptionsResponseFormat]
    r"""The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt."""
    tag_audio_events: NotRequired[bool]
    r"""Whether to tag audio events like (laughter), (footsteps), etc. in the transcription."""
    num_speakers: NotRequired[float]
    r"""The maximum amount of speakers talking in the uploaded file. Helps with predicting who speaks when, the maximum is 32."""
    timestamps_granularity: NotRequired[TimestampsGranularity]
    r"""The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word."""
    temperature: NotRequired[float]
    r"""The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit."""
    language: NotRequired[str]
    r"""The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency."""
    timestamp_granularities: NotRequired[List[TimestampGranularities]]
    r"""The timestamp granularities to populate for this transcription. response_format must be set to verbose_json to use timestamp granularities. Either or both of these options are supported: \"word\" or \"segment\". Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency."""


class PostV2ProxyAudioTranscriptionsRequestBody(BaseModel):
    r"""Transcribes audio into the input language."""

    file: Annotated[
        PostV2ProxyAudioTranscriptionsFile,
        FieldMetadata(multipart=MultipartFormMetadata(file=True)),
    ]
    r"""The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm."""

    model: Annotated[Optional[str], FieldMetadata(multipart=True)] = "openai/whisper-1"
    r"""ID of the model to use"""

    prompt: Annotated[Optional[str], FieldMetadata(multipart=True)] = None
    r"""An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language."""

    enable_logging: Annotated[Optional[bool], FieldMetadata(multipart=True)] = True
    r"""When enable_logging is set to false, zero retention mode is used. This disables history features like request stitching and is only available to enterprise customers."""

    diarize: Annotated[Optional[bool], FieldMetadata(multipart=True)] = False
    r"""Whether to annotate which speaker is currently talking in the uploaded file."""

    response_format: Annotated[
        Optional[PostV2ProxyAudioTranscriptionsResponseFormat],
        FieldMetadata(multipart=True),
    ] = None
    r"""The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt."""

    tag_audio_events: Annotated[Optional[bool], FieldMetadata(multipart=True)] = True
    r"""Whether to tag audio events like (laughter), (footsteps), etc. in the transcription."""

    num_speakers: Annotated[Optional[float], FieldMetadata(multipart=True)] = None
    r"""The maximum amount of speakers talking in the uploaded file. Helps with predicting who speaks when, the maximum is 32."""

    timestamps_granularity: Annotated[
        Optional[TimestampsGranularity], FieldMetadata(multipart=True)
    ] = "word"
    r"""The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word."""

    temperature: Annotated[Optional[float], FieldMetadata(multipart=True)] = None
    r"""The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit."""

    language: Annotated[Optional[str], FieldMetadata(multipart=True)] = None
    r"""The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency."""

    timestamp_granularities: Annotated[
        Optional[List[TimestampGranularities]], FieldMetadata(multipart=True)
    ] = None
    r"""The timestamp granularities to populate for this transcription. response_format must be set to verbose_json to use timestamp granularities. Either or both of these options are supported: \"word\" or \"segment\". Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency."""


class PostV2ProxyAudioTranscriptionsErrorTypedDict(TypedDict):
    message: str
    type: str
    param: Nullable[str]
    code: str


class PostV2ProxyAudioTranscriptionsError(BaseModel):
    message: str

    type: str

    param: Nullable[str]

    code: str

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["param"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyAudioTranscriptionsProxyResponseBodyData(BaseModel):
    error: PostV2ProxyAudioTranscriptionsError


class PostV2ProxyAudioTranscriptionsProxyResponseBody(OrqError):
    r"""Returns validation error"""

    data: PostV2ProxyAudioTranscriptionsProxyResponseBodyData

    def __init__(
        self,
        data: PostV2ProxyAudioTranscriptionsProxyResponseBodyData,
        raw_response: httpx.Response,
        body: Optional[str] = None,
    ):
        fallback = body or raw_response.text
        message = str(data.error.message) or fallback
        super().__init__(message, raw_response, body)
        self.data = data


class WordsTypedDict(TypedDict):
    word: NotRequired[str]
    start: NotRequired[float]
    end: NotRequired[float]


class Words(BaseModel):
    word: Optional[str] = None

    start: Optional[float] = None

    end: Optional[float] = None


class SegmentsTypedDict(TypedDict):
    id: float
    seek: float
    start: float
    end: float
    text: str
    tokens: List[float]
    temperature: float
    avg_logprob: float
    compression_ratio: float
    no_speech_prob: float


class Segments(BaseModel):
    id: float

    seek: float

    start: float

    end: float

    text: str

    tokens: List[float]

    temperature: float

    avg_logprob: float

    compression_ratio: float

    no_speech_prob: float


class ResponseBody2TypedDict(TypedDict):
    text: str
    task: NotRequired[str]
    language: NotRequired[str]
    duration: NotRequired[float]
    words: NotRequired[List[WordsTypedDict]]
    segments: NotRequired[List[SegmentsTypedDict]]


class ResponseBody2(BaseModel):
    text: str

    task: Optional[str] = None

    language: Optional[str] = None

    duration: Optional[float] = None

    words: Optional[List[Words]] = None

    segments: Optional[List[Segments]] = None


class ResponseBody1TypedDict(TypedDict):
    text: str


class ResponseBody1(BaseModel):
    text: str


PostV2ProxyAudioTranscriptionsResponseBodyTypedDict = TypeAliasType(
    "PostV2ProxyAudioTranscriptionsResponseBodyTypedDict",
    Union[ResponseBody1TypedDict, ResponseBody2TypedDict, str],
)
r"""Returns the transcription or verbose transcription"""


PostV2ProxyAudioTranscriptionsResponseBody = TypeAliasType(
    "PostV2ProxyAudioTranscriptionsResponseBody",
    Union[ResponseBody1, ResponseBody2, str],
)
r"""Returns the transcription or verbose transcription"""
