"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import (
    Annotated,
    NotRequired,
    TypeAliasType,
    TypedDict,
    deprecated,
)


UseCases = Literal[
    "Agents simulations",
    "Agents",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Conversation",
    "Documents QA",
    "Evaluation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "Sentiment analysis",
    "SQL",
    "Summarization",
    "Tagging",
    "Translation (document)",
    "Translation (sentences)",
]


CreatePromptLanguage = Literal[
    "Chinese",
    "Dutch",
    "English",
    "French",
    "German",
    "Russian",
    "Spanish",
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[UseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[Nullable[CreatePromptLanguage]]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptMetadata(BaseModel):
    use_cases: Optional[List[UseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: OptionalNullable[CreatePromptLanguage] = UNSET
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["use_cases", "language"]
        nullable_fields = ["language"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreatePromptRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""


CreatePrompt2PromptsRequestType = Literal["file",]
r"""The type of the content part. Always `file`."""


class CreatePrompt2FileTypedDict(TypedDict):
    file_data: NotRequired[str]
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""
    uri: NotRequired[str]
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""
    mime_type: NotRequired[str]
    r"""MIME type of the file (e.g., application/pdf, image/png)"""
    filename: NotRequired[str]
    r"""The name of the file, used when passing the file to the model as a string."""


class CreatePrompt2File(BaseModel):
    file_data: Optional[str] = None
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""

    uri: Optional[str] = None
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""MIME type of the file (e.g., application/pdf, image/png)"""

    filename: Optional[str] = None
    r"""The name of the file, used when passing the file to the model as a string."""


class CreatePrompt23TypedDict(TypedDict):
    type: CreatePrompt2PromptsRequestType
    r"""The type of the content part. Always `file`."""
    file: CreatePrompt2FileTypedDict


class CreatePrompt23(BaseModel):
    type: CreatePrompt2PromptsRequestType
    r"""The type of the content part. Always `file`."""

    file: CreatePrompt2File


CreatePrompt2PromptsType = Literal["image_url",]


class CreatePrompt2ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class CreatePrompt2ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class CreatePrompt22TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: CreatePrompt2PromptsType
    image_url: CreatePrompt2ImageURLTypedDict


class CreatePrompt22(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: CreatePrompt2PromptsType

    image_url: CreatePrompt2ImageURL


CreatePrompt2Type = Literal["text",]


class CreatePrompt21TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: CreatePrompt2Type
    text: str


class CreatePrompt21(BaseModel):
    r"""Text content part of a prompt message"""

    type: CreatePrompt2Type

    text: str


CreatePromptContent2TypedDict = TypeAliasType(
    "CreatePromptContent2TypedDict",
    Union[CreatePrompt21TypedDict, CreatePrompt22TypedDict, CreatePrompt23TypedDict],
)


CreatePromptContent2 = TypeAliasType(
    "CreatePromptContent2", Union[CreatePrompt21, CreatePrompt22, CreatePrompt23]
)


CreatePromptContentTypedDict = TypeAliasType(
    "CreatePromptContentTypedDict", Union[str, List[CreatePromptContent2TypedDict]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""


CreatePromptContent = TypeAliasType(
    "CreatePromptContent", Union[str, List[CreatePromptContent2]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""


CreatePromptType = Literal["function",]


class CreatePromptFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class CreatePromptFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class CreatePromptToolCallsTypedDict(TypedDict):
    type: CreatePromptType
    function: CreatePromptFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class CreatePromptToolCalls(BaseModel):
    type: CreatePromptType

    function: CreatePromptFunction

    id: Optional[str] = None

    index: Optional[float] = None


class CreatePromptMessagesTypedDict(TypedDict):
    role: CreatePromptRole
    r"""The role of the prompt message"""
    content: Nullable[CreatePromptContentTypedDict]
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""
    tool_calls: NotRequired[List[CreatePromptToolCallsTypedDict]]
    tool_call_id: NotRequired[str]


class CreatePromptMessages(BaseModel):
    role: CreatePromptRole
    r"""The role of the prompt message"""

    content: Nullable[CreatePromptContent]
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""

    tool_calls: Optional[List[CreatePromptToolCalls]] = None

    tool_call_id: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["tool_calls", "tool_call_id"]
        nullable_fields = ["content"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreatePromptFormat = Literal[
    "url",
    "b64_json",
    "text",
    "json_object",
]
r"""Only supported on `image` models."""


Six = Literal[
    "json",
    "text",
    "srt",
    "verbose_json",
    "vtt",
]


Five = Literal[
    "url",
    "base64_json",
]


ResponseFormat4 = Literal[
    "mp3",
    "opus",
    "aac",
    "flac",
    "wav",
    "pcm",
]


CreatePromptResponseFormatPromptsRequestType = Literal["text",]


class ResponseFormat3TypedDict(TypedDict):
    type: CreatePromptResponseFormatPromptsRequestType


class ResponseFormat3(BaseModel):
    type: CreatePromptResponseFormatPromptsRequestType


CreatePromptResponseFormatPromptsType = Literal["json_object",]


class ResponseFormat2TypedDict(TypedDict):
    type: CreatePromptResponseFormatPromptsType


class ResponseFormat2(BaseModel):
    type: CreatePromptResponseFormatPromptsType


CreatePromptResponseFormatType = Literal["json_schema",]


class CreatePromptResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    schema_: Dict[str, Any]
    description: NotRequired[str]
    strict: NotRequired[bool]


class CreatePromptResponseFormatJSONSchema(BaseModel):
    name: str

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]

    description: Optional[str] = None

    strict: Optional[bool] = None


class ResponseFormat1TypedDict(TypedDict):
    type: CreatePromptResponseFormatType
    json_schema: CreatePromptResponseFormatJSONSchemaTypedDict
    display_name: NotRequired[str]


class ResponseFormat1(BaseModel):
    type: CreatePromptResponseFormatType

    json_schema: CreatePromptResponseFormatJSONSchema

    display_name: Optional[str] = None


CreatePromptResponseFormatTypedDict = TypeAliasType(
    "CreatePromptResponseFormatTypedDict",
    Union[
        ResponseFormat2TypedDict,
        ResponseFormat3TypedDict,
        ResponseFormat1TypedDict,
        ResponseFormat4,
        Five,
        Six,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


CreatePromptResponseFormat = TypeAliasType(
    "CreatePromptResponseFormat",
    Union[
        ResponseFormat2, ResponseFormat3, ResponseFormat1, ResponseFormat4, Five, Six
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


PhotoRealVersion = Literal[
    "v1",
    "v2",
]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""


EncodingFormat = Literal[
    "float",
    "base64",
]
r"""The format to return the embeddings"""


ReasoningEffort = Literal[
    "none",
    "disable",
    "minimal",
    "low",
    "medium",
    "high",
]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


Verbosity = Literal[
    "low",
    "medium",
    "high",
]
r"""Controls the verbosity of the model output."""


class ModelParametersTypedDict(TypedDict):
    r"""Optional model parameters like temperature and maxTokens."""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[CreatePromptFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[str]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[Nullable[CreatePromptResponseFormatTypedDict]]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[PhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[EncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[ReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""
    budget_tokens: NotRequired[float]
    r"""Gives the model enhanced reasoning capabilities for complex tasks. A value of 0 disables thinking. The minimum budget tokens for thinking are 1024. The Budget Tokens should never exceed the Max Tokens parameter. Only supported by `Anthropic`"""
    verbosity: NotRequired[Verbosity]
    r"""Controls the verbosity of the model output."""


class ModelParameters(BaseModel):
    r"""Optional model parameters like temperature and maxTokens."""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[Optional[CreatePromptFormat], pydantic.Field(alias="format")] = (
        None
    )
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[str] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[CreatePromptResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[PhotoRealVersion], pydantic.Field(alias="photoRealVersion")
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[EncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[ReasoningEffort], pydantic.Field(alias="reasoningEffort")
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    budget_tokens: Annotated[Optional[float], pydantic.Field(alias="budgetTokens")] = (
        None
    )
    r"""Gives the model enhanced reasoning capabilities for complex tasks. A value of 0 disables thinking. The minimum budget tokens for thinking are 1024. The Budget Tokens should never exceed the Max Tokens parameter. Only supported by `Anthropic`"""

    verbosity: Optional[Verbosity] = None
    r"""Controls the verbosity of the model output."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
            "budgetTokens",
            "verbosity",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


@deprecated(
    "warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
)
class PromptConfigurationTypedDict(TypedDict):
    r"""[DEPRECATED]. Please use the `prompt` property instead. The current `prompt_config` will keep working but it will be deprecated in future versions. Configuration for the prompt including model and messages."""

    messages: List[CreatePromptMessagesTypedDict]
    r"""Array of messages that make up the conversation."""
    model: NotRequired[str]
    r"""Model ID used to generate the response, like `openai/gpt-4o` or `google/gemini-2.5-pro`. The full list of models can be found at https://docs.orq.ai/docs/ai-gateway-supported-models. Only chat models are supported."""
    model_parameters: NotRequired[ModelParametersTypedDict]
    r"""Optional model parameters like temperature and maxTokens."""


@deprecated(
    "warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
)
class PromptConfiguration(BaseModel):
    r"""[DEPRECATED]. Please use the `prompt` property instead. The current `prompt_config` will keep working but it will be deprecated in future versions. Configuration for the prompt including model and messages."""

    messages: List[CreatePromptMessages]
    r"""Array of messages that make up the conversation."""

    model: Optional[str] = None
    r"""Model ID used to generate the response, like `openai/gpt-4o` or `google/gemini-2.5-pro`. The full list of models can be found at https://docs.orq.ai/docs/ai-gateway-supported-models. Only chat models are supported."""

    model_parameters: Optional[ModelParameters] = None
    r"""Optional model parameters like temperature and maxTokens."""


CreatePromptMessagesPromptsRequestRequestBodyRole = Literal["tool",]
r"""The role of the messages author, in this case tool."""


CreatePromptMessagesPromptsRequestContentTypedDict = TypeAliasType(
    "CreatePromptMessagesPromptsRequestContentTypedDict", Union[str, List[str]]
)
r"""The contents of the tool message."""


CreatePromptMessagesPromptsRequestContent = TypeAliasType(
    "CreatePromptMessagesPromptsRequestContent", Union[str, List[str]]
)
r"""The contents of the tool message."""


class CreatePromptMessagesToolMessageTypedDict(TypedDict):
    role: CreatePromptMessagesPromptsRequestRequestBodyRole
    r"""The role of the messages author, in this case tool."""
    content: CreatePromptMessagesPromptsRequestContentTypedDict
    r"""The contents of the tool message."""
    tool_call_id: str
    r"""Tool call that this message is responding to."""


class CreatePromptMessagesToolMessage(BaseModel):
    role: CreatePromptMessagesPromptsRequestRequestBodyRole
    r"""The role of the messages author, in this case tool."""

    content: CreatePromptMessagesPromptsRequestContent
    r"""The contents of the tool message."""

    tool_call_id: str
    r"""Tool call that this message is responding to."""


CreatePrompt2PromptsRequestRequestBodyPromptMessages3ContentType = Literal["refusal",]
r"""The type of the content part."""


class CreatePrompt2RefusalContentPartTypedDict(TypedDict):
    type: CreatePrompt2PromptsRequestRequestBodyPromptMessages3ContentType
    r"""The type of the content part."""
    refusal: str
    r"""The refusal message generated by the model."""


class CreatePrompt2RefusalContentPart(BaseModel):
    type: CreatePrompt2PromptsRequestRequestBodyPromptMessages3ContentType
    r"""The type of the content part."""

    refusal: str
    r"""The refusal message generated by the model."""


CreatePrompt2PromptsRequestRequestBodyPromptMessages3Type = Literal["text",]
r"""The type of the content part."""


CreatePromptAnnotationsPromptsType = Literal["file_path",]


class CreatePromptAnnotationsFilePathTypedDict(TypedDict):
    file_id: str


class CreatePromptAnnotationsFilePath(BaseModel):
    file_id: str


class CreatePromptAnnotations2TypedDict(TypedDict):
    type: CreatePromptAnnotationsPromptsType
    text: str
    file_path: CreatePromptAnnotationsFilePathTypedDict
    start_index: int
    end_index: int


class CreatePromptAnnotations2(BaseModel):
    type: CreatePromptAnnotationsPromptsType

    text: str

    file_path: CreatePromptAnnotationsFilePath

    start_index: int

    end_index: int


CreatePromptAnnotationsType = Literal["file_citation",]


class CreatePromptAnnotationsFileCitationTypedDict(TypedDict):
    file_id: str
    quote: NotRequired[str]


class CreatePromptAnnotationsFileCitation(BaseModel):
    file_id: str

    quote: Optional[str] = None


class CreatePromptAnnotations1TypedDict(TypedDict):
    type: CreatePromptAnnotationsType
    text: str
    file_citation: CreatePromptAnnotationsFileCitationTypedDict
    start_index: int
    end_index: int


class CreatePromptAnnotations1(BaseModel):
    type: CreatePromptAnnotationsType

    text: str

    file_citation: CreatePromptAnnotationsFileCitation

    start_index: int

    end_index: int


CreatePrompt2AnnotationsTypedDict = TypeAliasType(
    "CreatePrompt2AnnotationsTypedDict",
    Union[CreatePromptAnnotations1TypedDict, CreatePromptAnnotations2TypedDict],
)


CreatePrompt2Annotations = TypeAliasType(
    "CreatePrompt2Annotations",
    Union[CreatePromptAnnotations1, CreatePromptAnnotations2],
)


class CreatePrompt2TextContentPartTypedDict(TypedDict):
    type: CreatePrompt2PromptsRequestRequestBodyPromptMessages3Type
    r"""The type of the content part."""
    text: str
    r"""The text content."""
    annotations: NotRequired[List[CreatePrompt2AnnotationsTypedDict]]
    r"""Annotations for the text content."""


class CreatePrompt2TextContentPart(BaseModel):
    type: CreatePrompt2PromptsRequestRequestBodyPromptMessages3Type
    r"""The type of the content part."""

    text: str
    r"""The text content."""

    annotations: Optional[List[CreatePrompt2Annotations]] = None
    r"""Annotations for the text content."""


CreatePromptContentPromptsRequest2TypedDict = TypeAliasType(
    "CreatePromptContentPromptsRequest2TypedDict",
    Union[
        CreatePrompt2RefusalContentPartTypedDict, CreatePrompt2TextContentPartTypedDict
    ],
)


CreatePromptContentPromptsRequest2 = TypeAliasType(
    "CreatePromptContentPromptsRequest2",
    Union[CreatePrompt2RefusalContentPart, CreatePrompt2TextContentPart],
)


CreatePromptMessagesPromptsContentTypedDict = TypeAliasType(
    "CreatePromptMessagesPromptsContentTypedDict",
    Union[str, List[CreatePromptContentPromptsRequest2TypedDict]],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


CreatePromptMessagesPromptsContent = TypeAliasType(
    "CreatePromptMessagesPromptsContent",
    Union[str, List[CreatePromptContentPromptsRequest2]],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


CreatePromptMessagesPromptsRequestRole = Literal["assistant",]
r"""The role of the messages author, in this case `assistant`."""


class CreatePromptMessagesAudioTypedDict(TypedDict):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


class CreatePromptMessagesAudio(BaseModel):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


CreatePromptMessagesType = Literal["function",]
r"""The type of the tool. Currently, only `function` is supported."""


class CreatePromptMessagesFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreatePromptMessagesFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreatePromptMessagesToolCallsTypedDict(TypedDict):
    id: str
    r"""The ID of the tool call."""
    type: CreatePromptMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""
    function: CreatePromptMessagesFunctionTypedDict


class CreatePromptMessagesToolCalls(BaseModel):
    id: str
    r"""The ID of the tool call."""

    type: CreatePromptMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""

    function: CreatePromptMessagesFunction


class CreatePromptMessagesAssistantMessageTypedDict(TypedDict):
    role: CreatePromptMessagesPromptsRequestRole
    r"""The role of the messages author, in this case `assistant`."""
    content: NotRequired[Nullable[CreatePromptMessagesPromptsContentTypedDict]]
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""
    refusal: NotRequired[Nullable[str]]
    r"""The refusal message by the assistant."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""
    audio: NotRequired[Nullable[CreatePromptMessagesAudioTypedDict]]
    r"""Data about a previous audio response from the model."""
    tool_calls: NotRequired[List[CreatePromptMessagesToolCallsTypedDict]]
    r"""The tool calls generated by the model, such as function calls."""
    reasoning: NotRequired[str]
    r"""Internal thought process of the model"""
    reasoning_signature: NotRequired[str]
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""
    redacted_reasoning: NotRequired[str]
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""


class CreatePromptMessagesAssistantMessage(BaseModel):
    role: CreatePromptMessagesPromptsRequestRole
    r"""The role of the messages author, in this case `assistant`."""

    content: OptionalNullable[CreatePromptMessagesPromptsContent] = UNSET
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""

    refusal: OptionalNullable[str] = UNSET
    r"""The refusal message by the assistant."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    audio: OptionalNullable[CreatePromptMessagesAudio] = UNSET
    r"""Data about a previous audio response from the model."""

    tool_calls: Optional[List[CreatePromptMessagesToolCalls]] = None
    r"""The tool calls generated by the model, such as function calls."""

    reasoning: Optional[str] = None
    r"""Internal thought process of the model"""

    reasoning_signature: Optional[str] = None
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""

    redacted_reasoning: Optional[str] = None
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "content",
            "refusal",
            "name",
            "audio",
            "tool_calls",
            "reasoning",
            "reasoning_signature",
            "redacted_reasoning",
        ]
        nullable_fields = ["content", "refusal", "audio"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreatePromptMessagesPromptsRole = Literal["user",]
r"""The role of the messages author, in this case `user`."""


CreatePrompt2PromptsRequestRequestBodyPromptMessages2Type = Literal["file",]
r"""The type of the content part. Always `file`."""


class CreatePrompt2PromptsFileTypedDict(TypedDict):
    r"""File data for the content part. Must contain either file_data or uri, but not both."""

    file_data: NotRequired[str]
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""
    uri: NotRequired[str]
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""
    mime_type: NotRequired[str]
    r"""MIME type of the file (e.g., application/pdf, image/png)"""
    filename: NotRequired[str]
    r"""The name of the file, used when passing the file to the model as a string."""


class CreatePrompt2PromptsFile(BaseModel):
    r"""File data for the content part. Must contain either file_data or uri, but not both."""

    file_data: Optional[str] = None
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""

    uri: Optional[str] = None
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""MIME type of the file (e.g., application/pdf, image/png)"""

    filename: Optional[str] = None
    r"""The name of the file, used when passing the file to the model as a string."""


class CreatePrompt24TypedDict(TypedDict):
    type: CreatePrompt2PromptsRequestRequestBodyPromptMessages2Type
    r"""The type of the content part. Always `file`."""
    file: CreatePrompt2PromptsFileTypedDict
    r"""File data for the content part. Must contain either file_data or uri, but not both."""


class CreatePrompt24(BaseModel):
    type: CreatePrompt2PromptsRequestRequestBodyPromptMessages2Type
    r"""The type of the content part. Always `file`."""

    file: CreatePrompt2PromptsFile
    r"""File data for the content part. Must contain either file_data or uri, but not both."""


CreatePrompt2PromptsRequestRequestBodyPromptMessagesType = Literal["input_audio",]


CreatePrompt2Format = Literal[
    "mp3",
    "wav",
]
r"""The format of the encoded audio data. Currently supports `wav` and `mp3`."""


class CreatePrompt2InputAudioTypedDict(TypedDict):
    data: str
    r"""Base64 encoded audio data."""
    format_: CreatePrompt2Format
    r"""The format of the encoded audio data. Currently supports `wav` and `mp3`."""


class CreatePrompt2InputAudio(BaseModel):
    data: str
    r"""Base64 encoded audio data."""

    format_: Annotated[CreatePrompt2Format, pydantic.Field(alias="format")]
    r"""The format of the encoded audio data. Currently supports `wav` and `mp3`."""


class CreatePrompt2Prompts3TypedDict(TypedDict):
    type: CreatePrompt2PromptsRequestRequestBodyPromptMessagesType
    input_audio: CreatePrompt2InputAudioTypedDict


class CreatePrompt2Prompts3(BaseModel):
    type: CreatePrompt2PromptsRequestRequestBodyPromptMessagesType

    input_audio: CreatePrompt2InputAudio


CreatePrompt2PromptsRequestRequestBodyPromptType = Literal["image_url",]


CreatePrompt2Detail = Literal[
    "low",
    "high",
    "auto",
]
r"""Specifies the detail level of the image."""


class CreatePrompt2PromptsImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded image data."""
    detail: NotRequired[CreatePrompt2Detail]
    r"""Specifies the detail level of the image."""


class CreatePrompt2PromptsImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded image data."""

    detail: Optional[CreatePrompt2Detail] = None
    r"""Specifies the detail level of the image."""


class CreatePrompt2Prompts2TypedDict(TypedDict):
    type: CreatePrompt2PromptsRequestRequestBodyPromptType
    image_url: CreatePrompt2PromptsImageURLTypedDict


class CreatePrompt2Prompts2(BaseModel):
    type: CreatePrompt2PromptsRequestRequestBodyPromptType

    image_url: CreatePrompt2PromptsImageURL


CreatePrompt2PromptsRequestRequestBodyType = Literal["text",]


class CreatePrompt2Prompts1TypedDict(TypedDict):
    type: CreatePrompt2PromptsRequestRequestBodyType
    text: str


class CreatePrompt2Prompts1(BaseModel):
    type: CreatePrompt2PromptsRequestRequestBodyType

    text: str


CreatePromptContentPrompts2TypedDict = TypeAliasType(
    "CreatePromptContentPrompts2TypedDict",
    Union[
        CreatePrompt2Prompts1TypedDict,
        CreatePrompt2Prompts2TypedDict,
        CreatePrompt2Prompts3TypedDict,
        CreatePrompt24TypedDict,
    ],
)


CreatePromptContentPrompts2 = TypeAliasType(
    "CreatePromptContentPrompts2",
    Union[
        CreatePrompt2Prompts1,
        CreatePrompt2Prompts2,
        CreatePrompt2Prompts3,
        CreatePrompt24,
    ],
)


CreatePromptMessagesContentTypedDict = TypeAliasType(
    "CreatePromptMessagesContentTypedDict",
    Union[str, List[CreatePromptContentPrompts2TypedDict]],
)
r"""The contents of the user message."""


CreatePromptMessagesContent = TypeAliasType(
    "CreatePromptMessagesContent", Union[str, List[CreatePromptContentPrompts2]]
)
r"""The contents of the user message."""


class CreatePromptMessagesUserMessageTypedDict(TypedDict):
    role: CreatePromptMessagesPromptsRole
    r"""The role of the messages author, in this case `user`."""
    content: CreatePromptMessagesContentTypedDict
    r"""The contents of the user message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class CreatePromptMessagesUserMessage(BaseModel):
    role: CreatePromptMessagesPromptsRole
    r"""The role of the messages author, in this case `user`."""

    content: CreatePromptMessagesContent
    r"""The contents of the user message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


CreatePromptMessagesRole = Literal["system",]
r"""The role of the messages author, in this case `system`."""


class CreatePromptMessagesSystemMessageTypedDict(TypedDict):
    role: CreatePromptMessagesRole
    r"""The role of the messages author, in this case `system`."""
    content: str
    r"""The contents of the system message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class CreatePromptMessagesSystemMessage(BaseModel):
    role: CreatePromptMessagesRole
    r"""The role of the messages author, in this case `system`."""

    content: str
    r"""The contents of the system message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


CreatePromptPromptsMessagesTypedDict = TypeAliasType(
    "CreatePromptPromptsMessagesTypedDict",
    Union[
        CreatePromptMessagesSystemMessageTypedDict,
        CreatePromptMessagesUserMessageTypedDict,
        CreatePromptMessagesToolMessageTypedDict,
        CreatePromptMessagesAssistantMessageTypedDict,
    ],
)


CreatePromptPromptsMessages = TypeAliasType(
    "CreatePromptPromptsMessages",
    Union[
        CreatePromptMessagesSystemMessage,
        CreatePromptMessagesUserMessage,
        CreatePromptMessagesToolMessage,
        CreatePromptMessagesAssistantMessage,
    ],
)


CreatePromptResponseFormatPromptsRequestRequestBodyPrompt3Type = Literal["json_schema",]


class CreatePromptResponseFormatPromptsRequestJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[Nullable[bool]]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class CreatePromptResponseFormatPromptsRequestJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: OptionalNullable[bool] = UNSET
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "schema", "strict"]
        nullable_fields = ["strict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreatePromptResponseFormatPromptsJSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreatePromptResponseFormatPromptsRequestRequestBodyPrompt3Type
    json_schema: CreatePromptResponseFormatPromptsRequestJSONSchemaTypedDict


class CreatePromptResponseFormatPromptsJSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreatePromptResponseFormatPromptsRequestRequestBodyPrompt3Type

    json_schema: CreatePromptResponseFormatPromptsRequestJSONSchema


CreatePromptResponseFormatPromptsRequestRequestBodyPromptType = Literal["json_object",]


class CreatePromptResponseFormatJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreatePromptResponseFormatPromptsRequestRequestBodyPromptType


class CreatePromptResponseFormatJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreatePromptResponseFormatPromptsRequestRequestBodyPromptType


CreatePromptResponseFormatPromptsRequestRequestBodyType = Literal["text",]


class CreatePromptResponseFormatTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: CreatePromptResponseFormatPromptsRequestRequestBodyType


class CreatePromptResponseFormatText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: CreatePromptResponseFormatPromptsRequestRequestBodyType


CreatePromptPromptsResponseFormatTypedDict = TypeAliasType(
    "CreatePromptPromptsResponseFormatTypedDict",
    Union[
        CreatePromptResponseFormatTextTypedDict,
        CreatePromptResponseFormatJSONObjectTypedDict,
        CreatePromptResponseFormatPromptsJSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


CreatePromptPromptsResponseFormat = TypeAliasType(
    "CreatePromptPromptsResponseFormat",
    Union[
        CreatePromptResponseFormatText,
        CreatePromptResponseFormatJSONObject,
        CreatePromptResponseFormatPromptsJSONSchema,
    ],
)
r"""An object specifying the format that the model must output"""


class PromptInputTypedDict(TypedDict):
    r"""Prompt configuration with model and messages. Either this field or `prompt_config` must be provided."""

    messages: List[CreatePromptPromptsMessagesTypedDict]
    r"""Array of messages that make up the conversation. Each message has a role (system, user, assistant, or tool) and content."""
    model: NotRequired[str]
    r"""Model ID used to generate the response, like `openai/gpt-4o` or `anthropic/claude-3-5-sonnet-20241022`. The full list of models can be found at https://docs.orq.ai/docs/ai-gateway-supported-models. Only chat models are supported."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    response_format: NotRequired[CreatePromptPromptsResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""


class PromptInput(BaseModel):
    r"""Prompt configuration with model and messages. Either this field or `prompt_config` must be provided."""

    messages: List[CreatePromptPromptsMessages]
    r"""Array of messages that make up the conversation. Each message has a role (system, user, assistant, or tool) and content."""

    model: Optional[str] = None
    r"""Model ID used to generate the response, like `openai/gpt-4o` or `anthropic/claude-3-5-sonnet-20241022`. The full list of models can be found at https://docs.orq.ai/docs/ai-gateway-supported-models. Only chat models are supported."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    response_format: Optional[CreatePromptPromptsResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["model", "temperature", "max_tokens", "response_format"]
        nullable_fields = ["temperature", "max_tokens"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreatePromptRequestBodyTypedDict(TypedDict):
    display_name: str
    r"""The prompts name, meant to be displayable in the UI."""
    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """
    description: NotRequired[Nullable[str]]
    r"""The prompts description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""
    metadata: NotRequired[CreatePromptMetadataTypedDict]
    prompt_config: NotRequired[PromptConfigurationTypedDict]
    r"""[DEPRECATED]. Please use the `prompt` property instead. The current `prompt_config` will keep working but it will be deprecated in future versions. Configuration for the prompt including model and messages."""
    prompt: NotRequired[PromptInputTypedDict]
    r"""Prompt configuration with model and messages. Either this field or `prompt_config` must be provided."""


class CreatePromptRequestBody(BaseModel):
    display_name: str
    r"""The prompts name, meant to be displayable in the UI."""

    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """

    description: OptionalNullable[str] = UNSET
    r"""The prompts description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    metadata: Optional[CreatePromptMetadata] = None

    prompt_config: Annotated[
        Optional[PromptConfiguration],
        pydantic.Field(
            deprecated="warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
        ),
    ] = None
    r"""[DEPRECATED]. Please use the `prompt` property instead. The current `prompt_config` will keep working but it will be deprecated in future versions. Configuration for the prompt including model and messages."""

    prompt: Optional[PromptInput] = None
    r"""Prompt configuration with model and messages. Either this field or `prompt_config` must be provided."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "metadata", "prompt_config", "prompt"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreatePromptPromptsType = Literal["prompt",]


CreatePromptModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderation",
    "vision",
]
r"""The modality of the model"""


CreatePromptPromptsFormat = Literal[
    "url",
    "b64_json",
    "text",
    "json_object",
]
r"""Only supported on `image` models."""


CreatePromptResponseFormat6 = Literal[
    "json",
    "text",
    "srt",
    "verbose_json",
    "vtt",
]


CreatePromptResponseFormat5 = Literal[
    "url",
    "base64_json",
]


CreatePromptResponseFormat4 = Literal[
    "mp3",
    "opus",
    "aac",
    "flac",
    "wav",
    "pcm",
]


CreatePromptResponseFormatPromptsResponse200ApplicationJSONType = Literal["text",]


class CreatePromptResponseFormat3TypedDict(TypedDict):
    type: CreatePromptResponseFormatPromptsResponse200ApplicationJSONType


class CreatePromptResponseFormat3(BaseModel):
    type: CreatePromptResponseFormatPromptsResponse200ApplicationJSONType


CreatePromptResponseFormatPromptsResponse200Type = Literal["json_object",]


class CreatePromptResponseFormat2TypedDict(TypedDict):
    type: CreatePromptResponseFormatPromptsResponse200Type


class CreatePromptResponseFormat2(BaseModel):
    type: CreatePromptResponseFormatPromptsResponse200Type


CreatePromptResponseFormatPromptsResponseType = Literal["json_schema",]


class CreatePromptResponseFormatPromptsResponseJSONSchemaTypedDict(TypedDict):
    name: str
    schema_: Dict[str, Any]
    description: NotRequired[str]
    strict: NotRequired[bool]


class CreatePromptResponseFormatPromptsResponseJSONSchema(BaseModel):
    name: str

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]

    description: Optional[str] = None

    strict: Optional[bool] = None


class CreatePromptResponseFormat1TypedDict(TypedDict):
    type: CreatePromptResponseFormatPromptsResponseType
    json_schema: CreatePromptResponseFormatPromptsResponseJSONSchemaTypedDict
    display_name: NotRequired[str]


class CreatePromptResponseFormat1(BaseModel):
    type: CreatePromptResponseFormatPromptsResponseType

    json_schema: CreatePromptResponseFormatPromptsResponseJSONSchema

    display_name: Optional[str] = None


CreatePromptPromptsResponseResponseFormatTypedDict = TypeAliasType(
    "CreatePromptPromptsResponseResponseFormatTypedDict",
    Union[
        CreatePromptResponseFormat2TypedDict,
        CreatePromptResponseFormat3TypedDict,
        CreatePromptResponseFormat1TypedDict,
        CreatePromptResponseFormat4,
        CreatePromptResponseFormat5,
        CreatePromptResponseFormat6,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


CreatePromptPromptsResponseResponseFormat = TypeAliasType(
    "CreatePromptPromptsResponseResponseFormat",
    Union[
        CreatePromptResponseFormat2,
        CreatePromptResponseFormat3,
        CreatePromptResponseFormat1,
        CreatePromptResponseFormat4,
        CreatePromptResponseFormat5,
        CreatePromptResponseFormat6,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


CreatePromptPhotoRealVersion = Literal[
    "v1",
    "v2",
]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""


CreatePromptEncodingFormat = Literal[
    "float",
    "base64",
]
r"""The format to return the embeddings"""


CreatePromptReasoningEffort = Literal[
    "none",
    "disable",
    "minimal",
    "low",
    "medium",
    "high",
]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


CreatePromptVerbosity = Literal[
    "low",
    "medium",
    "high",
]
r"""Controls the verbosity of the model output."""


class CreatePromptModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[CreatePromptPromptsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[str]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[CreatePromptPromptsResponseResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[CreatePromptPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[CreatePromptEncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[CreatePromptReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""
    budget_tokens: NotRequired[float]
    r"""Gives the model enhanced reasoning capabilities for complex tasks. A value of 0 disables thinking. The minimum budget tokens for thinking are 1024. The Budget Tokens should never exceed the Max Tokens parameter. Only supported by `Anthropic`"""
    verbosity: NotRequired[CreatePromptVerbosity]
    r"""Controls the verbosity of the model output."""


class CreatePromptModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[
        Optional[CreatePromptPromptsFormat], pydantic.Field(alias="format")
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[str] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[CreatePromptPromptsResponseResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[CreatePromptPhotoRealVersion], pydantic.Field(alias="photoRealVersion")
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[CreatePromptEncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[CreatePromptReasoningEffort], pydantic.Field(alias="reasoningEffort")
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    budget_tokens: Annotated[Optional[float], pydantic.Field(alias="budgetTokens")] = (
        None
    )
    r"""Gives the model enhanced reasoning capabilities for complex tasks. A value of 0 disables thinking. The minimum budget tokens for thinking are 1024. The Budget Tokens should never exceed the Max Tokens parameter. Only supported by `Anthropic`"""

    verbosity: Optional[CreatePromptVerbosity] = None
    r"""Controls the verbosity of the model output."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
            "budgetTokens",
            "verbosity",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreatePromptProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
    "litellm",
    "openailike",
    "cerebras",
    "bytedance",
    "mistral",
]


CreatePromptPromptsRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""


CreatePrompt2PromptsResponse200ApplicationJSONType = Literal["file",]
r"""The type of the content part. Always `file`."""


class CreatePrompt2PromptsResponseFileTypedDict(TypedDict):
    file_data: NotRequired[str]
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""
    uri: NotRequired[str]
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""
    mime_type: NotRequired[str]
    r"""MIME type of the file (e.g., application/pdf, image/png)"""
    filename: NotRequired[str]
    r"""The name of the file, used when passing the file to the model as a string."""


class CreatePrompt2PromptsResponseFile(BaseModel):
    file_data: Optional[str] = None
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""

    uri: Optional[str] = None
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""MIME type of the file (e.g., application/pdf, image/png)"""

    filename: Optional[str] = None
    r"""The name of the file, used when passing the file to the model as a string."""


class CreatePrompt2PromptsResponse3TypedDict(TypedDict):
    type: CreatePrompt2PromptsResponse200ApplicationJSONType
    r"""The type of the content part. Always `file`."""
    file: CreatePrompt2PromptsResponseFileTypedDict


class CreatePrompt2PromptsResponse3(BaseModel):
    type: CreatePrompt2PromptsResponse200ApplicationJSONType
    r"""The type of the content part. Always `file`."""

    file: CreatePrompt2PromptsResponseFile


CreatePrompt2PromptsResponse200Type = Literal["image_url",]


class CreatePrompt2PromptsResponseImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class CreatePrompt2PromptsResponseImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class CreatePrompt2PromptsResponse2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: CreatePrompt2PromptsResponse200Type
    image_url: CreatePrompt2PromptsResponseImageURLTypedDict


class CreatePrompt2PromptsResponse2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: CreatePrompt2PromptsResponse200Type

    image_url: CreatePrompt2PromptsResponseImageURL


CreatePrompt2PromptsResponseType = Literal["text",]


class CreatePrompt2PromptsResponse1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: CreatePrompt2PromptsResponseType
    text: str


class CreatePrompt2PromptsResponse1(BaseModel):
    r"""Text content part of a prompt message"""

    type: CreatePrompt2PromptsResponseType

    text: str


CreatePromptContentPromptsResponse2TypedDict = TypeAliasType(
    "CreatePromptContentPromptsResponse2TypedDict",
    Union[
        CreatePrompt2PromptsResponse1TypedDict,
        CreatePrompt2PromptsResponse2TypedDict,
        CreatePrompt2PromptsResponse3TypedDict,
    ],
)


CreatePromptContentPromptsResponse2 = TypeAliasType(
    "CreatePromptContentPromptsResponse2",
    Union[
        CreatePrompt2PromptsResponse1,
        CreatePrompt2PromptsResponse2,
        CreatePrompt2PromptsResponse3,
    ],
)


CreatePromptPromptsContentTypedDict = TypeAliasType(
    "CreatePromptPromptsContentTypedDict",
    Union[str, List[CreatePromptContentPromptsResponse2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""


CreatePromptPromptsContent = TypeAliasType(
    "CreatePromptPromptsContent", Union[str, List[CreatePromptContentPromptsResponse2]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""


CreatePromptPromptsResponseType = Literal["function",]


class CreatePromptPromptsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class CreatePromptPromptsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class CreatePromptPromptsToolCallsTypedDict(TypedDict):
    type: CreatePromptPromptsResponseType
    function: CreatePromptPromptsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class CreatePromptPromptsToolCalls(BaseModel):
    type: CreatePromptPromptsResponseType

    function: CreatePromptPromptsFunction

    id: Optional[str] = None

    index: Optional[float] = None


class CreatePromptPromptsResponseMessagesTypedDict(TypedDict):
    role: CreatePromptPromptsRole
    r"""The role of the prompt message"""
    content: Nullable[CreatePromptPromptsContentTypedDict]
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""
    tool_calls: NotRequired[List[CreatePromptPromptsToolCallsTypedDict]]
    tool_call_id: NotRequired[str]


class CreatePromptPromptsResponseMessages(BaseModel):
    role: CreatePromptPromptsRole
    r"""The role of the prompt message"""

    content: Nullable[CreatePromptPromptsContent]
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""

    tool_calls: Optional[List[CreatePromptPromptsToolCalls]] = None

    tool_call_id: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["tool_calls", "tool_call_id"]
        nullable_fields = ["content"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreatePromptPromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[CreatePromptPromptsResponseMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    model_type: NotRequired[Nullable[CreatePromptModelType]]
    r"""The modality of the model"""
    model_parameters: NotRequired[CreatePromptModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[CreatePromptProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The ID of the integration to use"""
    version: NotRequired[str]


class CreatePromptPromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[CreatePromptPromptsResponseMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    model_type: OptionalNullable[CreatePromptModelType] = UNSET
    r"""The modality of the model"""

    model_parameters: Optional[CreatePromptModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[CreatePromptProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The ID of the integration to use"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["model_db_id", "model_type", "integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreatePromptUseCases = Literal[
    "Agents simulations",
    "Agents",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Conversation",
    "Documents QA",
    "Evaluation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "Sentiment analysis",
    "SQL",
    "Summarization",
    "Tagging",
    "Translation (document)",
    "Translation (sentences)",
]


CreatePromptPromptsLanguage = Literal[
    "Chinese",
    "Dutch",
    "English",
    "French",
    "German",
    "Russian",
    "Spanish",
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptPromptsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[CreatePromptUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[Nullable[CreatePromptPromptsLanguage]]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class CreatePromptPromptsMetadata(BaseModel):
    use_cases: Optional[List[CreatePromptUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: OptionalNullable[CreatePromptPromptsLanguage] = UNSET
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["use_cases", "language"]
        nullable_fields = ["language"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreatePromptPromptTypedDict(TypedDict):
    r"""A prompt entity with configuration, metadata, and versioning."""

    id: str
    type: CreatePromptPromptsType
    owner: str
    domain_id: str
    created: str
    updated: str
    display_name: str
    r"""The prompts name, meant to be displayable in the UI."""
    prompt_config: CreatePromptPromptConfigTypedDict
    r"""A list of messages compatible with the openAI schema"""
    created_by_id: NotRequired[Nullable[str]]
    updated_by_id: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]
    r"""The prompts description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""
    metadata: NotRequired[CreatePromptPromptsMetadataTypedDict]


class CreatePromptPrompt(BaseModel):
    r"""A prompt entity with configuration, metadata, and versioning."""

    id: Annotated[str, pydantic.Field(alias="_id")]

    type: CreatePromptPromptsType

    owner: str

    domain_id: str

    created: str

    updated: str

    display_name: str
    r"""The prompts name, meant to be displayable in the UI."""

    prompt_config: CreatePromptPromptConfig
    r"""A list of messages compatible with the openAI schema"""

    created_by_id: OptionalNullable[str] = UNSET

    updated_by_id: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET
    r"""The prompts description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    metadata: Optional[CreatePromptPromptsMetadata] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["created_by_id", "updated_by_id", "description", "metadata"]
        nullable_fields = ["created_by_id", "updated_by_id", "description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
