"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .thinkingconfigdisabledschema import (
    ThinkingConfigDisabledSchema,
    ThinkingConfigDisabledSchemaTypedDict,
)
from .thinkingconfigenabledschema import (
    ThinkingConfigEnabledSchema,
    ThinkingConfigEnabledSchemaTypedDict,
)
from dataclasses import dataclass, field
import httpx
from orq_ai_sdk.models import OrqError
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import FieldMetadata, PathParamMetadata, get_discriminator
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class RetrieveAgentRequestRequestTypedDict(TypedDict):
    agent_key: str
    r"""The unique key of the agent to retrieve"""


class RetrieveAgentRequestRequest(BaseModel):
    agent_key: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""The unique key of the agent to retrieve"""


class RetrieveAgentRequestAgentsResponseBodyData(BaseModel):
    message: str


@dataclass(unsafe_hash=True)
class RetrieveAgentRequestAgentsResponseBody(OrqError):
    r"""Agent not found. The specified agent key does not exist in the workspace or you do not have permission to access it."""

    data: RetrieveAgentRequestAgentsResponseBodyData = field(hash=False)

    def __init__(
        self,
        data: RetrieveAgentRequestAgentsResponseBodyData,
        raw_response: httpx.Response,
        body: Optional[str] = None,
    ):
        fallback = body or raw_response.text
        message = str(data.message) or fallback
        super().__init__(message, raw_response, body)
        object.__setattr__(self, "data", data)


RetrieveAgentRequestStatus = Literal[
    "live",
    "draft",
    "pending",
    "published",
]
r"""The status of the agent. `Live` is the latest version of the agent. `Draft` is a version that is not yet published. `Pending` is a version that is pending approval. `Published` is a version that was live and has been replaced by a new version."""


RetrieveAgentRequestToolApprovalRequired = Literal[
    "all",
    "respect_tool",
    "none",
]
r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""


class RetrieveAgentRequestConditionsTypedDict(TypedDict):
    condition: str
    r"""The argument of the tool call to evaluate"""
    operator: str
    r"""The operator to use"""
    value: str
    r"""The value to compare against"""


class RetrieveAgentRequestConditions(BaseModel):
    condition: str
    r"""The argument of the tool call to evaluate"""

    operator: str
    r"""The operator to use"""

    value: str
    r"""The value to compare against"""


class RetrieveAgentRequestToolsTypedDict(TypedDict):
    id: str
    r"""The id of the resource"""
    action_type: str
    key: NotRequired[str]
    r"""Optional tool key for custom tools"""
    display_name: NotRequired[str]
    description: NotRequired[str]
    r"""Optional tool description"""
    requires_approval: NotRequired[bool]
    tool_id: NotRequired[str]
    r"""Nested tool ID for MCP tools (identifies specific tool within MCP server)"""
    conditions: NotRequired[List[RetrieveAgentRequestConditionsTypedDict]]
    timeout: NotRequired[float]
    r"""Tool execution timeout in seconds (default: 2 minutes, max: 10 minutes)"""


class RetrieveAgentRequestTools(BaseModel):
    id: str
    r"""The id of the resource"""

    action_type: str

    key: Optional[str] = None
    r"""Optional tool key for custom tools"""

    display_name: Optional[str] = None

    description: Optional[str] = None
    r"""Optional tool description"""

    requires_approval: Optional[bool] = False

    tool_id: Optional[str] = None
    r"""Nested tool ID for MCP tools (identifies specific tool within MCP server)"""

    conditions: Optional[List[RetrieveAgentRequestConditions]] = None

    timeout: Optional[float] = 120
    r"""Tool execution timeout in seconds (default: 2 minutes, max: 10 minutes)"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "key",
                "display_name",
                "description",
                "requires_approval",
                "tool_id",
                "conditions",
                "timeout",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class RetrieveAgentRequestEvaluatorsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: RetrieveAgentRequestExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class RetrieveAgentRequestEvaluators(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: RetrieveAgentRequestExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["sample_rate"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestAgentsExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class RetrieveAgentRequestGuardrailsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: RetrieveAgentRequestAgentsExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class RetrieveAgentRequestGuardrails(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: RetrieveAgentRequestAgentsExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["sample_rate"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RetrieveAgentRequestSettingsTypedDict(TypedDict):
    max_iterations: NotRequired[int]
    r"""Maximum iterations(llm calls) before the agent will stop executing."""
    max_execution_time: NotRequired[int]
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""
    tool_approval_required: NotRequired[RetrieveAgentRequestToolApprovalRequired]
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""
    tools: NotRequired[List[RetrieveAgentRequestToolsTypedDict]]
    evaluators: NotRequired[List[RetrieveAgentRequestEvaluatorsTypedDict]]
    r"""Configuration for an evaluator applied to the agent"""
    guardrails: NotRequired[List[RetrieveAgentRequestGuardrailsTypedDict]]
    r"""Configuration for a guardrail applied to the agent"""


class RetrieveAgentRequestSettings(BaseModel):
    max_iterations: Optional[int] = 100
    r"""Maximum iterations(llm calls) before the agent will stop executing."""

    max_execution_time: Optional[int] = 600
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""

    tool_approval_required: Optional[RetrieveAgentRequestToolApprovalRequired] = (
        "respect_tool"
    )
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""

    tools: Optional[List[RetrieveAgentRequestTools]] = None

    evaluators: Optional[List[RetrieveAgentRequestEvaluators]] = None
    r"""Configuration for an evaluator applied to the agent"""

    guardrails: Optional[List[RetrieveAgentRequestGuardrails]] = None
    r"""Configuration for a guardrail applied to the agent"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "max_iterations",
                "max_execution_time",
                "tool_approval_required",
                "tools",
                "evaluators",
                "guardrails",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


RetrieveAgentRequestFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class RetrieveAgentRequestAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: RetrieveAgentRequestVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: RetrieveAgentRequestFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class RetrieveAgentRequestAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: RetrieveAgentRequestVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[RetrieveAgentRequestFormat, pydantic.Field(alias="format")]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


RetrieveAgentRequestResponseFormatAgentsResponseType = Literal["json_schema",]


class RetrieveAgentRequestResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class RetrieveAgentRequestResponseFormatJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = False
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "schema", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RetrieveAgentRequestResponseFormatAgentsJSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: RetrieveAgentRequestResponseFormatAgentsResponseType
    json_schema: RetrieveAgentRequestResponseFormatJSONSchemaTypedDict


class RetrieveAgentRequestResponseFormatAgentsJSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: RetrieveAgentRequestResponseFormatAgentsResponseType

    json_schema: RetrieveAgentRequestResponseFormatJSONSchema


RetrieveAgentRequestResponseFormatAgentsType = Literal["json_object",]


class RetrieveAgentRequestResponseFormatJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: RetrieveAgentRequestResponseFormatAgentsType


class RetrieveAgentRequestResponseFormatJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: RetrieveAgentRequestResponseFormatAgentsType


RetrieveAgentRequestResponseFormatType = Literal["text",]


class RetrieveAgentRequestResponseFormatTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: RetrieveAgentRequestResponseFormatType


class RetrieveAgentRequestResponseFormatText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: RetrieveAgentRequestResponseFormatType


RetrieveAgentRequestResponseFormatTypedDict = TypeAliasType(
    "RetrieveAgentRequestResponseFormatTypedDict",
    Union[
        RetrieveAgentRequestResponseFormatTextTypedDict,
        RetrieveAgentRequestResponseFormatJSONObjectTypedDict,
        RetrieveAgentRequestResponseFormatAgentsJSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


RetrieveAgentRequestResponseFormat = Annotated[
    Union[
        Annotated[RetrieveAgentRequestResponseFormatText, Tag("text")],
        Annotated[RetrieveAgentRequestResponseFormatJSONObject, Tag("json_object")],
        Annotated[
            RetrieveAgentRequestResponseFormatAgentsJSONSchema, Tag("json_schema")
        ],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An object specifying the format that the model must output"""


RetrieveAgentRequestReasoningEffort = Literal[
    "none",
    "minimal",
    "low",
    "medium",
    "high",
    "xhigh",
]
r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

- `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
- All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
- The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
- `xhigh` is currently only supported for `gpt-5.1-codex-max`.

Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
"""


RetrieveAgentRequestStopTypedDict = TypeAliasType(
    "RetrieveAgentRequestStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


RetrieveAgentRequestStop = TypeAliasType(
    "RetrieveAgentRequestStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class RetrieveAgentRequestStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class RetrieveAgentRequestStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestThinkingTypedDict = TypeAliasType(
    "RetrieveAgentRequestThinkingTypedDict",
    Union[ThinkingConfigDisabledSchemaTypedDict, ThinkingConfigEnabledSchemaTypedDict],
)


RetrieveAgentRequestThinking = Annotated[
    Union[
        Annotated[ThinkingConfigDisabledSchema, Tag("disabled")],
        Annotated[ThinkingConfigEnabledSchema, Tag("enabled")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


RetrieveAgentRequestToolChoiceType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class RetrieveAgentRequestToolChoiceFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to call."""


class RetrieveAgentRequestToolChoiceFunction(BaseModel):
    name: str
    r"""The name of the function to call."""


class RetrieveAgentRequestToolChoice2TypedDict(TypedDict):
    function: RetrieveAgentRequestToolChoiceFunctionTypedDict
    type: NotRequired[RetrieveAgentRequestToolChoiceType]
    r"""The type of the tool. Currently, only function is supported."""


class RetrieveAgentRequestToolChoice2(BaseModel):
    function: RetrieveAgentRequestToolChoiceFunction

    type: Optional[RetrieveAgentRequestToolChoiceType] = None
    r"""The type of the tool. Currently, only function is supported."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestToolChoice1 = Literal[
    "none",
    "auto",
    "required",
]


RetrieveAgentRequestToolChoiceTypedDict = TypeAliasType(
    "RetrieveAgentRequestToolChoiceTypedDict",
    Union[RetrieveAgentRequestToolChoice2TypedDict, RetrieveAgentRequestToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


RetrieveAgentRequestToolChoice = TypeAliasType(
    "RetrieveAgentRequestToolChoice",
    Union[RetrieveAgentRequestToolChoice2, RetrieveAgentRequestToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


RetrieveAgentRequestModalities = Literal[
    "text",
    "audio",
]


RetrieveAgentRequestID1 = Literal[
    "orq_pii_detection",
    "orq_sexual_moderation",
    "orq_harmful_moderation",
]
r"""The key of the guardrail."""


RetrieveAgentRequestIDTypedDict = TypeAliasType(
    "RetrieveAgentRequestIDTypedDict", Union[RetrieveAgentRequestID1, str]
)


RetrieveAgentRequestID = TypeAliasType(
    "RetrieveAgentRequestID", Union[RetrieveAgentRequestID1, str]
)


RetrieveAgentRequestAgentsResponseExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RetrieveAgentRequestAgentsGuardrailsTypedDict(TypedDict):
    id: RetrieveAgentRequestIDTypedDict
    execute_on: RetrieveAgentRequestAgentsResponseExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RetrieveAgentRequestAgentsGuardrails(BaseModel):
    id: RetrieveAgentRequestID

    execute_on: RetrieveAgentRequestAgentsResponseExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RetrieveAgentRequestFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class RetrieveAgentRequestFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


class RetrieveAgentRequestAgentsRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class RetrieveAgentRequestAgentsRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestType = Literal["exact_match",]


class RetrieveAgentRequestCacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: RetrieveAgentRequestType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class RetrieveAgentRequestCache(BaseModel):
    r"""Cache configuration for the request."""

    type: RetrieveAgentRequestType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestLoadBalancerType = Literal["weight_based",]


class RetrieveAgentRequestLoadBalancerModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class RetrieveAgentRequestLoadBalancerModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RetrieveAgentRequestLoadBalancer1TypedDict(TypedDict):
    type: RetrieveAgentRequestLoadBalancerType
    models: List[RetrieveAgentRequestLoadBalancerModelsTypedDict]


class RetrieveAgentRequestLoadBalancer1(BaseModel):
    type: RetrieveAgentRequestLoadBalancerType

    models: List[RetrieveAgentRequestLoadBalancerModels]


RetrieveAgentRequestLoadBalancerTypedDict = RetrieveAgentRequestLoadBalancer1TypedDict
r"""Load balancer configuration for the request."""


RetrieveAgentRequestLoadBalancer = RetrieveAgentRequestLoadBalancer1
r"""Load balancer configuration for the request."""


class RetrieveAgentRequestTimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class RetrieveAgentRequestTimeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class RetrieveAgentRequestParametersTypedDict(TypedDict):
    r"""Model behavior parameters (snake_case) stored as part of the agent configuration. These become the default parameters used when the agent is executed. Commonly used: temperature (0-1, controls randomness), max_completion_tokens (response length), top_p (nucleus sampling). Advanced: frequency_penalty, presence_penalty, response_format (JSON/structured output), reasoning_effort (for o1/thinking models), seed (reproducibility), stop sequences. Model-specific support varies. Runtime parameters in agent execution requests can override these defaults."""

    audio: NotRequired[Nullable[RetrieveAgentRequestAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[RetrieveAgentRequestResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[RetrieveAgentRequestReasoningEffort]
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[RetrieveAgentRequestStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[Nullable[RetrieveAgentRequestStreamOptionsTypedDict]]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[RetrieveAgentRequestThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[RetrieveAgentRequestToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[Nullable[List[RetrieveAgentRequestModalities]]]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    guardrails: NotRequired[List[RetrieveAgentRequestAgentsGuardrailsTypedDict]]
    r"""A list of guardrails to apply to the request."""
    fallbacks: NotRequired[List[RetrieveAgentRequestFallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    retry: NotRequired[RetrieveAgentRequestAgentsRetryTypedDict]
    r"""Retry configuration for the request"""
    cache: NotRequired[RetrieveAgentRequestCacheTypedDict]
    r"""Cache configuration for the request."""
    load_balancer: NotRequired[RetrieveAgentRequestLoadBalancerTypedDict]
    r"""Load balancer configuration for the request."""
    timeout: NotRequired[RetrieveAgentRequestTimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


class RetrieveAgentRequestParameters(BaseModel):
    r"""Model behavior parameters (snake_case) stored as part of the agent configuration. These become the default parameters used when the agent is executed. Commonly used: temperature (0-1, controls randomness), max_completion_tokens (response length), top_p (nucleus sampling). Advanced: frequency_penalty, presence_penalty, response_format (JSON/structured output), reasoning_effort (for o1/thinking models), seed (reproducibility), stop sequences. Model-specific support varies. Runtime parameters in agent execution requests can override these defaults."""

    audio: OptionalNullable[RetrieveAgentRequestAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[RetrieveAgentRequestResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[RetrieveAgentRequestReasoningEffort] = None
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[RetrieveAgentRequestStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[RetrieveAgentRequestStreamOptions] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[RetrieveAgentRequestThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[RetrieveAgentRequestToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[RetrieveAgentRequestModalities]] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    guardrails: Optional[List[RetrieveAgentRequestAgentsGuardrails]] = None
    r"""A list of guardrails to apply to the request."""

    fallbacks: Optional[List[RetrieveAgentRequestFallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    retry: Optional[RetrieveAgentRequestAgentsRetry] = None
    r"""Retry configuration for the request"""

    cache: Optional[RetrieveAgentRequestCache] = None
    r"""Cache configuration for the request."""

    load_balancer: Optional[RetrieveAgentRequestLoadBalancer] = None
    r"""Load balancer configuration for the request."""

    timeout: Optional[RetrieveAgentRequestTimeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "response_format",
                "reasoning_effort",
                "verbosity",
                "seed",
                "stop",
                "stream_options",
                "thinking",
                "temperature",
                "top_p",
                "top_k",
                "tool_choice",
                "parallel_tool_calls",
                "modalities",
                "guardrails",
                "fallbacks",
                "retry",
                "cache",
                "load_balancer",
                "timeout",
            ]
        )
        nullable_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "seed",
                "stop",
                "stream_options",
                "temperature",
                "top_p",
                "top_k",
                "modalities",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class RetrieveAgentRequestRetryTypedDict(TypedDict):
    r"""Retry configuration for model requests. Allows customizing retry count (1-5) and HTTP status codes that trigger retries. Default codes: [429]. Common codes: 500 (internal error), 429 (rate limit), 502/503/504 (gateway errors)."""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class RetrieveAgentRequestRetry(BaseModel):
    r"""Retry configuration for model requests. Allows customizing retry count (1-5) and HTTP status codes that trigger retries. Default codes: [429]. Common codes: 500 (internal error), 429 (rate limit), 502/503/504 (gateway errors)."""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestFallbackModelConfigurationVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


RetrieveAgentRequestFallbackModelConfigurationFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class RetrieveAgentRequestFallbackModelConfigurationAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: RetrieveAgentRequestFallbackModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: RetrieveAgentRequestFallbackModelConfigurationFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class RetrieveAgentRequestFallbackModelConfigurationAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: RetrieveAgentRequestFallbackModelConfigurationVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[
        RetrieveAgentRequestFallbackModelConfigurationFormat,
        pydantic.Field(alias="format"),
    ]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


RetrieveAgentRequestResponseFormatAgentsResponse200ApplicationJSONResponseBodyType = (
    Literal["json_schema",]
)


class RetrieveAgentRequestResponseFormatAgentsResponseJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class RetrieveAgentRequestResponseFormatAgentsResponseJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = False
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "schema", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RetrieveAgentRequestResponseFormatAgentsResponse200JSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: RetrieveAgentRequestResponseFormatAgentsResponse200ApplicationJSONResponseBodyType
    json_schema: RetrieveAgentRequestResponseFormatAgentsResponseJSONSchemaTypedDict


class RetrieveAgentRequestResponseFormatAgentsResponse200JSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: RetrieveAgentRequestResponseFormatAgentsResponse200ApplicationJSONResponseBodyType

    json_schema: RetrieveAgentRequestResponseFormatAgentsResponseJSONSchema


RetrieveAgentRequestResponseFormatAgentsResponse200ApplicationJSONType = Literal[
    "json_object",
]


class RetrieveAgentRequestResponseFormatAgentsJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: RetrieveAgentRequestResponseFormatAgentsResponse200ApplicationJSONType


class RetrieveAgentRequestResponseFormatAgentsJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: RetrieveAgentRequestResponseFormatAgentsResponse200ApplicationJSONType


RetrieveAgentRequestResponseFormatAgentsResponse200Type = Literal["text",]


class RetrieveAgentRequestResponseFormatAgentsTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: RetrieveAgentRequestResponseFormatAgentsResponse200Type


class RetrieveAgentRequestResponseFormatAgentsText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: RetrieveAgentRequestResponseFormatAgentsResponse200Type


RetrieveAgentRequestFallbackModelConfigurationResponseFormatTypedDict = TypeAliasType(
    "RetrieveAgentRequestFallbackModelConfigurationResponseFormatTypedDict",
    Union[
        RetrieveAgentRequestResponseFormatAgentsTextTypedDict,
        RetrieveAgentRequestResponseFormatAgentsJSONObjectTypedDict,
        RetrieveAgentRequestResponseFormatAgentsResponse200JSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


RetrieveAgentRequestFallbackModelConfigurationResponseFormat = Annotated[
    Union[
        Annotated[RetrieveAgentRequestResponseFormatAgentsText, Tag("text")],
        Annotated[
            RetrieveAgentRequestResponseFormatAgentsJSONObject, Tag("json_object")
        ],
        Annotated[
            RetrieveAgentRequestResponseFormatAgentsResponse200JSONSchema,
            Tag("json_schema"),
        ],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An object specifying the format that the model must output"""


RetrieveAgentRequestFallbackModelConfigurationReasoningEffort = Literal[
    "none",
    "minimal",
    "low",
    "medium",
    "high",
    "xhigh",
]
r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

- `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
- All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
- The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
- `xhigh` is currently only supported for `gpt-5.1-codex-max`.

Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
"""


RetrieveAgentRequestFallbackModelConfigurationStopTypedDict = TypeAliasType(
    "RetrieveAgentRequestFallbackModelConfigurationStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


RetrieveAgentRequestFallbackModelConfigurationStop = TypeAliasType(
    "RetrieveAgentRequestFallbackModelConfigurationStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class RetrieveAgentRequestFallbackModelConfigurationStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class RetrieveAgentRequestFallbackModelConfigurationStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestFallbackModelConfigurationThinkingTypedDict = TypeAliasType(
    "RetrieveAgentRequestFallbackModelConfigurationThinkingTypedDict",
    Union[ThinkingConfigDisabledSchemaTypedDict, ThinkingConfigEnabledSchemaTypedDict],
)


RetrieveAgentRequestFallbackModelConfigurationThinking = Annotated[
    Union[
        Annotated[ThinkingConfigDisabledSchema, Tag("disabled")],
        Annotated[ThinkingConfigEnabledSchema, Tag("enabled")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


RetrieveAgentRequestToolChoiceAgentsType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class RetrieveAgentRequestToolChoiceAgentsFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to call."""


class RetrieveAgentRequestToolChoiceAgentsFunction(BaseModel):
    name: str
    r"""The name of the function to call."""


class RetrieveAgentRequestToolChoiceAgents2TypedDict(TypedDict):
    function: RetrieveAgentRequestToolChoiceAgentsFunctionTypedDict
    type: NotRequired[RetrieveAgentRequestToolChoiceAgentsType]
    r"""The type of the tool. Currently, only function is supported."""


class RetrieveAgentRequestToolChoiceAgents2(BaseModel):
    function: RetrieveAgentRequestToolChoiceAgentsFunction

    type: Optional[RetrieveAgentRequestToolChoiceAgentsType] = None
    r"""The type of the tool. Currently, only function is supported."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestToolChoiceAgents1 = Literal[
    "none",
    "auto",
    "required",
]


RetrieveAgentRequestFallbackModelConfigurationToolChoiceTypedDict = TypeAliasType(
    "RetrieveAgentRequestFallbackModelConfigurationToolChoiceTypedDict",
    Union[
        RetrieveAgentRequestToolChoiceAgents2TypedDict,
        RetrieveAgentRequestToolChoiceAgents1,
    ],
)
r"""Controls which (if any) tool is called by the model."""


RetrieveAgentRequestFallbackModelConfigurationToolChoice = TypeAliasType(
    "RetrieveAgentRequestFallbackModelConfigurationToolChoice",
    Union[RetrieveAgentRequestToolChoiceAgents2, RetrieveAgentRequestToolChoiceAgents1],
)
r"""Controls which (if any) tool is called by the model."""


RetrieveAgentRequestFallbackModelConfigurationModalities = Literal[
    "text",
    "audio",
]


RetrieveAgentRequestIDAgents1 = Literal[
    "orq_pii_detection",
    "orq_sexual_moderation",
    "orq_harmful_moderation",
]
r"""The key of the guardrail."""


RetrieveAgentRequestFallbackModelConfigurationIDTypedDict = TypeAliasType(
    "RetrieveAgentRequestFallbackModelConfigurationIDTypedDict",
    Union[RetrieveAgentRequestIDAgents1, str],
)


RetrieveAgentRequestFallbackModelConfigurationID = TypeAliasType(
    "RetrieveAgentRequestFallbackModelConfigurationID",
    Union[RetrieveAgentRequestIDAgents1, str],
)


RetrieveAgentRequestFallbackModelConfigurationExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RetrieveAgentRequestFallbackModelConfigurationGuardrailsTypedDict(TypedDict):
    id: RetrieveAgentRequestFallbackModelConfigurationIDTypedDict
    execute_on: RetrieveAgentRequestFallbackModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RetrieveAgentRequestFallbackModelConfigurationGuardrails(BaseModel):
    id: RetrieveAgentRequestFallbackModelConfigurationID

    execute_on: RetrieveAgentRequestFallbackModelConfigurationExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class RetrieveAgentRequestFallbackModelConfigurationFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class RetrieveAgentRequestFallbackModelConfigurationFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


class RetrieveAgentRequestFallbackModelConfigurationAgentsRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class RetrieveAgentRequestFallbackModelConfigurationAgentsRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestFallbackModelConfigurationType = Literal["exact_match",]


class RetrieveAgentRequestFallbackModelConfigurationCacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: RetrieveAgentRequestFallbackModelConfigurationType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class RetrieveAgentRequestFallbackModelConfigurationCache(BaseModel):
    r"""Cache configuration for the request."""

    type: RetrieveAgentRequestFallbackModelConfigurationType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestLoadBalancerAgentsType = Literal["weight_based",]


class RetrieveAgentRequestLoadBalancerAgentsModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class RetrieveAgentRequestLoadBalancerAgentsModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RetrieveAgentRequestLoadBalancerAgents1TypedDict(TypedDict):
    type: RetrieveAgentRequestLoadBalancerAgentsType
    models: List[RetrieveAgentRequestLoadBalancerAgentsModelsTypedDict]


class RetrieveAgentRequestLoadBalancerAgents1(BaseModel):
    type: RetrieveAgentRequestLoadBalancerAgentsType

    models: List[RetrieveAgentRequestLoadBalancerAgentsModels]


RetrieveAgentRequestFallbackModelConfigurationLoadBalancerTypedDict = (
    RetrieveAgentRequestLoadBalancerAgents1TypedDict
)
r"""Load balancer configuration for the request."""


RetrieveAgentRequestFallbackModelConfigurationLoadBalancer = (
    RetrieveAgentRequestLoadBalancerAgents1
)
r"""Load balancer configuration for the request."""


class RetrieveAgentRequestFallbackModelConfigurationTimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class RetrieveAgentRequestFallbackModelConfigurationTimeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class RetrieveAgentRequestFallbackModelConfigurationParametersTypedDict(TypedDict):
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    audio: NotRequired[
        Nullable[RetrieveAgentRequestFallbackModelConfigurationAudioTypedDict]
    ]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[
        RetrieveAgentRequestFallbackModelConfigurationResponseFormatTypedDict
    ]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[
        RetrieveAgentRequestFallbackModelConfigurationReasoningEffort
    ]
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[
        Nullable[RetrieveAgentRequestFallbackModelConfigurationStopTypedDict]
    ]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[
        Nullable[RetrieveAgentRequestFallbackModelConfigurationStreamOptionsTypedDict]
    ]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[
        RetrieveAgentRequestFallbackModelConfigurationThinkingTypedDict
    ]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[
        RetrieveAgentRequestFallbackModelConfigurationToolChoiceTypedDict
    ]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[
        Nullable[List[RetrieveAgentRequestFallbackModelConfigurationModalities]]
    ]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    guardrails: NotRequired[
        List[RetrieveAgentRequestFallbackModelConfigurationGuardrailsTypedDict]
    ]
    r"""A list of guardrails to apply to the request."""
    fallbacks: NotRequired[
        List[RetrieveAgentRequestFallbackModelConfigurationFallbacksTypedDict]
    ]
    r"""Array of fallback models to use if primary model fails"""
    retry: NotRequired[
        RetrieveAgentRequestFallbackModelConfigurationAgentsRetryTypedDict
    ]
    r"""Retry configuration for the request"""
    cache: NotRequired[RetrieveAgentRequestFallbackModelConfigurationCacheTypedDict]
    r"""Cache configuration for the request."""
    load_balancer: NotRequired[
        RetrieveAgentRequestFallbackModelConfigurationLoadBalancerTypedDict
    ]
    r"""Load balancer configuration for the request."""
    timeout: NotRequired[RetrieveAgentRequestFallbackModelConfigurationTimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


class RetrieveAgentRequestFallbackModelConfigurationParameters(BaseModel):
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    audio: OptionalNullable[RetrieveAgentRequestFallbackModelConfigurationAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[
        RetrieveAgentRequestFallbackModelConfigurationResponseFormat
    ] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[
        RetrieveAgentRequestFallbackModelConfigurationReasoningEffort
    ] = None
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[RetrieveAgentRequestFallbackModelConfigurationStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[
        RetrieveAgentRequestFallbackModelConfigurationStreamOptions
    ] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[RetrieveAgentRequestFallbackModelConfigurationThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[RetrieveAgentRequestFallbackModelConfigurationToolChoice] = (
        None
    )
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[
        List[RetrieveAgentRequestFallbackModelConfigurationModalities]
    ] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    guardrails: Optional[
        List[RetrieveAgentRequestFallbackModelConfigurationGuardrails]
    ] = None
    r"""A list of guardrails to apply to the request."""

    fallbacks: Optional[
        List[RetrieveAgentRequestFallbackModelConfigurationFallbacks]
    ] = None
    r"""Array of fallback models to use if primary model fails"""

    retry: Optional[RetrieveAgentRequestFallbackModelConfigurationAgentsRetry] = None
    r"""Retry configuration for the request"""

    cache: Optional[RetrieveAgentRequestFallbackModelConfigurationCache] = None
    r"""Cache configuration for the request."""

    load_balancer: Optional[
        RetrieveAgentRequestFallbackModelConfigurationLoadBalancer
    ] = None
    r"""Load balancer configuration for the request."""

    timeout: Optional[RetrieveAgentRequestFallbackModelConfigurationTimeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "response_format",
                "reasoning_effort",
                "verbosity",
                "seed",
                "stop",
                "stream_options",
                "thinking",
                "temperature",
                "top_p",
                "top_k",
                "tool_choice",
                "parallel_tool_calls",
                "modalities",
                "guardrails",
                "fallbacks",
                "retry",
                "cache",
                "load_balancer",
                "timeout",
            ]
        )
        nullable_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "seed",
                "stop",
                "stream_options",
                "temperature",
                "top_p",
                "top_k",
                "modalities",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class RetrieveAgentRequestFallbackModelConfigurationRetryTypedDict(TypedDict):
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class RetrieveAgentRequestFallbackModelConfigurationRetry(BaseModel):
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RetrieveAgentRequestFallbackModelConfiguration2TypedDict(TypedDict):
    r"""Fallback model configuration with optional parameters and retry settings."""

    id: str
    r"""A fallback model ID string. Must support tool calling."""
    parameters: NotRequired[
        RetrieveAgentRequestFallbackModelConfigurationParametersTypedDict
    ]
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""
    retry: NotRequired[RetrieveAgentRequestFallbackModelConfigurationRetryTypedDict]
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""


class RetrieveAgentRequestFallbackModelConfiguration2(BaseModel):
    r"""Fallback model configuration with optional parameters and retry settings."""

    id: str
    r"""A fallback model ID string. Must support tool calling."""

    parameters: Optional[RetrieveAgentRequestFallbackModelConfigurationParameters] = (
        None
    )
    r"""Optional model parameters specific to this fallback model. Overrides primary model parameters if this fallback is used."""

    retry: Optional[RetrieveAgentRequestFallbackModelConfigurationRetry] = None
    r"""Retry configuration for this fallback model. Allows customizing retry count (1-5) and HTTP status codes that trigger retries."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["parameters", "retry"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


RetrieveAgentRequestFallbackModelConfigurationTypedDict = TypeAliasType(
    "RetrieveAgentRequestFallbackModelConfigurationTypedDict",
    Union[RetrieveAgentRequestFallbackModelConfiguration2TypedDict, str],
)
r"""Fallback model for automatic failover when primary model request fails. Supports optional parameter overrides. Can be a simple model ID string or a configuration object with model-specific parameters. Fallbacks are tried in order."""


RetrieveAgentRequestFallbackModelConfiguration = TypeAliasType(
    "RetrieveAgentRequestFallbackModelConfiguration",
    Union[RetrieveAgentRequestFallbackModelConfiguration2, str],
)
r"""Fallback model for automatic failover when primary model request fails. Supports optional parameter overrides. Can be a simple model ID string or a configuration object with model-specific parameters. Fallbacks are tried in order."""


class RetrieveAgentRequestModelTypedDict(TypedDict):
    id: str
    r"""The database ID of the primary model"""
    integration_id: NotRequired[Nullable[str]]
    r"""Optional integration ID for custom model configurations"""
    parameters: NotRequired[RetrieveAgentRequestParametersTypedDict]
    r"""Model behavior parameters (snake_case) stored as part of the agent configuration. These become the default parameters used when the agent is executed. Commonly used: temperature (0-1, controls randomness), max_completion_tokens (response length), top_p (nucleus sampling). Advanced: frequency_penalty, presence_penalty, response_format (JSON/structured output), reasoning_effort (for o1/thinking models), seed (reproducibility), stop sequences. Model-specific support varies. Runtime parameters in agent execution requests can override these defaults."""
    retry: NotRequired[RetrieveAgentRequestRetryTypedDict]
    r"""Retry configuration for model requests. Allows customizing retry count (1-5) and HTTP status codes that trigger retries. Default codes: [429]. Common codes: 500 (internal error), 429 (rate limit), 502/503/504 (gateway errors)."""
    fallback_models: NotRequired[
        Nullable[List[RetrieveAgentRequestFallbackModelConfigurationTypedDict]]
    ]
    r"""Optional array of fallback models (string IDs or config objects) that will be used automatically in order if the primary model fails"""


class RetrieveAgentRequestModel(BaseModel):
    id: str
    r"""The database ID of the primary model"""

    integration_id: OptionalNullable[str] = UNSET
    r"""Optional integration ID for custom model configurations"""

    parameters: Optional[RetrieveAgentRequestParameters] = None
    r"""Model behavior parameters (snake_case) stored as part of the agent configuration. These become the default parameters used when the agent is executed. Commonly used: temperature (0-1, controls randomness), max_completion_tokens (response length), top_p (nucleus sampling). Advanced: frequency_penalty, presence_penalty, response_format (JSON/structured output), reasoning_effort (for o1/thinking models), seed (reproducibility), stop sequences. Model-specific support varies. Runtime parameters in agent execution requests can override these defaults."""

    retry: Optional[RetrieveAgentRequestRetry] = None
    r"""Retry configuration for model requests. Allows customizing retry count (1-5) and HTTP status codes that trigger retries. Default codes: [429]. Common codes: 500 (internal error), 429 (rate limit), 502/503/504 (gateway errors)."""

    fallback_models: OptionalNullable[
        List[RetrieveAgentRequestFallbackModelConfiguration]
    ] = UNSET
    r"""Optional array of fallback models (string IDs or config objects) that will be used automatically in order if the primary model fails"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["integration_id", "parameters", "retry", "fallback_models"]
        )
        nullable_fields = set(["integration_id", "fallback_models"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class RetrieveAgentRequestTeamOfAgentsTypedDict(TypedDict):
    key: str
    r"""The unique key of the agent within the workspace"""
    role: NotRequired[str]
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""


class RetrieveAgentRequestTeamOfAgents(BaseModel):
    key: str
    r"""The unique key of the agent within the workspace"""

    role: Optional[str] = None
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["role"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RetrieveAgentRequestMetricsTypedDict(TypedDict):
    total_cost: NotRequired[float]


class RetrieveAgentRequestMetrics(BaseModel):
    total_cost: Optional[float] = 0

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["total_cost"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RetrieveAgentRequestKnowledgeBasesTypedDict(TypedDict):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


class RetrieveAgentRequestKnowledgeBases(BaseModel):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


RetrieveAgentRequestSource = Literal[
    "internal",
    "external",
    "experiment",
]


class RetrieveAgentRequestResponseBodyTypedDict(TypedDict):
    r"""Agent successfully retrieved. Returns the complete agent manifest with all configuration details, including models, tools, knowledge bases, and execution settings."""

    id: str
    key: str
    r"""Unique identifier for the agent within the workspace"""
    display_name: str
    workspace_id: str
    project_id: str
    role: str
    description: str
    instructions: str
    status: RetrieveAgentRequestStatus
    r"""The status of the agent. `Live` is the latest version of the agent. `Draft` is a version that is not yet published. `Pending` is a version that is pending approval. `Published` is a version that was live and has been replaced by a new version."""
    model: RetrieveAgentRequestModelTypedDict
    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """
    memory_stores: List[str]
    r"""Array of memory store identifiers. Accepts both memory store IDs and keys."""
    team_of_agents: List[RetrieveAgentRequestTeamOfAgentsTypedDict]
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""
    created_by_id: NotRequired[Nullable[str]]
    updated_by_id: NotRequired[Nullable[str]]
    created: NotRequired[str]
    updated: NotRequired[str]
    system_prompt: NotRequired[str]
    settings: NotRequired[RetrieveAgentRequestSettingsTypedDict]
    version_hash: NotRequired[str]
    metrics: NotRequired[RetrieveAgentRequestMetricsTypedDict]
    variables: NotRequired[Dict[str, Any]]
    r"""Extracted variables from agent instructions"""
    knowledge_bases: NotRequired[List[RetrieveAgentRequestKnowledgeBasesTypedDict]]
    r"""Agent knowledge bases reference"""
    source: NotRequired[RetrieveAgentRequestSource]


class RetrieveAgentRequestResponseBody(BaseModel):
    r"""Agent successfully retrieved. Returns the complete agent manifest with all configuration details, including models, tools, knowledge bases, and execution settings."""

    id: Annotated[str, pydantic.Field(alias="_id")]

    key: str
    r"""Unique identifier for the agent within the workspace"""

    display_name: str

    workspace_id: str

    project_id: str

    role: str

    description: str

    instructions: str

    status: RetrieveAgentRequestStatus
    r"""The status of the agent. `Live` is the latest version of the agent. `Draft` is a version that is not yet published. `Pending` is a version that is pending approval. `Published` is a version that was live and has been replaced by a new version."""

    model: RetrieveAgentRequestModel

    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """

    memory_stores: List[str]
    r"""Array of memory store identifiers. Accepts both memory store IDs and keys."""

    team_of_agents: List[RetrieveAgentRequestTeamOfAgents]
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""

    created_by_id: OptionalNullable[str] = UNSET

    updated_by_id: OptionalNullable[str] = UNSET

    created: Optional[str] = None

    updated: Optional[str] = None

    system_prompt: Optional[str] = None

    settings: Optional[RetrieveAgentRequestSettings] = None

    version_hash: Optional[str] = None

    metrics: Optional[RetrieveAgentRequestMetrics] = None

    variables: Optional[Dict[str, Any]] = None
    r"""Extracted variables from agent instructions"""

    knowledge_bases: Optional[List[RetrieveAgentRequestKnowledgeBases]] = None
    r"""Agent knowledge bases reference"""

    source: Optional[RetrieveAgentRequestSource] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "created_by_id",
                "updated_by_id",
                "created",
                "updated",
                "system_prompt",
                "settings",
                "version_hash",
                "metrics",
                "variables",
                "knowledge_bases",
                "source",
            ]
        )
        nullable_fields = set(["created_by_id", "updated_by_id"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m
