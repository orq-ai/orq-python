"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .publiccontact import PublicContact, PublicContactTypedDict
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import eventstreaming
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


CreateChatCompletionMessagesProxyRequestRequestBody5Role = Literal["tool"]
r"""The role of the messages author, in this case tool."""

CreateChatCompletionMessagesProxyRequestContentTypedDict = TypeAliasType(
    "CreateChatCompletionMessagesProxyRequestContentTypedDict", Union[str, List[str]]
)
r"""The contents of the tool message."""


CreateChatCompletionMessagesProxyRequestContent = TypeAliasType(
    "CreateChatCompletionMessagesProxyRequestContent", Union[str, List[str]]
)
r"""The contents of the tool message."""


class CreateChatCompletionMessagesToolMessageTypedDict(TypedDict):
    role: CreateChatCompletionMessagesProxyRequestRequestBody5Role
    r"""The role of the messages author, in this case tool."""
    content: CreateChatCompletionMessagesProxyRequestContentTypedDict
    r"""The contents of the tool message."""
    tool_call_id: str
    r"""Tool call that this message is responding to."""


class CreateChatCompletionMessagesToolMessage(BaseModel):
    role: CreateChatCompletionMessagesProxyRequestRequestBody5Role
    r"""The role of the messages author, in this case tool."""

    content: CreateChatCompletionMessagesProxyRequestContent
    r"""The contents of the tool message."""

    tool_call_id: str
    r"""Tool call that this message is responding to."""


CreateChatCompletion2ProxyRequestRequestBodyMessages4Type = Literal["refusal"]
r"""The type of the content part."""


class CreateChatCompletion2RefusalContentPartTypedDict(TypedDict):
    type: CreateChatCompletion2ProxyRequestRequestBodyMessages4Type
    r"""The type of the content part."""
    refusal: str
    r"""The refusal message generated by the model."""


class CreateChatCompletion2RefusalContentPart(BaseModel):
    type: CreateChatCompletion2ProxyRequestRequestBodyMessages4Type
    r"""The type of the content part."""

    refusal: str
    r"""The refusal message generated by the model."""


CreateChatCompletion2ProxyRequestRequestBodyMessagesType = Literal["text"]
r"""The type of the content part."""

CreateChatCompletionAnnotationsProxyType = Literal["file_path"]


class CreateChatCompletionAnnotationsFilePathTypedDict(TypedDict):
    file_id: str


class CreateChatCompletionAnnotationsFilePath(BaseModel):
    file_id: str


class CreateChatCompletionAnnotations2TypedDict(TypedDict):
    type: CreateChatCompletionAnnotationsProxyType
    text: str
    file_path: CreateChatCompletionAnnotationsFilePathTypedDict
    start_index: int
    end_index: int


class CreateChatCompletionAnnotations2(BaseModel):
    type: CreateChatCompletionAnnotationsProxyType

    text: str

    file_path: CreateChatCompletionAnnotationsFilePath

    start_index: int

    end_index: int


CreateChatCompletionAnnotationsType = Literal["file_citation"]


class CreateChatCompletionAnnotationsFileCitationTypedDict(TypedDict):
    file_id: str
    quote: NotRequired[str]


class CreateChatCompletionAnnotationsFileCitation(BaseModel):
    file_id: str

    quote: Optional[str] = None


class CreateChatCompletionAnnotations1TypedDict(TypedDict):
    type: CreateChatCompletionAnnotationsType
    text: str
    file_citation: CreateChatCompletionAnnotationsFileCitationTypedDict
    start_index: int
    end_index: int


class CreateChatCompletionAnnotations1(BaseModel):
    type: CreateChatCompletionAnnotationsType

    text: str

    file_citation: CreateChatCompletionAnnotationsFileCitation

    start_index: int

    end_index: int


CreateChatCompletion2AnnotationsTypedDict = TypeAliasType(
    "CreateChatCompletion2AnnotationsTypedDict",
    Union[
        CreateChatCompletionAnnotations1TypedDict,
        CreateChatCompletionAnnotations2TypedDict,
    ],
)


CreateChatCompletion2Annotations = TypeAliasType(
    "CreateChatCompletion2Annotations",
    Union[CreateChatCompletionAnnotations1, CreateChatCompletionAnnotations2],
)


class CreateChatCompletion2TextContentPartTypedDict(TypedDict):
    type: CreateChatCompletion2ProxyRequestRequestBodyMessagesType
    r"""The type of the content part."""
    text: str
    r"""The text content."""
    annotations: NotRequired[List[CreateChatCompletion2AnnotationsTypedDict]]
    r"""Annotations for the text content."""


class CreateChatCompletion2TextContentPart(BaseModel):
    type: CreateChatCompletion2ProxyRequestRequestBodyMessagesType
    r"""The type of the content part."""

    text: str
    r"""The text content."""

    annotations: Optional[List[CreateChatCompletion2Annotations]] = None
    r"""Annotations for the text content."""


CreateChatCompletionContentProxy2TypedDict = TypeAliasType(
    "CreateChatCompletionContentProxy2TypedDict",
    Union[
        CreateChatCompletion2RefusalContentPartTypedDict,
        CreateChatCompletion2TextContentPartTypedDict,
    ],
)


CreateChatCompletionContentProxy2 = TypeAliasType(
    "CreateChatCompletionContentProxy2",
    Union[
        CreateChatCompletion2RefusalContentPart, CreateChatCompletion2TextContentPart
    ],
)


CreateChatCompletionMessagesProxyContentTypedDict = TypeAliasType(
    "CreateChatCompletionMessagesProxyContentTypedDict",
    Union[str, List[CreateChatCompletionContentProxy2TypedDict]],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


CreateChatCompletionMessagesProxyContent = TypeAliasType(
    "CreateChatCompletionMessagesProxyContent",
    Union[str, List[CreateChatCompletionContentProxy2]],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


CreateChatCompletionMessagesProxyRequestRequestBodyRole = Literal["assistant"]
r"""The role of the messages author, in this case `assistant`."""


class CreateChatCompletionMessagesAudioTypedDict(TypedDict):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


class CreateChatCompletionMessagesAudio(BaseModel):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


CreateChatCompletionMessagesType = Literal["function"]
r"""The type of the tool. Currently, only `function` is supported."""


class CreateChatCompletionMessagesFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreateChatCompletionMessagesFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreateChatCompletionMessagesToolCallsTypedDict(TypedDict):
    id: str
    r"""The ID of the tool call."""
    type: CreateChatCompletionMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""
    function: CreateChatCompletionMessagesFunctionTypedDict


class CreateChatCompletionMessagesToolCalls(BaseModel):
    id: str
    r"""The ID of the tool call."""

    type: CreateChatCompletionMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""

    function: CreateChatCompletionMessagesFunction


class CreateChatCompletionMessagesAssistantMessageTypedDict(TypedDict):
    role: CreateChatCompletionMessagesProxyRequestRequestBodyRole
    r"""The role of the messages author, in this case `assistant`."""
    content: NotRequired[Nullable[CreateChatCompletionMessagesProxyContentTypedDict]]
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""
    refusal: NotRequired[Nullable[str]]
    r"""The refusal message by the assistant."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""
    audio: NotRequired[Nullable[CreateChatCompletionMessagesAudioTypedDict]]
    r"""Data about a previous audio response from the model."""
    tool_calls: NotRequired[List[CreateChatCompletionMessagesToolCallsTypedDict]]
    r"""The tool calls generated by the model, such as function calls."""
    reasoning: NotRequired[str]
    r"""Internal thought process of the model"""
    reasoning_signature: NotRequired[str]
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""
    redacted_reasoning: NotRequired[str]
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""


class CreateChatCompletionMessagesAssistantMessage(BaseModel):
    role: CreateChatCompletionMessagesProxyRequestRequestBodyRole
    r"""The role of the messages author, in this case `assistant`."""

    content: OptionalNullable[CreateChatCompletionMessagesProxyContent] = UNSET
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""

    refusal: OptionalNullable[str] = UNSET
    r"""The refusal message by the assistant."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    audio: OptionalNullable[CreateChatCompletionMessagesAudio] = UNSET
    r"""Data about a previous audio response from the model."""

    tool_calls: Optional[List[CreateChatCompletionMessagesToolCalls]] = None
    r"""The tool calls generated by the model, such as function calls."""

    reasoning: Optional[str] = None
    r"""Internal thought process of the model"""

    reasoning_signature: Optional[str] = None
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""

    redacted_reasoning: Optional[str] = None
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "content",
            "refusal",
            "name",
            "audio",
            "tool_calls",
            "reasoning",
            "reasoning_signature",
            "redacted_reasoning",
        ]
        nullable_fields = ["content", "refusal", "audio"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateChatCompletionMessagesProxyRequestRole = Literal["user"]
r"""The role of the messages author, in this case `user`."""

CreateChatCompletion2ProxyRequestRequestBodyType = Literal["file"]
r"""The type of the content part. Always `file`."""


class CreateChatCompletion2FileTypedDict(TypedDict):
    file_data: str
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""
    filename: str
    r"""The name of the file, used when passing the file to the model as a string."""


class CreateChatCompletion2File(BaseModel):
    file_data: str
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""

    filename: str
    r"""The name of the file, used when passing the file to the model as a string."""


class CreateChatCompletion24TypedDict(TypedDict):
    type: CreateChatCompletion2ProxyRequestRequestBodyType
    r"""The type of the content part. Always `file`."""
    file: CreateChatCompletion2FileTypedDict


class CreateChatCompletion24(BaseModel):
    type: CreateChatCompletion2ProxyRequestRequestBodyType
    r"""The type of the content part. Always `file`."""

    file: CreateChatCompletion2File


CreateChatCompletion2ProxyRequestType = Literal["input_audio"]

CreateChatCompletion2Format = Literal["mp3", "wav"]
r"""The format of the encoded audio data. Currently supports `wav` and `mp3`."""


class CreateChatCompletion2InputAudioTypedDict(TypedDict):
    data: str
    r"""Base64 encoded audio data."""
    format_: CreateChatCompletion2Format
    r"""The format of the encoded audio data. Currently supports `wav` and `mp3`."""


class CreateChatCompletion2InputAudio(BaseModel):
    data: str
    r"""Base64 encoded audio data."""

    format_: Annotated[CreateChatCompletion2Format, pydantic.Field(alias="format")]
    r"""The format of the encoded audio data. Currently supports `wav` and `mp3`."""


class CreateChatCompletion23TypedDict(TypedDict):
    type: CreateChatCompletion2ProxyRequestType
    input_audio: CreateChatCompletion2InputAudioTypedDict


class CreateChatCompletion23(BaseModel):
    type: CreateChatCompletion2ProxyRequestType

    input_audio: CreateChatCompletion2InputAudio


CreateChatCompletion2ProxyType = Literal["image_url"]

CreateChatCompletion2Detail = Literal["low", "high", "auto"]
r"""Specifies the detail level of the image."""


class CreateChatCompletion2ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded image data."""
    detail: NotRequired[CreateChatCompletion2Detail]
    r"""Specifies the detail level of the image."""


class CreateChatCompletion2ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded image data."""

    detail: Optional[CreateChatCompletion2Detail] = None
    r"""Specifies the detail level of the image."""


class CreateChatCompletion22TypedDict(TypedDict):
    type: CreateChatCompletion2ProxyType
    image_url: CreateChatCompletion2ImageURLTypedDict


class CreateChatCompletion22(BaseModel):
    type: CreateChatCompletion2ProxyType

    image_url: CreateChatCompletion2ImageURL


CreateChatCompletion2Type = Literal["text"]


class CreateChatCompletion21TypedDict(TypedDict):
    type: CreateChatCompletion2Type
    text: str


class CreateChatCompletion21(BaseModel):
    type: CreateChatCompletion2Type

    text: str


CreateChatCompletionContent2TypedDict = TypeAliasType(
    "CreateChatCompletionContent2TypedDict",
    Union[
        CreateChatCompletion21TypedDict,
        CreateChatCompletion22TypedDict,
        CreateChatCompletion23TypedDict,
        CreateChatCompletion24TypedDict,
    ],
)


CreateChatCompletionContent2 = TypeAliasType(
    "CreateChatCompletionContent2",
    Union[
        CreateChatCompletion21,
        CreateChatCompletion22,
        CreateChatCompletion23,
        CreateChatCompletion24,
    ],
)


CreateChatCompletionMessagesContentTypedDict = TypeAliasType(
    "CreateChatCompletionMessagesContentTypedDict",
    Union[str, List[CreateChatCompletionContent2TypedDict]],
)
r"""The contents of the user message."""


CreateChatCompletionMessagesContent = TypeAliasType(
    "CreateChatCompletionMessagesContent",
    Union[str, List[CreateChatCompletionContent2]],
)
r"""The contents of the user message."""


class CreateChatCompletionMessagesUserMessageTypedDict(TypedDict):
    role: CreateChatCompletionMessagesProxyRequestRole
    r"""The role of the messages author, in this case `user`."""
    content: CreateChatCompletionMessagesContentTypedDict
    r"""The contents of the user message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class CreateChatCompletionMessagesUserMessage(BaseModel):
    role: CreateChatCompletionMessagesProxyRequestRole
    r"""The role of the messages author, in this case `user`."""

    content: CreateChatCompletionMessagesContent
    r"""The contents of the user message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


CreateChatCompletionMessagesProxyRole = Literal["system"]
r"""The role of the messages author, in this case `system`."""


class CreateChatCompletionMessagesSystemMessageTypedDict(TypedDict):
    role: CreateChatCompletionMessagesProxyRole
    r"""The role of the messages author, in this case `system`."""
    content: str
    r"""The contents of the system message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class CreateChatCompletionMessagesSystemMessage(BaseModel):
    role: CreateChatCompletionMessagesProxyRole
    r"""The role of the messages author, in this case `system`."""

    content: str
    r"""The contents of the system message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


CreateChatCompletionMessagesRole = Literal["developer"]
r"""The role of the messages author, in this case  `developer`."""


class CreateChatCompletionMessagesDeveloperMessageTypedDict(TypedDict):
    role: CreateChatCompletionMessagesRole
    r"""The role of the messages author, in this case  `developer`."""
    content: str
    r"""The contents of the developer message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class CreateChatCompletionMessagesDeveloperMessage(BaseModel):
    role: CreateChatCompletionMessagesRole
    r"""The role of the messages author, in this case  `developer`."""

    content: str
    r"""The contents of the developer message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


CreateChatCompletionMessagesTypedDict = TypeAliasType(
    "CreateChatCompletionMessagesTypedDict",
    Union[
        CreateChatCompletionMessagesDeveloperMessageTypedDict,
        CreateChatCompletionMessagesSystemMessageTypedDict,
        CreateChatCompletionMessagesUserMessageTypedDict,
        CreateChatCompletionMessagesToolMessageTypedDict,
        CreateChatCompletionMessagesAssistantMessageTypedDict,
    ],
)


CreateChatCompletionMessages = TypeAliasType(
    "CreateChatCompletionMessages",
    Union[
        CreateChatCompletionMessagesDeveloperMessage,
        CreateChatCompletionMessagesSystemMessage,
        CreateChatCompletionMessagesUserMessage,
        CreateChatCompletionMessagesToolMessage,
        CreateChatCompletionMessagesAssistantMessage,
    ],
)


Voice = Literal["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

CreateChatCompletionFormat = Literal["wav", "mp3", "flac", "opus", "pcm16"]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class CreateChatCompletionAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: Voice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: CreateChatCompletionFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class CreateChatCompletionAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: Voice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[CreateChatCompletionFormat, pydantic.Field(alias="format")]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


CreateChatCompletionResponseFormatProxyRequestType = Literal["json_schema"]


class CreateChatCompletionResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[Nullable[bool]]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class CreateChatCompletionResponseFormatJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: OptionalNullable[bool] = UNSET
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "schema", "strict"]
        nullable_fields = ["strict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionResponseFormat3TypedDict(TypedDict):
    type: CreateChatCompletionResponseFormatProxyRequestType
    json_schema: CreateChatCompletionResponseFormatJSONSchemaTypedDict


class CreateChatCompletionResponseFormat3(BaseModel):
    type: CreateChatCompletionResponseFormatProxyRequestType

    json_schema: CreateChatCompletionResponseFormatJSONSchema


CreateChatCompletionResponseFormatProxyType = Literal["json_object"]


class CreateChatCompletionResponseFormat2TypedDict(TypedDict):
    type: CreateChatCompletionResponseFormatProxyType


class CreateChatCompletionResponseFormat2(BaseModel):
    type: CreateChatCompletionResponseFormatProxyType


CreateChatCompletionResponseFormatType = Literal["text"]


class CreateChatCompletionResponseFormat1TypedDict(TypedDict):
    type: CreateChatCompletionResponseFormatType


class CreateChatCompletionResponseFormat1(BaseModel):
    type: CreateChatCompletionResponseFormatType


CreateChatCompletionResponseFormatTypedDict = TypeAliasType(
    "CreateChatCompletionResponseFormatTypedDict",
    Union[
        CreateChatCompletionResponseFormat1TypedDict,
        CreateChatCompletionResponseFormat2TypedDict,
        CreateChatCompletionResponseFormat3TypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


CreateChatCompletionResponseFormat = TypeAliasType(
    "CreateChatCompletionResponseFormat",
    Union[
        CreateChatCompletionResponseFormat1,
        CreateChatCompletionResponseFormat2,
        CreateChatCompletionResponseFormat3,
    ],
)
r"""An object specifying the format that the model must output"""


StopTypedDict = TypeAliasType("StopTypedDict", Union[str, List[str]])
r"""Up to 4 sequences where the API will stop generating further tokens."""


Stop = TypeAliasType("Stop", Union[str, List[str]])
r"""Up to 4 sequences where the API will stop generating further tokens."""


class StreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class StreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


CreateChatCompletionType = Literal["enabled", "disabled"]
r"""Enables or disables the thinking mode capability"""


class ThinkingTypedDict(TypedDict):
    type: CreateChatCompletionType
    r"""Enables or disables the thinking mode capability"""
    budget_tokens: float
    r"""Determines how many tokens the model can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than `max_tokens`."""


class Thinking(BaseModel):
    type: CreateChatCompletionType
    r"""Enables or disables the thinking mode capability"""

    budget_tokens: float
    r"""Determines how many tokens the model can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than `max_tokens`."""


CreateChatCompletionProxyType = Literal["function"]
r"""The type of the tool. Currently, only function is supported."""

CreateChatCompletionProxyRequestType = Literal["object"]


class ParametersTypedDict(TypedDict):
    r"""The parameters the functions accepts, described as a JSON Schema object"""

    type: CreateChatCompletionProxyRequestType
    properties: Dict[str, Any]
    required: NotRequired[List[str]]
    additional_properties: NotRequired[bool]


class Parameters(BaseModel):
    r"""The parameters the functions accepts, described as a JSON Schema object"""

    type: CreateChatCompletionProxyRequestType

    properties: Dict[str, Any]

    required: Optional[List[str]] = None

    additional_properties: Annotated[
        Optional[bool], pydantic.Field(alias="additionalProperties")
    ] = None


class CreateChatCompletionFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""
    description: NotRequired[str]
    r"""A description of what the function does, used by the model to choose when and how to call the function."""
    parameters: NotRequired[ParametersTypedDict]
    r"""The parameters the functions accepts, described as a JSON Schema object"""
    strict: NotRequired[Nullable[bool]]
    r"""Whether to enable strict schema adherence when generating the function call."""


class CreateChatCompletionFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""

    description: Optional[str] = None
    r"""A description of what the function does, used by the model to choose when and how to call the function."""

    parameters: Optional[Parameters] = None
    r"""The parameters the functions accepts, described as a JSON Schema object"""

    strict: OptionalNullable[bool] = UNSET
    r"""Whether to enable strict schema adherence when generating the function call."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["name", "description", "parameters", "strict"]
        nullable_fields = ["strict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ToolsTypedDict(TypedDict):
    function: CreateChatCompletionFunctionTypedDict
    type: NotRequired[CreateChatCompletionProxyType]
    r"""The type of the tool. Currently, only function is supported."""


class Tools(BaseModel):
    function: CreateChatCompletionFunction

    type: Optional[CreateChatCompletionProxyType] = None
    r"""The type of the tool. Currently, only function is supported."""


ToolChoiceType = Literal["function"]
r"""The type of the tool. Currently, only function is supported."""


class ToolChoiceFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""


class ToolChoiceFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""


class ToolChoice2TypedDict(TypedDict):
    function: ToolChoiceFunctionTypedDict
    type: NotRequired[ToolChoiceType]
    r"""The type of the tool. Currently, only function is supported."""


class ToolChoice2(BaseModel):
    function: ToolChoiceFunction

    type: Optional[ToolChoiceType] = None
    r"""The type of the tool. Currently, only function is supported."""


ToolChoice1 = Literal["none", "auto", "required"]

ToolChoiceTypedDict = TypeAliasType(
    "ToolChoiceTypedDict", Union[ToolChoice2TypedDict, ToolChoice1]
)
r"""Controls which (if any) tool is called by the model."""


ToolChoice = TypeAliasType("ToolChoice", Union[ToolChoice2, ToolChoice1])
r"""Controls which (if any) tool is called by the model."""


Modalities = Literal["text", "audio"]


class WebSearchOptionsTypedDict(TypedDict):
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""

    enabled: NotRequired[bool]
    r"""Whether to enable web search for this request."""


class WebSearchOptions(BaseModel):
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""

    enabled: Optional[bool] = None
    r"""Whether to enable web search for this request."""


class RetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class Retry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""


class FallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class Fallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


Version = Literal["latest"]
r"""Version of the prompt to use (currently only \"latest\" supported)"""


class PromptTypedDict(TypedDict):
    r"""Prompt configuration for the request"""

    id: str
    r"""Unique identifier of the prompt to use"""
    version: Version
    r"""Version of the prompt to use (currently only \"latest\" supported)"""


class Prompt(BaseModel):
    r"""Prompt configuration for the request"""

    id: str
    r"""Unique identifier of the prompt to use"""

    version: Version
    r"""Version of the prompt to use (currently only \"latest\" supported)"""


class CreateChatCompletionThreadTypedDict(TypedDict):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""
    tags: NotRequired[List[str]]
    r"""Optional tags to differentiate or categorize threads"""


class CreateChatCompletionThread(BaseModel):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""

    tags: Optional[List[str]] = None
    r"""Optional tags to differentiate or categorize threads"""


CreateChatCompletionProxyRequestRequestBodyType = Literal["exact_match"]


class CacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: CreateChatCompletionProxyRequestRequestBodyType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class Cache(BaseModel):
    r"""Cache configuration for the request."""

    type: CreateChatCompletionProxyRequestRequestBodyType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


CreateChatCompletionSearchType = Literal[
    "vector_search", "keyword_search", "hybrid_search"
]
r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""


class CreateChatCompletionOrExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class CreateChatCompletionOrExists(BaseModel):
    r"""Exists"""

    exists: bool


CreateChatCompletionOrProxyNinTypedDict = TypeAliasType(
    "CreateChatCompletionOrProxyNinTypedDict", Union[str, float, bool]
)


CreateChatCompletionOrProxyNin = TypeAliasType(
    "CreateChatCompletionOrProxyNin", Union[str, float, bool]
)


class CreateChatCompletionOrNinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[CreateChatCompletionOrProxyNinTypedDict]


class CreateChatCompletionOrNin(BaseModel):
    r"""Not in"""

    nin: List[CreateChatCompletionOrProxyNin]


CreateChatCompletionOrProxyInTypedDict = TypeAliasType(
    "CreateChatCompletionOrProxyInTypedDict", Union[str, float, bool]
)


CreateChatCompletionOrProxyIn = TypeAliasType(
    "CreateChatCompletionOrProxyIn", Union[str, float, bool]
)


class CreateChatCompletionOrInTypedDict(TypedDict):
    r"""In"""

    in_: List[CreateChatCompletionOrProxyInTypedDict]


class CreateChatCompletionOrIn(BaseModel):
    r"""In"""

    in_: Annotated[List[CreateChatCompletionOrProxyIn], pydantic.Field(alias="in")]


class CreateChatCompletionOrLteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletionOrLte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletionOrLtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class CreateChatCompletionOrLt(BaseModel):
    r"""Less than"""

    lt: float


class CreateChatCompletionOrGteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletionOrGte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletionOr3TypedDict(TypedDict):
    gt: float


class CreateChatCompletionOr3(BaseModel):
    gt: float


CreateChatCompletionOrProxyNeTypedDict = TypeAliasType(
    "CreateChatCompletionOrProxyNeTypedDict", Union[str, float, bool]
)


CreateChatCompletionOrProxyNe = TypeAliasType(
    "CreateChatCompletionOrProxyNe", Union[str, float, bool]
)


class CreateChatCompletionOrNeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: CreateChatCompletionOrProxyNeTypedDict


class CreateChatCompletionOrNe(BaseModel):
    r"""Not equal to"""

    ne: CreateChatCompletionOrProxyNe


CreateChatCompletionOrProxyEqTypedDict = TypeAliasType(
    "CreateChatCompletionOrProxyEqTypedDict", Union[str, float, bool]
)


CreateChatCompletionOrProxyEq = TypeAliasType(
    "CreateChatCompletionOrProxyEq", Union[str, float, bool]
)


class CreateChatCompletionOrEqTypedDict(TypedDict):
    r"""Equal to"""

    eq: CreateChatCompletionOrProxyEqTypedDict


class CreateChatCompletionOrEq(BaseModel):
    r"""Equal to"""

    eq: CreateChatCompletionOrProxyEq


CreateChatCompletionFilterByProxyOrTypedDict = TypeAliasType(
    "CreateChatCompletionFilterByProxyOrTypedDict",
    Union[
        CreateChatCompletionOrEqTypedDict,
        CreateChatCompletionOrNeTypedDict,
        CreateChatCompletionOr3TypedDict,
        CreateChatCompletionOrGteTypedDict,
        CreateChatCompletionOrLtTypedDict,
        CreateChatCompletionOrLteTypedDict,
        CreateChatCompletionOrInTypedDict,
        CreateChatCompletionOrNinTypedDict,
        CreateChatCompletionOrExistsTypedDict,
    ],
)


CreateChatCompletionFilterByProxyOr = TypeAliasType(
    "CreateChatCompletionFilterByProxyOr",
    Union[
        CreateChatCompletionOrEq,
        CreateChatCompletionOrNe,
        CreateChatCompletionOr3,
        CreateChatCompletionOrGte,
        CreateChatCompletionOrLt,
        CreateChatCompletionOrLte,
        CreateChatCompletionOrIn,
        CreateChatCompletionOrNin,
        CreateChatCompletionOrExists,
    ],
)


class CreateChatCompletionFilterByOrTypedDict(TypedDict):
    r"""Or"""

    or_: List[Dict[str, CreateChatCompletionFilterByProxyOrTypedDict]]


class CreateChatCompletionFilterByOr(BaseModel):
    r"""Or"""

    or_: Annotated[
        List[Dict[str, CreateChatCompletionFilterByProxyOr]], pydantic.Field(alias="or")
    ]


class CreateChatCompletionAndExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class CreateChatCompletionAndExists(BaseModel):
    r"""Exists"""

    exists: bool


CreateChatCompletionAndProxyNinTypedDict = TypeAliasType(
    "CreateChatCompletionAndProxyNinTypedDict", Union[str, float, bool]
)


CreateChatCompletionAndProxyNin = TypeAliasType(
    "CreateChatCompletionAndProxyNin", Union[str, float, bool]
)


class CreateChatCompletionAndNinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[CreateChatCompletionAndProxyNinTypedDict]


class CreateChatCompletionAndNin(BaseModel):
    r"""Not in"""

    nin: List[CreateChatCompletionAndProxyNin]


CreateChatCompletionAndProxyInTypedDict = TypeAliasType(
    "CreateChatCompletionAndProxyInTypedDict", Union[str, float, bool]
)


CreateChatCompletionAndProxyIn = TypeAliasType(
    "CreateChatCompletionAndProxyIn", Union[str, float, bool]
)


class CreateChatCompletionAndInTypedDict(TypedDict):
    r"""In"""

    in_: List[CreateChatCompletionAndProxyInTypedDict]


class CreateChatCompletionAndIn(BaseModel):
    r"""In"""

    in_: Annotated[List[CreateChatCompletionAndProxyIn], pydantic.Field(alias="in")]


class CreateChatCompletionAndLteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletionAndLte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletionAndLtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class CreateChatCompletionAndLt(BaseModel):
    r"""Less than"""

    lt: float


class CreateChatCompletionAndGteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletionAndGte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletionAnd3TypedDict(TypedDict):
    gt: float


class CreateChatCompletionAnd3(BaseModel):
    gt: float


CreateChatCompletionAndProxyNeTypedDict = TypeAliasType(
    "CreateChatCompletionAndProxyNeTypedDict", Union[str, float, bool]
)


CreateChatCompletionAndProxyNe = TypeAliasType(
    "CreateChatCompletionAndProxyNe", Union[str, float, bool]
)


class CreateChatCompletionAndNeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: CreateChatCompletionAndProxyNeTypedDict


class CreateChatCompletionAndNe(BaseModel):
    r"""Not equal to"""

    ne: CreateChatCompletionAndProxyNe


CreateChatCompletionAndProxyEqTypedDict = TypeAliasType(
    "CreateChatCompletionAndProxyEqTypedDict", Union[str, float, bool]
)


CreateChatCompletionAndProxyEq = TypeAliasType(
    "CreateChatCompletionAndProxyEq", Union[str, float, bool]
)


class CreateChatCompletionAndEqTypedDict(TypedDict):
    r"""Equal to"""

    eq: CreateChatCompletionAndProxyEqTypedDict


class CreateChatCompletionAndEq(BaseModel):
    r"""Equal to"""

    eq: CreateChatCompletionAndProxyEq


CreateChatCompletionFilterByProxyAndTypedDict = TypeAliasType(
    "CreateChatCompletionFilterByProxyAndTypedDict",
    Union[
        CreateChatCompletionAndEqTypedDict,
        CreateChatCompletionAndNeTypedDict,
        CreateChatCompletionAnd3TypedDict,
        CreateChatCompletionAndGteTypedDict,
        CreateChatCompletionAndLtTypedDict,
        CreateChatCompletionAndLteTypedDict,
        CreateChatCompletionAndInTypedDict,
        CreateChatCompletionAndNinTypedDict,
        CreateChatCompletionAndExistsTypedDict,
    ],
)


CreateChatCompletionFilterByProxyAnd = TypeAliasType(
    "CreateChatCompletionFilterByProxyAnd",
    Union[
        CreateChatCompletionAndEq,
        CreateChatCompletionAndNe,
        CreateChatCompletionAnd3,
        CreateChatCompletionAndGte,
        CreateChatCompletionAndLt,
        CreateChatCompletionAndLte,
        CreateChatCompletionAndIn,
        CreateChatCompletionAndNin,
        CreateChatCompletionAndExists,
    ],
)


class CreateChatCompletionFilterByAndTypedDict(TypedDict):
    r"""And"""

    and_: List[Dict[str, CreateChatCompletionFilterByProxyAndTypedDict]]


class CreateChatCompletionFilterByAnd(BaseModel):
    r"""And"""

    and_: Annotated[
        List[Dict[str, CreateChatCompletionFilterByProxyAnd]],
        pydantic.Field(alias="and"),
    ]


class CreateChatCompletion1ExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class CreateChatCompletion1Exists(BaseModel):
    r"""Exists"""

    exists: bool


CreateChatCompletion1ProxyNinTypedDict = TypeAliasType(
    "CreateChatCompletion1ProxyNinTypedDict", Union[str, float, bool]
)


CreateChatCompletion1ProxyNin = TypeAliasType(
    "CreateChatCompletion1ProxyNin", Union[str, float, bool]
)


class CreateChatCompletion1NinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[CreateChatCompletion1ProxyNinTypedDict]


class CreateChatCompletion1Nin(BaseModel):
    r"""Not in"""

    nin: List[CreateChatCompletion1ProxyNin]


CreateChatCompletion1ProxyInTypedDict = TypeAliasType(
    "CreateChatCompletion1ProxyInTypedDict", Union[str, float, bool]
)


CreateChatCompletion1ProxyIn = TypeAliasType(
    "CreateChatCompletion1ProxyIn", Union[str, float, bool]
)


class CreateChatCompletion1InTypedDict(TypedDict):
    r"""In"""

    in_: List[CreateChatCompletion1ProxyInTypedDict]


class CreateChatCompletion1In(BaseModel):
    r"""In"""

    in_: Annotated[List[CreateChatCompletion1ProxyIn], pydantic.Field(alias="in")]


class CreateChatCompletion1LteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletion1Lte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletion1LtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class CreateChatCompletion1Lt(BaseModel):
    r"""Less than"""

    lt: float


class CreateChatCompletion1GteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletion1Gte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletion13TypedDict(TypedDict):
    gt: float


class CreateChatCompletion13(BaseModel):
    gt: float


CreateChatCompletion1ProxyNeTypedDict = TypeAliasType(
    "CreateChatCompletion1ProxyNeTypedDict", Union[str, float, bool]
)


CreateChatCompletion1ProxyNe = TypeAliasType(
    "CreateChatCompletion1ProxyNe", Union[str, float, bool]
)


class CreateChatCompletion1NeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: CreateChatCompletion1ProxyNeTypedDict


class CreateChatCompletion1Ne(BaseModel):
    r"""Not equal to"""

    ne: CreateChatCompletion1ProxyNe


CreateChatCompletion1ProxyEqTypedDict = TypeAliasType(
    "CreateChatCompletion1ProxyEqTypedDict", Union[str, float, bool]
)


CreateChatCompletion1ProxyEq = TypeAliasType(
    "CreateChatCompletion1ProxyEq", Union[str, float, bool]
)


class CreateChatCompletion1EqTypedDict(TypedDict):
    r"""Equal to"""

    eq: CreateChatCompletion1ProxyEqTypedDict


class CreateChatCompletion1Eq(BaseModel):
    r"""Equal to"""

    eq: CreateChatCompletion1ProxyEq


CreateChatCompletionFilterBy1TypedDict = TypeAliasType(
    "CreateChatCompletionFilterBy1TypedDict",
    Union[
        CreateChatCompletion1EqTypedDict,
        CreateChatCompletion1NeTypedDict,
        CreateChatCompletion13TypedDict,
        CreateChatCompletion1GteTypedDict,
        CreateChatCompletion1LtTypedDict,
        CreateChatCompletion1LteTypedDict,
        CreateChatCompletion1InTypedDict,
        CreateChatCompletion1NinTypedDict,
        CreateChatCompletion1ExistsTypedDict,
    ],
)


CreateChatCompletionFilterBy1 = TypeAliasType(
    "CreateChatCompletionFilterBy1",
    Union[
        CreateChatCompletion1Eq,
        CreateChatCompletion1Ne,
        CreateChatCompletion13,
        CreateChatCompletion1Gte,
        CreateChatCompletion1Lt,
        CreateChatCompletion1Lte,
        CreateChatCompletion1In,
        CreateChatCompletion1Nin,
        CreateChatCompletion1Exists,
    ],
)


CreateChatCompletionFilterByTypedDict = TypeAliasType(
    "CreateChatCompletionFilterByTypedDict",
    Union[
        CreateChatCompletionFilterByAndTypedDict,
        CreateChatCompletionFilterByOrTypedDict,
        Dict[str, CreateChatCompletionFilterBy1TypedDict],
    ],
)
r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""


CreateChatCompletionFilterBy = TypeAliasType(
    "CreateChatCompletionFilterBy",
    Union[
        CreateChatCompletionFilterByAnd,
        CreateChatCompletionFilterByOr,
        Dict[str, CreateChatCompletionFilterBy1],
    ],
)
r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""


class CreateChatCompletionSearchOptionsTypedDict(TypedDict):
    r"""Additional search options"""

    include_vectors: NotRequired[bool]
    r"""Whether to include the vector in the chunk"""
    include_metadata: NotRequired[bool]
    r"""Whether to include the metadata in the chunk"""
    include_scores: NotRequired[bool]
    r"""Whether to include the scores in the chunk"""


class CreateChatCompletionSearchOptions(BaseModel):
    r"""Additional search options"""

    include_vectors: Optional[bool] = None
    r"""Whether to include the vector in the chunk"""

    include_metadata: Optional[bool] = None
    r"""Whether to include the metadata in the chunk"""

    include_scores: Optional[bool] = None
    r"""Whether to include the scores in the chunk"""


CreateChatCompletionProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
    "litellm",
    "openailike",
    "cerebras",
    "bytedance",
]

CreateChatCompletionModelType = Literal["rerank"]


class CreateChatCompletionModelParametersTypedDict(TypedDict):
    threshold: NotRequired[float]
    r"""The threshold value used to filter the rerank results, only documents with a relevance score greater than the threshold will be returned"""


class CreateChatCompletionModelParameters(BaseModel):
    threshold: Optional[float] = None
    r"""The threshold value used to filter the rerank results, only documents with a relevance score greater than the threshold will be returned"""


class CreateChatCompletionRerankConfigTypedDict(TypedDict):
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    enabled: NotRequired[bool]
    provider: NotRequired[CreateChatCompletionProvider]
    top_k: NotRequired[int]
    r"""The number of results to return by the reranking model"""
    model: NotRequired[str]
    r"""The name of the model to use"""
    model_db_id: NotRequired[str]
    r"""The ID of the model in the database"""
    model_type: NotRequired[CreateChatCompletionModelType]
    model_parameters: NotRequired[CreateChatCompletionModelParametersTypedDict]


class CreateChatCompletionRerankConfig(BaseModel):
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    enabled: Optional[bool] = None

    provider: Optional[CreateChatCompletionProvider] = None

    top_k: Optional[int] = None
    r"""The number of results to return by the reranking model"""

    model: Optional[str] = None
    r"""The name of the model to use"""

    model_db_id: Optional[str] = None
    r"""The ID of the model in the database"""

    model_type: Optional[CreateChatCompletionModelType] = None

    model_parameters: Optional[CreateChatCompletionModelParameters] = None


CreateChatCompletionProxyProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
    "litellm",
    "openailike",
    "cerebras",
    "bytedance",
]


class CreateChatCompletionAgenticRagConfigTypedDict(TypedDict):
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    model_db_id: str
    provider: CreateChatCompletionProxyProvider
    integration_id: NotRequired[Nullable[str]]


class CreateChatCompletionAgenticRagConfig(BaseModel):
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    model_db_id: str

    provider: CreateChatCompletionProxyProvider

    integration_id: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["integration_id"]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class KnowledgeBasesTypedDict(TypedDict):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""
    top_k: NotRequired[int]
    r"""The number of results to return. If not provided, will default to the knowledge base configured `top_k`."""
    threshold: NotRequired[float]
    r"""The threshold to apply to the search. If not provided, will default to the knowledge base configured `threshold`"""
    search_type: NotRequired[CreateChatCompletionSearchType]
    r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""
    filter_by: NotRequired[CreateChatCompletionFilterByTypedDict]
    r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""
    search_options: NotRequired[CreateChatCompletionSearchOptionsTypedDict]
    r"""Additional search options"""
    rerank_config: NotRequired[Nullable[CreateChatCompletionRerankConfigTypedDict]]
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""
    agentic_rag_config: NotRequired[CreateChatCompletionAgenticRagConfigTypedDict]
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""
    query: NotRequired[str]
    r"""The query to use to search the knowledge base. If not provided we will use the last user message from the messages of the requests"""


class KnowledgeBases(BaseModel):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""

    top_k: Optional[int] = None
    r"""The number of results to return. If not provided, will default to the knowledge base configured `top_k`."""

    threshold: Optional[float] = None
    r"""The threshold to apply to the search. If not provided, will default to the knowledge base configured `threshold`"""

    search_type: Optional[CreateChatCompletionSearchType] = "hybrid_search"
    r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""

    filter_by: Optional[CreateChatCompletionFilterBy] = None
    r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""

    search_options: Optional[CreateChatCompletionSearchOptions] = None
    r"""Additional search options"""

    rerank_config: OptionalNullable[CreateChatCompletionRerankConfig] = UNSET
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    agentic_rag_config: Optional[CreateChatCompletionAgenticRagConfig] = None
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    query: Optional[str] = None
    r"""The query to use to search the knowledge base. If not provided we will use the last user message from the messages of the requests"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "top_k",
            "threshold",
            "search_type",
            "filter_by",
            "search_options",
            "rerank_config",
            "agentic_rag_config",
            "query",
        ]
        nullable_fields = ["rerank_config"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionOrqTypedDict(TypedDict):
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    retry: NotRequired[RetryTypedDict]
    r"""Retry configuration for the request"""
    fallbacks: NotRequired[List[FallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    prompt: NotRequired[PromptTypedDict]
    r"""Prompt configuration for the request"""
    contact: NotRequired[PublicContactTypedDict]
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""
    thread: NotRequired[CreateChatCompletionThreadTypedDict]
    r"""Thread information to group related requests"""
    inputs: NotRequired[Dict[str, Any]]
    r"""Values to replace in the prompt messages using {{variableName}} syntax"""
    cache: NotRequired[CacheTypedDict]
    r"""Cache configuration for the request."""
    knowledge_bases: NotRequired[List[KnowledgeBasesTypedDict]]


class CreateChatCompletionOrq(BaseModel):
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    retry: Optional[Retry] = None
    r"""Retry configuration for the request"""

    fallbacks: Optional[List[Fallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    prompt: Optional[Prompt] = None
    r"""Prompt configuration for the request"""

    contact: Optional[PublicContact] = None
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""

    thread: Optional[CreateChatCompletionThread] = None
    r"""Thread information to group related requests"""

    inputs: Optional[Dict[str, Any]] = None
    r"""Values to replace in the prompt messages using {{variableName}} syntax"""

    cache: Optional[Cache] = None
    r"""Cache configuration for the request."""

    knowledge_bases: Optional[List[KnowledgeBases]] = None


class CreateChatCompletionRequestBodyTypedDict(TypedDict):
    messages: List[CreateChatCompletionMessagesTypedDict]
    r"""A list of messages comprising the conversation so far."""
    model: str
    r"""ID of the model to use"""
    metadata: NotRequired[Dict[str, str]]
    r"""Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can have a maximum length of 64 characters and values can have a maximum length of 512 characters."""
    audio: NotRequired[Nullable[CreateChatCompletionAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[CreateChatCompletionResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[str]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[StopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[Nullable[StreamOptionsTypedDict]]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[ThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tools: NotRequired[List[ToolsTypedDict]]
    r"""A list of tools the model may call."""
    tool_choice: NotRequired[ToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[Nullable[List[Modalities]]]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    web_search_options: NotRequired[WebSearchOptionsTypedDict]
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""
    orq: NotRequired[CreateChatCompletionOrqTypedDict]
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""
    stream: NotRequired[bool]


class CreateChatCompletionRequestBody(BaseModel):
    messages: List[CreateChatCompletionMessages]
    r"""A list of messages comprising the conversation so far."""

    model: str
    r"""ID of the model to use"""

    metadata: Optional[Dict[str, str]] = None
    r"""Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can have a maximum length of 64 characters and values can have a maximum length of 512 characters."""

    audio: OptionalNullable[CreateChatCompletionAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[CreateChatCompletionResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[str] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[Stop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[StreamOptions] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[Thinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tools: Optional[List[Tools]] = None
    r"""A list of tools the model may call."""

    tool_choice: Optional[ToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[Modalities]] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    web_search_options: Optional[WebSearchOptions] = None
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""

    orq: Optional[CreateChatCompletionOrq] = None
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    stream: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "metadata",
            "audio",
            "frequency_penalty",
            "max_tokens",
            "max_completion_tokens",
            "logprobs",
            "top_logprobs",
            "n",
            "presence_penalty",
            "response_format",
            "reasoning_effort",
            "verbosity",
            "seed",
            "stop",
            "stream_options",
            "thinking",
            "temperature",
            "top_p",
            "top_k",
            "tools",
            "tool_choice",
            "parallel_tool_calls",
            "modalities",
            "web_search_options",
            "orq",
            "stream",
        ]
        nullable_fields = [
            "audio",
            "frequency_penalty",
            "max_tokens",
            "max_completion_tokens",
            "logprobs",
            "top_logprobs",
            "n",
            "presence_penalty",
            "seed",
            "stop",
            "stream_options",
            "temperature",
            "top_p",
            "top_k",
            "modalities",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateChatCompletionFinishReason = Literal[
    "stop", "length", "tool_calls", "content_filter", "function_call"
]
r"""The reason the model stopped generating tokens."""


class CreateChatCompletionProxyTopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class CreateChatCompletionProxyTopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionProxyContentTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[CreateChatCompletionProxyTopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class CreateChatCompletionProxyContent(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[CreateChatCompletionProxyTopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionProxyResponseTopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class CreateChatCompletionProxyResponseTopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionRefusalTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[CreateChatCompletionProxyResponseTopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class CreateChatCompletionRefusal(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[CreateChatCompletionProxyResponseTopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionLogprobsTypedDict(TypedDict):
    r"""Log probability information for the choice."""

    content: Nullable[List[CreateChatCompletionProxyContentTypedDict]]
    r"""A list of message content tokens with log probability information."""
    refusal: Nullable[List[CreateChatCompletionRefusalTypedDict]]
    r"""A list of message refusal tokens with log probability information."""


class CreateChatCompletionLogprobs(BaseModel):
    r"""Log probability information for the choice."""

    content: Nullable[List[CreateChatCompletionProxyContent]]
    r"""A list of message content tokens with log probability information."""

    refusal: Nullable[List[CreateChatCompletionRefusal]]
    r"""A list of message refusal tokens with log probability information."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["content", "refusal"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateChatCompletionProxyResponse200Type = Literal["function"]
r"""The type of the tool. Currently, only `function` is supported."""


class CreateChatCompletionProxyResponseFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreateChatCompletionProxyResponseFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreateChatCompletionProxyToolCallsTypedDict(TypedDict):
    id: NotRequired[str]
    r"""The ID of the tool call."""
    type: NotRequired[CreateChatCompletionProxyResponse200Type]
    r"""The type of the tool. Currently, only `function` is supported."""
    function: NotRequired[CreateChatCompletionProxyResponseFunctionTypedDict]


class CreateChatCompletionProxyToolCalls(BaseModel):
    id: Optional[str] = None
    r"""The ID of the tool call."""

    type: Optional[CreateChatCompletionProxyResponse200Type] = None
    r"""The type of the tool. Currently, only `function` is supported."""

    function: Optional[CreateChatCompletionProxyResponseFunction] = None


CreateChatCompletionProxyRole = Literal["assistant"]


class CreateChatCompletionProxyResponseAudioTypedDict(TypedDict):
    r"""Audio response data in streaming mode."""

    id: NotRequired[str]
    transcript: NotRequired[str]
    data: NotRequired[str]
    expires_at: NotRequired[int]


class CreateChatCompletionProxyResponseAudio(BaseModel):
    r"""Audio response data in streaming mode."""

    id: Optional[str] = None

    transcript: Optional[str] = None

    data: Optional[str] = None

    expires_at: Optional[int] = None


class DeltaTypedDict(TypedDict):
    r"""A chat completion delta generated by streamed model responses."""

    content: NotRequired[Nullable[str]]
    r"""The contents of the chunk message."""
    refusal: NotRequired[Nullable[str]]
    tool_calls: NotRequired[List[CreateChatCompletionProxyToolCallsTypedDict]]
    role: NotRequired[CreateChatCompletionProxyRole]
    reasoning: NotRequired[str]
    r"""Internal thought process of the model"""
    reasoning_signature: NotRequired[str]
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""
    redacted_reasoning: NotRequired[str]
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""
    audio: NotRequired[Nullable[CreateChatCompletionProxyResponseAudioTypedDict]]
    r"""Audio response data in streaming mode."""


class Delta(BaseModel):
    r"""A chat completion delta generated by streamed model responses."""

    content: OptionalNullable[str] = UNSET
    r"""The contents of the chunk message."""

    refusal: OptionalNullable[str] = UNSET

    tool_calls: Optional[List[CreateChatCompletionProxyToolCalls]] = None

    role: Optional[CreateChatCompletionProxyRole] = None

    reasoning: Optional[str] = None
    r"""Internal thought process of the model"""

    reasoning_signature: Optional[str] = None
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""

    redacted_reasoning: Optional[str] = None
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""

    audio: OptionalNullable[CreateChatCompletionProxyResponseAudio] = UNSET
    r"""Audio response data in streaming mode."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "content",
            "refusal",
            "tool_calls",
            "role",
            "reasoning",
            "reasoning_signature",
            "redacted_reasoning",
            "audio",
        ]
        nullable_fields = ["content", "refusal", "audio"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionProxyChoicesTypedDict(TypedDict):
    finish_reason: Nullable[CreateChatCompletionFinishReason]
    r"""The reason the model stopped generating tokens."""
    delta: DeltaTypedDict
    r"""A chat completion delta generated by streamed model responses."""
    index: NotRequired[float]
    r"""The index of the choice in the list of choices."""
    logprobs: NotRequired[Nullable[CreateChatCompletionLogprobsTypedDict]]
    r"""Log probability information for the choice."""


class CreateChatCompletionProxyChoices(BaseModel):
    finish_reason: Nullable[CreateChatCompletionFinishReason]
    r"""The reason the model stopped generating tokens."""

    delta: Delta
    r"""A chat completion delta generated by streamed model responses."""

    index: Optional[float] = None
    r"""The index of the choice in the list of choices."""

    logprobs: OptionalNullable[CreateChatCompletionLogprobs] = UNSET
    r"""Log probability information for the choice."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["index", "logprobs"]
        nullable_fields = ["finish_reason", "logprobs"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionPromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[int]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio input tokens consumed by the request."""


class CreateChatCompletionPromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[int] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio input tokens consumed by the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["cached_tokens", "audio_tokens"]
        nullable_fields = ["cached_tokens", "audio_tokens"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionCompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]
    accepted_prediction_tokens: NotRequired[Nullable[float]]
    rejected_prediction_tokens: NotRequired[Nullable[float]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio output tokens produced by the response."""


class CreateChatCompletionCompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    accepted_prediction_tokens: OptionalNullable[float] = UNSET

    rejected_prediction_tokens: OptionalNullable[float] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio output tokens produced by the response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        nullable_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionProxyUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""
    prompt_tokens_details: NotRequired[
        Nullable[CreateChatCompletionPromptTokensDetailsTypedDict]
    ]
    completion_tokens_details: NotRequired[
        Nullable[CreateChatCompletionCompletionTokensDetailsTypedDict]
    ]


class CreateChatCompletionProxyUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""

    prompt_tokens_details: OptionalNullable[CreateChatCompletionPromptTokensDetails] = (
        UNSET
    )

    completion_tokens_details: OptionalNullable[
        CreateChatCompletionCompletionTokensDetails
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "completion_tokens",
            "prompt_tokens",
            "total_tokens",
            "prompt_tokens_details",
            "completion_tokens_details",
        ]
        nullable_fields = ["prompt_tokens_details", "completion_tokens_details"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateChatCompletionProxyObject = Literal["chat.completion.chunk"]


class CreateChatCompletionDataTypedDict(TypedDict):
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    choices: List[CreateChatCompletionProxyChoicesTypedDict]
    r"""A list of chat completion choices. Can contain more than one elements if n is greater than 1. Can also be empty for the last chunk if you set stream_options: {\"include_usage\": true}."""
    object: CreateChatCompletionProxyObject
    id: NotRequired[str]
    r"""A unique identifier for the chat completion."""
    created: NotRequired[float]
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    model: NotRequired[str]
    r"""The model used for the chat completion."""
    system_fingerprint: NotRequired[Nullable[str]]
    r"""This fingerprint represents the backend configuration that the model runs with."""
    usage: NotRequired[CreateChatCompletionProxyUsageTypedDict]
    r"""Usage statistics for the completion request."""


class CreateChatCompletionData(BaseModel):
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    choices: List[CreateChatCompletionProxyChoices]
    r"""A list of chat completion choices. Can contain more than one elements if n is greater than 1. Can also be empty for the last chunk if you set stream_options: {\"include_usage\": true}."""

    object: CreateChatCompletionProxyObject

    id: Optional[str] = None
    r"""A unique identifier for the chat completion."""

    created: Optional[float] = None
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    model: Optional[str] = None
    r"""The model used for the chat completion."""

    system_fingerprint: OptionalNullable[str] = UNSET
    r"""This fingerprint represents the backend configuration that the model runs with."""

    usage: Optional[CreateChatCompletionProxyUsage] = None
    r"""Usage statistics for the completion request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["id", "created", "model", "system_fingerprint", "usage"]
        nullable_fields = ["system_fingerprint"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionProxyResponseBodyTypedDict(TypedDict):
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    data: NotRequired[CreateChatCompletionDataTypedDict]
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""


class CreateChatCompletionProxyResponseBody(BaseModel):
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    data: Optional[CreateChatCompletionData] = None
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""


FinishReason = Literal[
    "stop", "length", "tool_calls", "content_filter", "function_call"
]
r"""The reason the model stopped generating tokens."""

CreateChatCompletionProxyResponseType = Literal["function"]


class CreateChatCompletionProxyFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreateChatCompletionProxyFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreateChatCompletionToolCallsTypedDict(TypedDict):
    id: NotRequired[str]
    type: NotRequired[CreateChatCompletionProxyResponseType]
    function: NotRequired[CreateChatCompletionProxyFunctionTypedDict]


class CreateChatCompletionToolCalls(BaseModel):
    id: Optional[str] = None

    type: Optional[CreateChatCompletionProxyResponseType] = None

    function: Optional[CreateChatCompletionProxyFunction] = None


CreateChatCompletionRole = Literal["assistant"]


class CreateChatCompletionProxyAudioTypedDict(TypedDict):
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""

    id: str
    expires_at: int
    data: str
    transcript: str


class CreateChatCompletionProxyAudio(BaseModel):
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""

    id: str

    expires_at: int

    data: str

    transcript: str


class CreateChatCompletionMessageTypedDict(TypedDict):
    r"""A chat completion message generated by the model."""

    content: NotRequired[Nullable[str]]
    refusal: NotRequired[Nullable[str]]
    tool_calls: NotRequired[List[CreateChatCompletionToolCallsTypedDict]]
    role: NotRequired[CreateChatCompletionRole]
    reasoning: NotRequired[str]
    r"""Internal thought process of the model"""
    reasoning_signature: NotRequired[str]
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""
    redacted_reasoning: NotRequired[str]
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""
    audio: NotRequired[Nullable[CreateChatCompletionProxyAudioTypedDict]]
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""


class CreateChatCompletionMessage(BaseModel):
    r"""A chat completion message generated by the model."""

    content: OptionalNullable[str] = UNSET

    refusal: OptionalNullable[str] = UNSET

    tool_calls: Optional[List[CreateChatCompletionToolCalls]] = None

    role: Optional[CreateChatCompletionRole] = None

    reasoning: Optional[str] = None
    r"""Internal thought process of the model"""

    reasoning_signature: Optional[str] = None
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""

    redacted_reasoning: Optional[str] = None
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""

    audio: OptionalNullable[CreateChatCompletionProxyAudio] = UNSET
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "content",
            "refusal",
            "tool_calls",
            "role",
            "reasoning",
            "reasoning_signature",
            "redacted_reasoning",
            "audio",
        ]
        nullable_fields = ["content", "refusal", "audio"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class TopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class TopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionContentTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[TopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class CreateChatCompletionContent(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[TopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionTopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class CreateChatCompletionTopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class RefusalTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[CreateChatCompletionTopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class Refusal(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[CreateChatCompletionTopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class LogprobsTypedDict(TypedDict):
    r"""Log probability information for the choice."""

    content: Nullable[List[CreateChatCompletionContentTypedDict]]
    r"""A list of message content tokens with log probability information."""
    refusal: Nullable[List[RefusalTypedDict]]
    r"""A list of message refusal tokens with log probability information."""


class Logprobs(BaseModel):
    r"""Log probability information for the choice."""

    content: Nullable[List[CreateChatCompletionContent]]
    r"""A list of message content tokens with log probability information."""

    refusal: Nullable[List[Refusal]]
    r"""A list of message refusal tokens with log probability information."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["content", "refusal"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionChoicesTypedDict(TypedDict):
    finish_reason: Nullable[FinishReason]
    r"""The reason the model stopped generating tokens."""
    message: CreateChatCompletionMessageTypedDict
    r"""A chat completion message generated by the model."""
    index: NotRequired[float]
    r"""The index of the choice in the list of choices."""
    logprobs: NotRequired[Nullable[LogprobsTypedDict]]
    r"""Log probability information for the choice."""


class CreateChatCompletionChoices(BaseModel):
    finish_reason: Nullable[FinishReason]
    r"""The reason the model stopped generating tokens."""

    message: CreateChatCompletionMessage
    r"""A chat completion message generated by the model."""

    index: Optional[float] = None
    r"""The index of the choice in the list of choices."""

    logprobs: OptionalNullable[Logprobs] = UNSET
    r"""Log probability information for the choice."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["index", "logprobs"]
        nullable_fields = ["finish_reason", "logprobs"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[int]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio input tokens consumed by the request."""


class PromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[int] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio input tokens consumed by the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["cached_tokens", "audio_tokens"]
        nullable_fields = ["cached_tokens", "audio_tokens"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]
    accepted_prediction_tokens: NotRequired[Nullable[float]]
    rejected_prediction_tokens: NotRequired[Nullable[float]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio output tokens produced by the response."""


class CompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    accepted_prediction_tokens: OptionalNullable[float] = UNSET

    rejected_prediction_tokens: OptionalNullable[float] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio output tokens produced by the response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        nullable_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateChatCompletionUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""
    prompt_tokens_details: NotRequired[Nullable[PromptTokensDetailsTypedDict]]
    completion_tokens_details: NotRequired[Nullable[CompletionTokensDetailsTypedDict]]


class CreateChatCompletionUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""

    prompt_tokens_details: OptionalNullable[PromptTokensDetails] = UNSET

    completion_tokens_details: OptionalNullable[CompletionTokensDetails] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "completion_tokens",
            "prompt_tokens",
            "total_tokens",
            "prompt_tokens_details",
            "completion_tokens_details",
        ]
        nullable_fields = ["prompt_tokens_details", "completion_tokens_details"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateChatCompletionObject = Literal["chat.completion"]


class CreateChatCompletionResponseBodyTypedDict(TypedDict):
    r"""Represents a chat completion response returned by model, based on the provided input."""

    choices: List[CreateChatCompletionChoicesTypedDict]
    r"""A list of chat completion choices. Can be more than one if n is greater than 1."""
    object: CreateChatCompletionObject
    id: NotRequired[str]
    r"""A unique identifier for the chat completion."""
    created: NotRequired[float]
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    model: NotRequired[str]
    r"""The model used for the chat completion."""
    system_fingerprint: NotRequired[Nullable[str]]
    r"""This fingerprint represents the backend configuration that the model runs with."""
    usage: NotRequired[CreateChatCompletionUsageTypedDict]
    r"""Usage statistics for the completion request."""


class CreateChatCompletionResponseBody(BaseModel):
    r"""Represents a chat completion response returned by model, based on the provided input."""

    choices: List[CreateChatCompletionChoices]
    r"""A list of chat completion choices. Can be more than one if n is greater than 1."""

    object: CreateChatCompletionObject

    id: Optional[str] = None
    r"""A unique identifier for the chat completion."""

    created: Optional[float] = None
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    model: Optional[str] = None
    r"""The model used for the chat completion."""

    system_fingerprint: OptionalNullable[str] = UNSET
    r"""This fingerprint represents the backend configuration that the model runs with."""

    usage: Optional[CreateChatCompletionUsage] = None
    r"""Usage statistics for the completion request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["id", "created", "model", "system_fingerprint", "usage"]
        nullable_fields = ["system_fingerprint"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateChatCompletionResponseTypedDict = TypeAliasType(
    "CreateChatCompletionResponseTypedDict",
    Union[
        CreateChatCompletionResponseBodyTypedDict,
        Union[
            eventstreaming.EventStream[CreateChatCompletionProxyResponseBodyTypedDict],
            eventstreaming.EventStreamAsync[
                CreateChatCompletionProxyResponseBodyTypedDict
            ],
        ],
    ],
)


CreateChatCompletionResponse = TypeAliasType(
    "CreateChatCompletionResponse",
    Union[
        CreateChatCompletionResponseBody,
        Union[
            eventstreaming.EventStream[CreateChatCompletionProxyResponseBody],
            eventstreaming.EventStreamAsync[CreateChatCompletionProxyResponseBody],
        ],
    ],
)
