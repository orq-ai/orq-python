"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import eventstreaming, get_discriminator
import pydantic
from pydantic import ConfigDict, Discriminator, Tag, model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


Effort = Literal[
    "low",
    "medium",
    "high",
]
r"""The effort level for reasoning (o3-mini model only)"""


class ReasoningTypedDict(TypedDict):
    r"""Configuration for reasoning models"""

    effort: NotRequired[Effort]
    r"""The effort level for reasoning (o3-mini model only)"""


class Reasoning(BaseModel):
    r"""Configuration for reasoning models"""

    effort: Optional[Effort] = None
    r"""The effort level for reasoning (o3-mini model only)"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["effort"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateResponseFormatRouterResponsesType = Literal["json_schema",]
r"""Ensures the response matches a supplied JSON schema"""


class Format3TypedDict(TypedDict):
    type: CreateResponseFormatRouterResponsesType
    r"""Ensures the response matches a supplied JSON schema"""
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    schema_: Dict[str, Any]
    r"""The JSON schema to validate the response against"""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    strict: NotRequired[bool]
    r"""Whether to enable strict `schema` adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when `strict` is `true`"""


class Format3(BaseModel):
    type: CreateResponseFormatRouterResponsesType
    r"""Ensures the response matches a supplied JSON schema"""

    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]
    r"""The JSON schema to validate the response against"""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    strict: Optional[bool] = True
    r"""Whether to enable strict `schema` adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when `strict` is `true`"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateResponseFormatType = Literal["json_object",]
r"""Ensures the response is a valid JSON object"""


class Format2TypedDict(TypedDict):
    type: CreateResponseFormatType
    r"""Ensures the response is a valid JSON object"""


class Format2(BaseModel):
    type: CreateResponseFormatType
    r"""Ensures the response is a valid JSON object"""


FormatType = Literal["text",]
r"""Plain text response format"""


class Format1TypedDict(TypedDict):
    type: FormatType
    r"""Plain text response format"""


class Format1(BaseModel):
    type: FormatType
    r"""Plain text response format"""


CreateResponseFormatTypedDict = TypeAliasType(
    "CreateResponseFormatTypedDict",
    Union[Format1TypedDict, Format2TypedDict, Format3TypedDict],
)


CreateResponseFormat = Annotated[
    Union[
        Annotated[Format1, Tag("text")],
        Annotated[Format2, Tag("json_object")],
        Annotated[Format3, Tag("json_schema")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class CreateResponseTextTypedDict(TypedDict):
    format_: CreateResponseFormatTypedDict


class CreateResponseText(BaseModel):
    format_: Annotated[CreateResponseFormat, pydantic.Field(alias="format")]


CreateResponse2RouterResponsesRequestRequestBodyInputType = Literal["function_call",]
r"""The type of input item"""


class CreateResponse23TypedDict(TypedDict):
    r"""Represents a function tool call, provided as input to the model."""

    type: CreateResponse2RouterResponsesRequestRequestBodyInputType
    r"""The type of input item"""
    call_id: str
    r"""The ID of the function call"""
    id: str
    r"""The unique identifier for this function call"""
    name: str
    r"""The name of the function being called"""
    arguments: str
    r"""The arguments to the function as a JSON string"""
    status: str
    r"""The status of the function call"""


class CreateResponse23(BaseModel):
    r"""Represents a function tool call, provided as input to the model."""

    type: CreateResponse2RouterResponsesRequestRequestBodyInputType
    r"""The type of input item"""

    call_id: str
    r"""The ID of the function call"""

    id: str
    r"""The unique identifier for this function call"""

    name: str
    r"""The name of the function being called"""

    arguments: str
    r"""The arguments to the function as a JSON string"""

    status: str
    r"""The status of the function call"""


CreateResponse2RouterResponsesRequestRequestBodyType = Literal["function_call_output",]
r"""The type of input item"""


class CreateResponse2RouterResponses2TypedDict(TypedDict):
    r"""Represents the output of a function tool call, provided as input to the model."""

    type: CreateResponse2RouterResponsesRequestRequestBodyType
    r"""The type of input item"""
    call_id: str
    r"""The ID of the function call this output is for"""
    output: str
    r"""The output from the function call"""


class CreateResponse2RouterResponses2(BaseModel):
    r"""Represents the output of a function tool call, provided as input to the model."""

    type: CreateResponse2RouterResponsesRequestRequestBodyType
    r"""The type of input item"""

    call_id: str
    r"""The ID of the function call this output is for"""

    output: str
    r"""The output from the function call"""


TwoRole = Literal[
    "user",
    "assistant",
    "system",
    "developer",
]
r"""The role of the message author"""


CreateResponse2RouterResponsesRequestType = Literal["input_file",]
r"""The type of input content part"""


class Two3TypedDict(TypedDict):
    r"""A file input content part."""

    type: CreateResponse2RouterResponsesRequestType
    r"""The type of input content part"""
    file_data: NotRequired[str]
    r"""Base64 encoded file data"""
    file_id: NotRequired[str]
    r"""File ID from the Files API"""
    filename: NotRequired[str]
    r"""Name of the file"""
    file_url: NotRequired[str]
    r"""URL of the file to fetch"""


class Two3(BaseModel):
    r"""A file input content part."""

    type: CreateResponse2RouterResponsesRequestType
    r"""The type of input content part"""

    file_data: Optional[str] = None
    r"""Base64 encoded file data"""

    file_id: Optional[str] = None
    r"""File ID from the Files API"""

    filename: Optional[str] = None
    r"""Name of the file"""

    file_url: Optional[str] = None
    r"""URL of the file to fetch"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["file_data", "file_id", "filename", "file_url"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateResponse2RouterResponsesType = Literal["input_image",]
r"""The type of input content part"""


TwoDetail = Literal[
    "high",
    "low",
    "auto",
]
r"""Level of detail for image analysis"""


class CreateResponse22TypedDict(TypedDict):
    r"""An image input content part."""

    type: CreateResponse2RouterResponsesType
    r"""The type of input content part"""
    detail: NotRequired[TwoDetail]
    r"""Level of detail for image analysis"""
    file_id: NotRequired[Nullable[str]]
    r"""File ID for the image"""
    image_url: NotRequired[Nullable[str]]
    r"""URL of the image (can be http URL or data URL)"""


class CreateResponse22(BaseModel):
    r"""An image input content part."""

    type: CreateResponse2RouterResponsesType
    r"""The type of input content part"""

    detail: Optional[TwoDetail] = "auto"
    r"""Level of detail for image analysis"""

    file_id: OptionalNullable[str] = UNSET
    r"""File ID for the image"""

    image_url: OptionalNullable[str] = UNSET
    r"""URL of the image (can be http URL or data URL)"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["detail", "file_id", "image_url"])
        nullable_fields = set(["file_id", "image_url"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateResponse2Type = Literal["input_text",]
r"""The type of input content"""


class CreateResponse2RouterResponses1TypedDict(TypedDict):
    r"""A text input content part"""

    type: CreateResponse2Type
    r"""The type of input content"""
    text: str
    r"""The text content"""


class CreateResponse2RouterResponses1(BaseModel):
    r"""A text input content part"""

    type: CreateResponse2Type
    r"""The type of input content"""

    text: str
    r"""The text content"""


CreateResponseContent2TypedDict = TypeAliasType(
    "CreateResponseContent2TypedDict",
    Union[
        CreateResponse2RouterResponses1TypedDict,
        CreateResponse22TypedDict,
        Two3TypedDict,
    ],
)


CreateResponseContent2 = Annotated[
    Union[
        Annotated[CreateResponse2RouterResponses1, Tag("input_text")],
        Annotated[CreateResponse22, Tag("input_image")],
        Annotated[Two3, Tag("input_file")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


TwoContentTypedDict = TypeAliasType(
    "TwoContentTypedDict", Union[str, List[CreateResponseContent2TypedDict]]
)
r"""The content of the message, either a string or an array of content parts"""


TwoContent = TypeAliasType("TwoContent", Union[str, List[CreateResponseContent2]])
r"""The content of the message, either a string or an array of content parts"""


class CreateResponse21TypedDict(TypedDict):
    r"""Represents a message in the conversation, with a role and content (string or rich content parts)."""

    role: TwoRole
    r"""The role of the message author"""
    content: TwoContentTypedDict
    r"""The content of the message, either a string or an array of content parts"""


class CreateResponse21(BaseModel):
    r"""Represents a message in the conversation, with a role and content (string or rich content parts)."""

    role: TwoRole
    r"""The role of the message author"""

    content: TwoContent
    r"""The content of the message, either a string or an array of content parts"""


Input2TypedDict = TypeAliasType(
    "Input2TypedDict",
    Union[
        CreateResponse21TypedDict,
        CreateResponse2RouterResponses2TypedDict,
        CreateResponse23TypedDict,
    ],
)


Input2 = TypeAliasType(
    "Input2", Union[CreateResponse21, CreateResponse2RouterResponses2, CreateResponse23]
)


CreateResponseInputTypedDict = TypeAliasType(
    "CreateResponseInputTypedDict", Union[str, List[Input2TypedDict]]
)
r"""The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input."""


CreateResponseInput = TypeAliasType("CreateResponseInput", Union[str, List[Input2]])
r"""The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input."""


Include = Literal[
    "code_interpreter_call.outputs",
    "computer_call_output.output.image_url",
    "file_search_call.results",
    "message.input_image.image_url",
    "message.output_text.logprobs",
    "reasoning.encrypted_content",
]


ServiceTier = Literal[
    "auto",
    "default",
    "flex",
    "priority",
]
r"""Specifies the latency tier to use for processing the request. Defaults to \"auto\"."""


CreateResponseToolsRouterResponsesRequestRequestBodyType = Literal["file_search",]
r"""The type of tool"""


Ranker = Literal[
    "auto",
    "default_2024_08_21",
]
r"""The ranking algorithm"""


class RankingOptionsTypedDict(TypedDict):
    r"""Options for ranking search results"""

    ranker: NotRequired[Ranker]
    r"""The ranking algorithm"""
    score_threshold: NotRequired[float]
    r"""Minimum relevance score"""


class RankingOptions(BaseModel):
    r"""Options for ranking search results"""

    ranker: Optional[Ranker] = "auto"
    r"""The ranking algorithm"""

    score_threshold: Optional[float] = 0
    r"""Minimum relevance score"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ranker", "score_threshold"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class Tools3TypedDict(TypedDict):
    r"""Configuration for file search tool"""

    type: CreateResponseToolsRouterResponsesRequestRequestBodyType
    r"""The type of tool"""
    vector_store_ids: NotRequired[List[str]]
    r"""The vector stores to search"""
    max_num_results: NotRequired[int]
    r"""Maximum number of results to return"""
    filters: NotRequired[Any]
    r"""Filters to apply to the search"""
    ranking_options: NotRequired[RankingOptionsTypedDict]
    r"""Options for ranking search results"""


class Tools3(BaseModel):
    r"""Configuration for file search tool"""

    type: CreateResponseToolsRouterResponsesRequestRequestBodyType
    r"""The type of tool"""

    vector_store_ids: Optional[List[str]] = None
    r"""The vector stores to search"""

    max_num_results: Optional[int] = 20
    r"""Maximum number of results to return"""

    filters: Optional[Any] = None
    r"""Filters to apply to the search"""

    ranking_options: Optional[RankingOptions] = None
    r"""Options for ranking search results"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["vector_store_ids", "max_num_results", "filters", "ranking_options"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateResponseToolsRouterResponsesType = Literal["web_search_preview",]
r"""The type of tool"""


SearchContextSize = Literal[
    "small",
    "medium",
    "large",
]
r"""Amount of context to retrieve for each search result"""


CreateResponseToolsRouterResponsesRequestType = Literal[
    "approximate",
    "exact",
]
r"""The type of location"""


class UserLocationTypedDict(TypedDict):
    r"""User location for search localization"""

    type: NotRequired[CreateResponseToolsRouterResponsesRequestType]
    r"""The type of location"""
    city: NotRequired[Nullable[str]]
    r"""The city name"""
    country: NotRequired[str]
    r"""The country code"""
    region: NotRequired[Nullable[str]]
    r"""The region/state"""
    timezone: NotRequired[Nullable[str]]
    r"""The timezone"""


class UserLocation(BaseModel):
    r"""User location for search localization"""

    type: Optional[CreateResponseToolsRouterResponsesRequestType] = None
    r"""The type of location"""

    city: OptionalNullable[str] = UNSET
    r"""The city name"""

    country: Optional[str] = None
    r"""The country code"""

    region: OptionalNullable[str] = UNSET
    r"""The region/state"""

    timezone: OptionalNullable[str] = UNSET
    r"""The timezone"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type", "city", "country", "region", "timezone"])
        nullable_fields = set(["city", "region", "timezone"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class Tools2TypedDict(TypedDict):
    r"""Configuration for web search tool"""

    type: CreateResponseToolsRouterResponsesType
    r"""The type of tool"""
    domains: NotRequired[List[str]]
    r"""List of domains to restrict search to"""
    search_context_size: NotRequired[SearchContextSize]
    r"""Amount of context to retrieve for each search result"""
    user_location: NotRequired[UserLocationTypedDict]
    r"""User location for search localization"""


class Tools2(BaseModel):
    r"""Configuration for web search tool"""

    type: CreateResponseToolsRouterResponsesType
    r"""The type of tool"""

    domains: Optional[List[str]] = None
    r"""List of domains to restrict search to"""

    search_context_size: Optional[SearchContextSize] = "medium"
    r"""Amount of context to retrieve for each search result"""

    user_location: Optional[UserLocation] = None
    r"""User location for search localization"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["domains", "search_context_size", "user_location"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ToolsType = Literal["function",]
r"""The type of tool"""


CreateResponseToolsType = Literal["object",]
r"""The type of the parameters object"""


class PropertiesTypedDict(TypedDict):
    type: str
    description: NotRequired[str]
    enum: NotRequired[List[str]]


class Properties(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: str

    description: Optional[str] = None

    enum: Optional[List[str]] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "enum"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val
        for k, v in serialized.items():
            m[k] = v

        return m


class ToolsParametersTypedDict(TypedDict):
    r"""The parameters the function accepts"""

    type: CreateResponseToolsType
    r"""The type of the parameters object"""
    properties: Dict[str, PropertiesTypedDict]
    r"""The parameters the function accepts, described as a JSON Schema object"""
    required: NotRequired[List[str]]
    r"""List of required parameter names"""
    additional_properties: NotRequired[bool]
    r"""Whether to allow properties not defined in the schema"""


class ToolsParameters(BaseModel):
    r"""The parameters the function accepts"""

    type: CreateResponseToolsType
    r"""The type of the parameters object"""

    properties: Dict[str, Properties]
    r"""The parameters the function accepts, described as a JSON Schema object"""

    required: Optional[List[str]] = None
    r"""List of required parameter names"""

    additional_properties: Annotated[
        Optional[bool], pydantic.Field(alias="additionalProperties")
    ] = None
    r"""Whether to allow properties not defined in the schema"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["required", "additionalProperties"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class Tools1TypedDict(TypedDict):
    r"""A function tool definition"""

    type: ToolsType
    r"""The type of tool"""
    name: str
    r"""The name of the function to be called"""
    parameters: ToolsParametersTypedDict
    r"""The parameters the function accepts"""
    description: NotRequired[Nullable[str]]
    r"""A description of what the function does"""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating function calls"""


class Tools1(BaseModel):
    r"""A function tool definition"""

    type: ToolsType
    r"""The type of tool"""

    name: str
    r"""The name of the function to be called"""

    parameters: ToolsParameters
    r"""The parameters the function accepts"""

    description: OptionalNullable[str] = UNSET
    r"""A description of what the function does"""

    strict: Optional[bool] = True
    r"""Whether to enable strict schema adherence when generating function calls"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "strict"])
        nullable_fields = set(["description"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateResponseToolsTypedDict = TypeAliasType(
    "CreateResponseToolsTypedDict",
    Union[Tools2TypedDict, Tools1TypedDict, Tools3TypedDict],
)


CreateResponseTools = Annotated[
    Union[
        Annotated[Tools1, Tag("function")],
        Annotated[Tools2, Tag("web_search_preview")],
        Annotated[Tools3, Tag("file_search")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


CreateResponseToolChoiceRouterResponsesRequestType = Literal["mcp",]


class ToolChoice4TypedDict(TypedDict):
    type: CreateResponseToolChoiceRouterResponsesRequestType
    server_label: str
    name: NotRequired[Nullable[str]]


class ToolChoice4(BaseModel):
    type: CreateResponseToolChoiceRouterResponsesRequestType

    server_label: str

    name: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        nullable_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateResponseToolChoiceRouterResponsesType = Literal["function",]


class ToolChoice3TypedDict(TypedDict):
    type: CreateResponseToolChoiceRouterResponsesType
    name: str


class ToolChoice3(BaseModel):
    type: CreateResponseToolChoiceRouterResponsesType

    name: str


CreateResponseToolChoiceType = Literal[
    "file_search",
    "web_search_preview",
    "computer_use_preview",
    "code_interpreter",
    "image_generation",
]


class CreateResponseToolChoice2TypedDict(TypedDict):
    type: CreateResponseToolChoiceType


class CreateResponseToolChoice2(BaseModel):
    type: CreateResponseToolChoiceType


CreateResponseToolChoice1 = Literal[
    "none",
    "auto",
    "required",
]
r"""Controls which (if any) tool is called by the model. `none` means the model will not call any tool. `auto` means the model can pick between generating a message or calling a tool. `required` means the model must call a tool."""


CreateResponseToolChoiceTypedDict = TypeAliasType(
    "CreateResponseToolChoiceTypedDict",
    Union[
        CreateResponseToolChoice2TypedDict,
        ToolChoice3TypedDict,
        ToolChoice4TypedDict,
        CreateResponseToolChoice1,
    ],
)
r"""How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool."""


CreateResponseToolChoice = TypeAliasType(
    "CreateResponseToolChoice",
    Union[
        CreateResponseToolChoice2, ToolChoice3, ToolChoice4, CreateResponseToolChoice1
    ],
)
r"""How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool."""


class CreateResponseRequestBodyTypedDict(TypedDict):
    model: str
    r"""ID of the model to use. You can use the List models API to see all of your available models."""
    input: CreateResponseInputTypedDict
    r"""The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input."""
    metadata: NotRequired[Dict[str, str]]
    r"""Developer-defined key-value pairs that will be included in response objects"""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."""
    previous_response_id: NotRequired[Nullable[str]]
    r"""The ID of a previous response to continue the conversation from. The model will have access to the previous response context."""
    instructions: NotRequired[Nullable[str]]
    r"""Developer-provided instructions that the model should follow. Overwrites the default system message."""
    reasoning: NotRequired[Nullable[ReasoningTypedDict]]
    r"""Configuration for reasoning models"""
    max_output_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens that can be generated in the response"""
    text: NotRequired[Nullable[CreateResponseTextTypedDict]]
    include: NotRequired[Nullable[List[Include]]]
    r"""Specifies which (potentially large) fields to include in the response. By default, the results of Code Interpreter and file searches are excluded. Available options:
    - code_interpreter_call.outputs: Include the outputs of Code Interpreter tool calls
    - computer_call_output.output.image_url: Include the image URLs from computer use tool calls
    - file_search_call.results: Include the results of file search tool calls
    - message.input_image.image_url: Include URLs of input images
    - message.output_text.logprobs: Include log probabilities for output text (when logprobs is enabled)
    - reasoning.encrypted_content: Include encrypted reasoning content for reasoning models
    """
    parallel_tool_calls: NotRequired[Nullable[bool]]
    r"""Whether to enable parallel function calling during tool use."""
    store: NotRequired[Nullable[bool]]
    r"""Whether to store this response for use in distillations or evals."""
    service_tier: NotRequired[Nullable[ServiceTier]]
    r"""Specifies the latency tier to use for processing the request. Defaults to \"auto\"."""
    tools: NotRequired[List[CreateResponseToolsTypedDict]]
    r"""A list of tools the model may call. Use this to provide a list of functions the model may generate JSON inputs for."""
    tool_choice: NotRequired[CreateResponseToolChoiceTypedDict]
    r"""How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool."""
    stream: NotRequired[bool]


class CreateResponseRequestBody(BaseModel):
    model: str
    r"""ID of the model to use. You can use the List models API to see all of your available models."""

    input: CreateResponseInput
    r"""The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input."""

    metadata: Optional[Dict[str, str]] = None
    r"""Developer-defined key-value pairs that will be included in response objects"""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."""

    previous_response_id: OptionalNullable[str] = UNSET
    r"""The ID of a previous response to continue the conversation from. The model will have access to the previous response context."""

    instructions: OptionalNullable[str] = UNSET
    r"""Developer-provided instructions that the model should follow. Overwrites the default system message."""

    reasoning: OptionalNullable[Reasoning] = UNSET
    r"""Configuration for reasoning models"""

    max_output_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens that can be generated in the response"""

    text: OptionalNullable[CreateResponseText] = UNSET

    include: OptionalNullable[List[Include]] = UNSET
    r"""Specifies which (potentially large) fields to include in the response. By default, the results of Code Interpreter and file searches are excluded. Available options:
    - code_interpreter_call.outputs: Include the outputs of Code Interpreter tool calls
    - computer_call_output.output.image_url: Include the image URLs from computer use tool calls
    - file_search_call.results: Include the results of file search tool calls
    - message.input_image.image_url: Include URLs of input images
    - message.output_text.logprobs: Include log probabilities for output text (when logprobs is enabled)
    - reasoning.encrypted_content: Include encrypted reasoning content for reasoning models
    """

    parallel_tool_calls: OptionalNullable[bool] = UNSET
    r"""Whether to enable parallel function calling during tool use."""

    store: OptionalNullable[bool] = True
    r"""Whether to store this response for use in distillations or evals."""

    service_tier: OptionalNullable[ServiceTier] = UNSET
    r"""Specifies the latency tier to use for processing the request. Defaults to \"auto\"."""

    tools: Optional[List[CreateResponseTools]] = None
    r"""A list of tools the model may call. Use this to provide a list of functions the model may generate JSON inputs for."""

    tool_choice: Optional[CreateResponseToolChoice] = None
    r"""How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool."""

    stream: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "metadata",
                "temperature",
                "top_p",
                "previous_response_id",
                "instructions",
                "reasoning",
                "max_output_tokens",
                "text",
                "include",
                "parallel_tool_calls",
                "store",
                "service_tier",
                "tools",
                "tool_choice",
                "stream",
            ]
        )
        nullable_fields = set(
            [
                "temperature",
                "top_p",
                "previous_response_id",
                "instructions",
                "reasoning",
                "max_output_tokens",
                "text",
                "include",
                "parallel_tool_calls",
                "store",
                "service_tier",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateResponseDataTypedDict(TypedDict):
    r"""One server-sent event emitted while the response streams"""

    type: str
    r"""The type of streaming event"""


class CreateResponseData(BaseModel):
    r"""One server-sent event emitted while the response streams"""

    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: str
    r"""The type of streaming event"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class CreateResponseRouterResponsesResponseBodyTypedDict(TypedDict):
    r"""One server-sent event emitted while the response streams"""

    data: NotRequired[CreateResponseDataTypedDict]
    r"""One server-sent event emitted while the response streams"""


class CreateResponseRouterResponsesResponseBody(BaseModel):
    r"""One server-sent event emitted while the response streams"""

    data: Optional[CreateResponseData] = None
    r"""One server-sent event emitted while the response streams"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["data"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateResponseObject = Literal["response",]
r"""The object type, which is always \"response\" """


CreateResponseStatus = Literal[
    "completed",
    "failed",
    "in_progress",
    "incomplete",
]
r"""The status of the response"""


class CreateResponseErrorTypedDict(TypedDict):
    r"""The error that occurred, if any"""

    code: str
    r"""The error code"""
    message: str
    r"""The error message"""


class CreateResponseError(BaseModel):
    r"""The error that occurred, if any"""

    code: str
    r"""The error code"""

    message: str
    r"""The error message"""


Reason = Literal[
    "max_output_tokens",
    "content_filter",
]
r"""The reason the response is incomplete"""


class IncompleteDetailsTypedDict(TypedDict):
    r"""Details about why the response is incomplete"""

    reason: Reason
    r"""The reason the response is incomplete"""


class IncompleteDetails(BaseModel):
    r"""Details about why the response is incomplete"""

    reason: Reason
    r"""The reason the response is incomplete"""


CreateResponseOutputRouterResponsesResponse200Type = Literal["function_call",]
r"""The type of output item"""


CreateResponseOutputRouterResponsesResponseStatus = Literal[
    "in_progress",
    "completed",
    "incomplete",
    "failed",
]
r"""The status of the function call"""


class Output4TypedDict(TypedDict):
    r"""A function tool call output"""

    id: str
    r"""The unique identifier for this output item"""
    type: CreateResponseOutputRouterResponsesResponse200Type
    r"""The type of output item"""
    call_id: str
    r"""The ID of the function call"""
    name: str
    r"""The name of the function being called"""
    arguments: str
    r"""The arguments to the function as a JSON string"""
    status: CreateResponseOutputRouterResponsesResponseStatus
    r"""The status of the function call"""


class Output4(BaseModel):
    r"""A function tool call output"""

    id: str
    r"""The unique identifier for this output item"""

    type: CreateResponseOutputRouterResponsesResponse200Type
    r"""The type of output item"""

    call_id: str
    r"""The ID of the function call"""

    name: str
    r"""The name of the function being called"""

    arguments: str
    r"""The arguments to the function as a JSON string"""

    status: CreateResponseOutputRouterResponsesResponseStatus
    r"""The status of the function call"""


CreateResponseOutputRouterResponsesResponseType = Literal["file_search_call",]
r"""The type of output item"""


CreateResponseOutputRouterResponsesStatus = Literal[
    "in_progress",
    "completed",
    "incomplete",
    "failed",
]
r"""The status of the file search"""


class Output3TypedDict(TypedDict):
    r"""A file search tool call output"""

    id: str
    r"""The unique identifier for this output item"""
    type: CreateResponseOutputRouterResponsesResponseType
    r"""The type of output item"""
    status: CreateResponseOutputRouterResponsesStatus
    r"""The status of the file search"""
    queries: NotRequired[List[str]]
    r"""The search queries used"""
    results: NotRequired[Any]
    r"""The file search results"""


class Output3(BaseModel):
    r"""A file search tool call output"""

    id: str
    r"""The unique identifier for this output item"""

    type: CreateResponseOutputRouterResponsesResponseType
    r"""The type of output item"""

    status: CreateResponseOutputRouterResponsesStatus
    r"""The status of the file search"""

    queries: Optional[List[str]] = None
    r"""The search queries used"""

    results: Optional[Any] = None
    r"""The file search results"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["queries", "results"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateResponseOutputRouterResponsesType = Literal["web_search_call",]
r"""The type of output item"""


CreateResponseOutputStatus = Literal[
    "in_progress",
    "completed",
    "incomplete",
    "failed",
]
r"""The status of the web search"""


class Output2TypedDict(TypedDict):
    r"""A web search tool call output"""

    id: str
    r"""The unique identifier for this output item"""
    type: CreateResponseOutputRouterResponsesType
    r"""The type of output item"""
    status: CreateResponseOutputStatus
    r"""The status of the web search"""


class Output2(BaseModel):
    r"""A web search tool call output"""

    id: str
    r"""The unique identifier for this output item"""

    type: CreateResponseOutputRouterResponsesType
    r"""The type of output item"""

    status: CreateResponseOutputStatus
    r"""The status of the web search"""


CreateResponseOutputType = Literal["message",]
r"""The type of output item"""


OutputRole = Literal["assistant",]
r"""The role of the message author"""


OutputStatus = Literal[
    "in_progress",
    "completed",
    "incomplete",
    "failed",
]
r"""The status of the message"""


ContentType = Literal["output_text",]
r"""The type of content part"""


CreateResponseAnnotationsType = Literal["file_citation",]


class Annotations2TypedDict(TypedDict):
    r"""A citation to a file"""

    type: CreateResponseAnnotationsType
    index: float
    r"""The index in the text where the citation appears"""
    file_id: str
    r"""The ID of the file being cited"""
    filename: str
    r"""The name of the file being cited"""


class Annotations2(BaseModel):
    r"""A citation to a file"""

    type: CreateResponseAnnotationsType

    index: float
    r"""The index in the text where the citation appears"""

    file_id: str
    r"""The ID of the file being cited"""

    filename: str
    r"""The name of the file being cited"""


AnnotationsType = Literal["url_citation",]


class Annotations1TypedDict(TypedDict):
    r"""A citation to a URL"""

    type: AnnotationsType
    start_index: float
    r"""The start index of the citation in the text"""
    end_index: float
    r"""The end index of the citation in the text"""
    url: str
    r"""The URL being cited"""
    title: str
    r"""The title of the cited resource"""


class Annotations1(BaseModel):
    r"""A citation to a URL"""

    type: AnnotationsType

    start_index: float
    r"""The start index of the citation in the text"""

    end_index: float
    r"""The end index of the citation in the text"""

    url: str
    r"""The URL being cited"""

    title: str
    r"""The title of the cited resource"""


AnnotationsTypedDict = TypeAliasType(
    "AnnotationsTypedDict", Union[Annotations2TypedDict, Annotations1TypedDict]
)
r"""An annotation in the output text"""


Annotations = Annotated[
    Union[
        Annotated[Annotations1, Tag("url_citation")],
        Annotated[Annotations2, Tag("file_citation")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An annotation in the output text"""


class Content1TypedDict(TypedDict):
    r"""Text output from the model"""

    type: ContentType
    r"""The type of content part"""
    text: str
    r"""The text content"""
    annotations: NotRequired[List[AnnotationsTypedDict]]
    r"""Annotations in the text such as citations"""
    logprobs: NotRequired[List[Any]]
    r"""Log probabilities of the output tokens if requested"""


class Content1(BaseModel):
    r"""Text output from the model"""

    type: ContentType
    r"""The type of content part"""

    text: str
    r"""The text content"""

    annotations: Optional[List[Annotations]] = None
    r"""Annotations in the text such as citations"""

    logprobs: Optional[List[Any]] = None
    r"""Log probabilities of the output tokens if requested"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["annotations", "logprobs"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


OutputContentTypedDict = Content1TypedDict


OutputContent = Content1


class Output1TypedDict(TypedDict):
    r"""An assistant message output"""

    id: str
    r"""The unique identifier for this message"""
    type: CreateResponseOutputType
    r"""The type of output item"""
    role: OutputRole
    r"""The role of the message author"""
    status: OutputStatus
    r"""The status of the message"""
    content: NotRequired[List[OutputContentTypedDict]]
    r"""The content parts of the message"""


class Output1(BaseModel):
    r"""An assistant message output"""

    id: str
    r"""The unique identifier for this message"""

    type: CreateResponseOutputType
    r"""The type of output item"""

    role: OutputRole
    r"""The role of the message author"""

    status: OutputStatus
    r"""The status of the message"""

    content: Optional[List[OutputContent]] = None
    r"""The content parts of the message"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["content"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


OutputTypedDict = TypeAliasType(
    "OutputTypedDict",
    Union[Output2TypedDict, Output1TypedDict, Output3TypedDict, Output4TypedDict],
)


Output = Annotated[
    Union[
        Annotated[Output1, Tag("message")],
        Annotated[Output2, Tag("web_search_call")],
        Annotated[Output3, Tag("file_search_call")],
        Annotated[Output4, Tag("function_call")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class CreateResponseInputTokensDetailsTypedDict(TypedDict):
    r"""Breakdown of input token usage"""

    cached_tokens: NotRequired[int]
    r"""Number of tokens from cache"""


class CreateResponseInputTokensDetails(BaseModel):
    r"""Breakdown of input token usage"""

    cached_tokens: Optional[int] = None
    r"""Number of tokens from cache"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["cached_tokens"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class OutputTokensDetailsTypedDict(TypedDict):
    r"""Breakdown of output token usage"""

    reasoning_tokens: NotRequired[int]
    r"""Number of tokens used for reasoning (o3 models)"""
    accepted_prediction_tokens: NotRequired[int]
    r"""Number of tokens generated by automatic prediction that were accepted"""
    rejected_prediction_tokens: NotRequired[int]
    r"""Number of tokens generated by automatic prediction that were rejected"""


class OutputTokensDetails(BaseModel):
    r"""Breakdown of output token usage"""

    reasoning_tokens: Optional[int] = None
    r"""Number of tokens used for reasoning (o3 models)"""

    accepted_prediction_tokens: Optional[int] = None
    r"""Number of tokens generated by automatic prediction that were accepted"""

    rejected_prediction_tokens: Optional[int] = None
    r"""Number of tokens generated by automatic prediction that were rejected"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "reasoning_tokens",
                "accepted_prediction_tokens",
                "rejected_prediction_tokens",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateResponseUsageTypedDict(TypedDict):
    r"""Usage statistics for the response"""

    input_tokens: NotRequired[float]
    r"""Number of tokens in the input"""
    output_tokens: NotRequired[float]
    r"""Number of tokens in the generated output"""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (input + output)"""
    input_tokens_details: NotRequired[CreateResponseInputTokensDetailsTypedDict]
    r"""Breakdown of input token usage"""
    output_tokens_details: NotRequired[OutputTokensDetailsTypedDict]
    r"""Breakdown of output token usage"""


class CreateResponseUsage(BaseModel):
    r"""Usage statistics for the response"""

    input_tokens: Optional[float] = None
    r"""Number of tokens in the input"""

    output_tokens: Optional[float] = None
    r"""Number of tokens in the generated output"""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (input + output)"""

    input_tokens_details: Optional[CreateResponseInputTokensDetails] = None
    r"""Breakdown of input token usage"""

    output_tokens_details: Optional[OutputTokensDetails] = None
    r"""Breakdown of output token usage"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "input_tokens",
                "output_tokens",
                "total_tokens",
                "input_tokens_details",
                "output_tokens_details",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateResponseToolChoiceRouterResponsesResponseType = Literal["function",]


class CreateResponseToolChoiceFunctionTypedDict(TypedDict):
    name: str


class CreateResponseToolChoiceFunction(BaseModel):
    name: str


class CreateResponseToolChoiceRouterResponses2TypedDict(TypedDict):
    type: CreateResponseToolChoiceRouterResponsesResponseType
    function: CreateResponseToolChoiceFunctionTypedDict


class CreateResponseToolChoiceRouterResponses2(BaseModel):
    type: CreateResponseToolChoiceRouterResponsesResponseType

    function: CreateResponseToolChoiceFunction


CreateResponseToolChoiceRouterResponses1 = Literal[
    "none",
    "auto",
    "required",
]


CreateResponseRouterResponsesToolChoiceTypedDict = TypeAliasType(
    "CreateResponseRouterResponsesToolChoiceTypedDict",
    Union[
        CreateResponseToolChoiceRouterResponses2TypedDict,
        CreateResponseToolChoiceRouterResponses1,
    ],
)
r"""Controls which (if any) tool is called by the model"""


CreateResponseRouterResponsesToolChoice = TypeAliasType(
    "CreateResponseRouterResponsesToolChoice",
    Union[
        CreateResponseToolChoiceRouterResponses2,
        CreateResponseToolChoiceRouterResponses1,
    ],
)
r"""Controls which (if any) tool is called by the model"""


CreateResponseToolsRouterResponsesResponse200ApplicationJSONType = Literal[
    "file_search",
]
r"""The type of tool"""


ToolsRanker = Literal[
    "auto",
    "default_2024_08_21",
]
r"""The ranking algorithm"""


class ToolsRankingOptionsTypedDict(TypedDict):
    r"""Options for ranking search results"""

    ranker: NotRequired[ToolsRanker]
    r"""The ranking algorithm"""
    score_threshold: NotRequired[float]
    r"""Minimum relevance score"""


class ToolsRankingOptions(BaseModel):
    r"""Options for ranking search results"""

    ranker: Optional[ToolsRanker] = "auto"
    r"""The ranking algorithm"""

    score_threshold: Optional[float] = 0
    r"""Minimum relevance score"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ranker", "score_threshold"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateResponseTools3TypedDict(TypedDict):
    r"""Configuration for file search tool"""

    type: CreateResponseToolsRouterResponsesResponse200ApplicationJSONType
    r"""The type of tool"""
    vector_store_ids: NotRequired[List[str]]
    r"""The vector stores to search"""
    max_num_results: NotRequired[int]
    r"""Maximum number of results to return"""
    filters: NotRequired[Any]
    r"""Filters to apply to the search"""
    ranking_options: NotRequired[ToolsRankingOptionsTypedDict]
    r"""Options for ranking search results"""


class CreateResponseTools3(BaseModel):
    r"""Configuration for file search tool"""

    type: CreateResponseToolsRouterResponsesResponse200ApplicationJSONType
    r"""The type of tool"""

    vector_store_ids: Optional[List[str]] = None
    r"""The vector stores to search"""

    max_num_results: Optional[int] = 20
    r"""Maximum number of results to return"""

    filters: Optional[Any] = None
    r"""Filters to apply to the search"""

    ranking_options: Optional[ToolsRankingOptions] = None
    r"""Options for ranking search results"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["vector_store_ids", "max_num_results", "filters", "ranking_options"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateResponseToolsRouterResponsesResponse200Type = Literal["web_search_preview",]
r"""The type of tool"""


ToolsSearchContextSize = Literal[
    "small",
    "medium",
    "large",
]
r"""Amount of context to retrieve for each search result"""


CreateResponseToolsRouterResponsesResponse200ApplicationJSONResponseBodyType = Literal[
    "approximate",
    "exact",
]
r"""The type of location"""


class ToolsUserLocationTypedDict(TypedDict):
    r"""User location for search localization"""

    type: NotRequired[
        CreateResponseToolsRouterResponsesResponse200ApplicationJSONResponseBodyType
    ]
    r"""The type of location"""
    city: NotRequired[Nullable[str]]
    r"""The city name"""
    country: NotRequired[str]
    r"""The country code"""
    region: NotRequired[Nullable[str]]
    r"""The region/state"""
    timezone: NotRequired[Nullable[str]]
    r"""The timezone"""


class ToolsUserLocation(BaseModel):
    r"""User location for search localization"""

    type: Optional[
        CreateResponseToolsRouterResponsesResponse200ApplicationJSONResponseBodyType
    ] = None
    r"""The type of location"""

    city: OptionalNullable[str] = UNSET
    r"""The city name"""

    country: Optional[str] = None
    r"""The country code"""

    region: OptionalNullable[str] = UNSET
    r"""The region/state"""

    timezone: OptionalNullable[str] = UNSET
    r"""The timezone"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type", "city", "country", "region", "timezone"])
        nullable_fields = set(["city", "region", "timezone"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateResponseTools2TypedDict(TypedDict):
    r"""Configuration for web search tool"""

    type: CreateResponseToolsRouterResponsesResponse200Type
    r"""The type of tool"""
    domains: NotRequired[List[str]]
    r"""List of domains to restrict search to"""
    search_context_size: NotRequired[ToolsSearchContextSize]
    r"""Amount of context to retrieve for each search result"""
    user_location: NotRequired[ToolsUserLocationTypedDict]
    r"""User location for search localization"""


class CreateResponseTools2(BaseModel):
    r"""Configuration for web search tool"""

    type: CreateResponseToolsRouterResponsesResponse200Type
    r"""The type of tool"""

    domains: Optional[List[str]] = None
    r"""List of domains to restrict search to"""

    search_context_size: Optional[ToolsSearchContextSize] = "medium"
    r"""Amount of context to retrieve for each search result"""

    user_location: Optional[ToolsUserLocation] = None
    r"""User location for search localization"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["domains", "search_context_size", "user_location"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateResponseToolsRouterResponsesResponseType = Literal["function",]
r"""The type of tool"""


CreateResponseToolsRouterResponsesResponse200ApplicationJSONResponseBody1Type = Literal[
    "object",
]
r"""The type of the parameters object"""


class ToolsPropertiesTypedDict(TypedDict):
    type: str
    description: NotRequired[str]
    enum: NotRequired[List[str]]


class ToolsProperties(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: str

    description: Optional[str] = None

    enum: Optional[List[str]] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "enum"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val
        for k, v in serialized.items():
            m[k] = v

        return m


class CreateResponseToolsParametersTypedDict(TypedDict):
    r"""The parameters the function accepts"""

    type: CreateResponseToolsRouterResponsesResponse200ApplicationJSONResponseBody1Type
    r"""The type of the parameters object"""
    properties: Dict[str, ToolsPropertiesTypedDict]
    r"""The parameters the function accepts, described as a JSON Schema object"""
    required: NotRequired[List[str]]
    r"""List of required parameter names"""
    additional_properties: NotRequired[bool]
    r"""Whether to allow properties not defined in the schema"""


class CreateResponseToolsParameters(BaseModel):
    r"""The parameters the function accepts"""

    type: CreateResponseToolsRouterResponsesResponse200ApplicationJSONResponseBody1Type
    r"""The type of the parameters object"""

    properties: Dict[str, ToolsProperties]
    r"""The parameters the function accepts, described as a JSON Schema object"""

    required: Optional[List[str]] = None
    r"""List of required parameter names"""

    additional_properties: Annotated[
        Optional[bool], pydantic.Field(alias="additionalProperties")
    ] = None
    r"""Whether to allow properties not defined in the schema"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["required", "additionalProperties"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateResponseTools1TypedDict(TypedDict):
    r"""A function tool definition"""

    type: CreateResponseToolsRouterResponsesResponseType
    r"""The type of tool"""
    name: str
    r"""The name of the function to be called"""
    parameters: CreateResponseToolsParametersTypedDict
    r"""The parameters the function accepts"""
    description: NotRequired[Nullable[str]]
    r"""A description of what the function does"""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating function calls"""


class CreateResponseTools1(BaseModel):
    r"""A function tool definition"""

    type: CreateResponseToolsRouterResponsesResponseType
    r"""The type of tool"""

    name: str
    r"""The name of the function to be called"""

    parameters: CreateResponseToolsParameters
    r"""The parameters the function accepts"""

    description: OptionalNullable[str] = UNSET
    r"""A description of what the function does"""

    strict: Optional[bool] = True
    r"""Whether to enable strict schema adherence when generating function calls"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "strict"])
        nullable_fields = set(["description"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateResponseRouterResponsesToolsTypedDict = TypeAliasType(
    "CreateResponseRouterResponsesToolsTypedDict",
    Union[
        CreateResponseTools2TypedDict,
        CreateResponseTools1TypedDict,
        CreateResponseTools3TypedDict,
    ],
)


CreateResponseRouterResponsesTools = Annotated[
    Union[
        Annotated[CreateResponseTools1, Tag("function")],
        Annotated[CreateResponseTools2, Tag("web_search_preview")],
        Annotated[CreateResponseTools3, Tag("file_search")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class CreateResponseReasoningTypedDict(TypedDict):
    effort: NotRequired[Nullable[str]]
    summary: NotRequired[Nullable[str]]


class CreateResponseReasoning(BaseModel):
    effort: OptionalNullable[str] = UNSET

    summary: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["effort", "summary"])
        nullable_fields = set(["effort", "summary"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateResponseFormatRouterResponsesResponse200ApplicationJSONType = Literal[
    "json_schema",
]
r"""Ensures the response matches a supplied JSON schema"""


class CreateResponseFormat3TypedDict(TypedDict):
    type: CreateResponseFormatRouterResponsesResponse200ApplicationJSONType
    r"""Ensures the response matches a supplied JSON schema"""
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    schema_: Dict[str, Any]
    r"""The JSON schema to validate the response against"""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    strict: NotRequired[bool]
    r"""Whether to enable strict `schema` adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when `strict` is `true`"""


class CreateResponseFormat3(BaseModel):
    type: CreateResponseFormatRouterResponsesResponse200ApplicationJSONType
    r"""Ensures the response matches a supplied JSON schema"""

    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]
    r"""The JSON schema to validate the response against"""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    strict: Optional[bool] = True
    r"""Whether to enable strict `schema` adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when `strict` is `true`"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateResponseFormatRouterResponsesResponse200Type = Literal["json_object",]
r"""Ensures the response is a valid JSON object"""


class CreateResponseFormat2TypedDict(TypedDict):
    type: CreateResponseFormatRouterResponsesResponse200Type
    r"""Ensures the response is a valid JSON object"""


class CreateResponseFormat2(BaseModel):
    type: CreateResponseFormatRouterResponsesResponse200Type
    r"""Ensures the response is a valid JSON object"""


CreateResponseFormatRouterResponsesResponseType = Literal["text",]
r"""Plain text response format"""


class CreateResponseFormat1TypedDict(TypedDict):
    type: CreateResponseFormatRouterResponsesResponseType
    r"""Plain text response format"""


class CreateResponseFormat1(BaseModel):
    type: CreateResponseFormatRouterResponsesResponseType
    r"""Plain text response format"""


CreateResponseRouterResponsesFormatTypedDict = TypeAliasType(
    "CreateResponseRouterResponsesFormatTypedDict",
    Union[
        CreateResponseFormat1TypedDict,
        CreateResponseFormat2TypedDict,
        CreateResponseFormat3TypedDict,
    ],
)


CreateResponseRouterResponsesFormat = Annotated[
    Union[
        Annotated[CreateResponseFormat1, Tag("text")],
        Annotated[CreateResponseFormat2, Tag("json_object")],
        Annotated[CreateResponseFormat3, Tag("json_schema")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class CreateResponseRouterResponsesTextTypedDict(TypedDict):
    format_: CreateResponseRouterResponsesFormatTypedDict


class CreateResponseRouterResponsesText(BaseModel):
    format_: Annotated[
        CreateResponseRouterResponsesFormat, pydantic.Field(alias="format")
    ]


Truncation = Literal[
    "auto",
    "disabled",
]
r"""Controls how the model handles inputs longer than the maximum token length"""


CreateResponseServiceTier = Literal[
    "auto",
    "default",
    "flex",
    "priority",
]
r"""The service tier used for processing the request"""


class CreateResponseResponseBodyTypedDict(TypedDict):
    r"""Represents the completed model response returned when `stream` is false"""

    id: str
    r"""The unique identifier for the response"""
    object: CreateResponseObject
    r"""The object type, which is always \"response\" """
    created_at: float
    r"""The Unix timestamp (in seconds) of when the response was created"""
    status: CreateResponseStatus
    r"""The status of the response"""
    error: Nullable[CreateResponseErrorTypedDict]
    r"""The error that occurred, if any"""
    incomplete_details: Nullable[IncompleteDetailsTypedDict]
    r"""Details about why the response is incomplete"""
    model: str
    r"""The model used to generate the response"""
    output: List[OutputTypedDict]
    r"""The list of output items generated by the model"""
    parallel_tool_calls: bool
    instructions: NotRequired[Nullable[str]]
    r"""The instructions provided for the response"""
    output_text: NotRequired[Nullable[str]]
    r"""A convenience field with the concatenated text from all text content parts"""
    usage: NotRequired[CreateResponseUsageTypedDict]
    r"""Usage statistics for the response"""
    temperature: NotRequired[Nullable[float]]
    top_p: NotRequired[Nullable[float]]
    max_output_tokens: NotRequired[Nullable[int]]
    previous_response_id: NotRequired[Nullable[str]]
    metadata: NotRequired[Dict[str, str]]
    tool_choice: NotRequired[CreateResponseRouterResponsesToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model"""
    tools: NotRequired[List[CreateResponseRouterResponsesToolsTypedDict]]
    reasoning: NotRequired[Nullable[CreateResponseReasoningTypedDict]]
    store: NotRequired[bool]
    text: NotRequired[CreateResponseRouterResponsesTextTypedDict]
    truncation: NotRequired[Nullable[Truncation]]
    r"""Controls how the model handles inputs longer than the maximum token length"""
    user: NotRequired[Nullable[str]]
    r"""A unique identifier representing your end-user"""
    service_tier: NotRequired[Nullable[CreateResponseServiceTier]]
    r"""The service tier used for processing the request"""
    background: NotRequired[Nullable[bool]]
    r"""Whether the response was processed in the background"""
    top_logprobs: NotRequired[Nullable[int]]
    r"""The number of top log probabilities to return for each output token"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens"""


class CreateResponseResponseBody(BaseModel):
    r"""Represents the completed model response returned when `stream` is false"""

    id: str
    r"""The unique identifier for the response"""

    object: CreateResponseObject
    r"""The object type, which is always \"response\" """

    created_at: float
    r"""The Unix timestamp (in seconds) of when the response was created"""

    status: CreateResponseStatus
    r"""The status of the response"""

    error: Nullable[CreateResponseError]
    r"""The error that occurred, if any"""

    incomplete_details: Nullable[IncompleteDetails]
    r"""Details about why the response is incomplete"""

    model: str
    r"""The model used to generate the response"""

    output: List[Output]
    r"""The list of output items generated by the model"""

    parallel_tool_calls: bool

    instructions: OptionalNullable[str] = UNSET
    r"""The instructions provided for the response"""

    output_text: OptionalNullable[str] = UNSET
    r"""A convenience field with the concatenated text from all text content parts"""

    usage: Optional[CreateResponseUsage] = None
    r"""Usage statistics for the response"""

    temperature: OptionalNullable[float] = UNSET

    top_p: OptionalNullable[float] = UNSET

    max_output_tokens: OptionalNullable[int] = UNSET

    previous_response_id: OptionalNullable[str] = UNSET

    metadata: Optional[Dict[str, str]] = None

    tool_choice: Optional[CreateResponseRouterResponsesToolChoice] = None
    r"""Controls which (if any) tool is called by the model"""

    tools: Optional[List[CreateResponseRouterResponsesTools]] = None

    reasoning: OptionalNullable[CreateResponseReasoning] = UNSET

    store: Optional[bool] = None

    text: Optional[CreateResponseRouterResponsesText] = None

    truncation: OptionalNullable[Truncation] = "disabled"
    r"""Controls how the model handles inputs longer than the maximum token length"""

    user: OptionalNullable[str] = UNSET
    r"""A unique identifier representing your end-user"""

    service_tier: OptionalNullable[CreateResponseServiceTier] = UNSET
    r"""The service tier used for processing the request"""

    background: OptionalNullable[bool] = UNSET
    r"""Whether the response was processed in the background"""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""The number of top log probabilities to return for each output token"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "instructions",
                "output_text",
                "usage",
                "temperature",
                "top_p",
                "max_output_tokens",
                "previous_response_id",
                "metadata",
                "tool_choice",
                "tools",
                "reasoning",
                "store",
                "text",
                "truncation",
                "user",
                "service_tier",
                "background",
                "top_logprobs",
                "logprobs",
            ]
        )
        nullable_fields = set(
            [
                "error",
                "incomplete_details",
                "instructions",
                "output_text",
                "temperature",
                "top_p",
                "max_output_tokens",
                "previous_response_id",
                "reasoning",
                "truncation",
                "user",
                "service_tier",
                "background",
                "top_logprobs",
                "logprobs",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateResponseResponseTypedDict = TypeAliasType(
    "CreateResponseResponseTypedDict",
    Union[
        CreateResponseResponseBodyTypedDict,
        Union[
            eventstreaming.EventStream[
                CreateResponseRouterResponsesResponseBodyTypedDict
            ],
            eventstreaming.EventStreamAsync[
                CreateResponseRouterResponsesResponseBodyTypedDict
            ],
        ],
    ],
)


CreateResponseResponse = TypeAliasType(
    "CreateResponseResponse",
    Union[
        CreateResponseResponseBody,
        Union[
            eventstreaming.EventStream[CreateResponseRouterResponsesResponseBody],
            eventstreaming.EventStreamAsync[CreateResponseRouterResponsesResponseBody],
        ],
    ],
)


try:
    Format3.model_rebuild()
except NameError:
    pass
try:
    CreateResponseText.model_rebuild()
except NameError:
    pass
try:
    ToolsParameters.model_rebuild()
except NameError:
    pass
try:
    CreateResponseToolsParameters.model_rebuild()
except NameError:
    pass
try:
    CreateResponseFormat3.model_rebuild()
except NameError:
    pass
try:
    CreateResponseRouterResponsesText.model_rebuild()
except NameError:
    pass
