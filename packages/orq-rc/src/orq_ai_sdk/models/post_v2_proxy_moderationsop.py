"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
import httpx
from orq_ai_sdk.models import OrqError
from orq_ai_sdk.types import BaseModel, Nullable, UNSET_SENTINEL
import pydantic
from pydantic import model_serializer
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


InputTypedDict = TypeAliasType("InputTypedDict", Union[str, List[str]])
r"""Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models."""


Input = TypeAliasType("Input", Union[str, List[str]])
r"""Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models."""


class PostV2ProxyModerationsRequestBodyTypedDict(TypedDict):
    r"""Classifies if text violates content policy"""

    input: InputTypedDict
    r"""Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models."""
    model: NotRequired[str]
    r"""The content moderation model you would like to use. Defaults to omni-moderation-latest"""


class PostV2ProxyModerationsRequestBody(BaseModel):
    r"""Classifies if text violates content policy"""

    input: Input
    r"""Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models."""

    model: Optional[str] = None
    r"""The content moderation model you would like to use. Defaults to omni-moderation-latest"""


class PostV2ProxyModerationsErrorTypedDict(TypedDict):
    message: str
    type: str
    param: Nullable[str]
    code: str


class PostV2ProxyModerationsError(BaseModel):
    message: str

    type: str

    param: Nullable[str]

    code: str

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["param"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyModerationsProxyResponseBodyData(BaseModel):
    error: PostV2ProxyModerationsError


class PostV2ProxyModerationsProxyResponseBody(OrqError):
    r"""Returns validation error"""

    data: PostV2ProxyModerationsProxyResponseBodyData

    def __init__(
        self,
        data: PostV2ProxyModerationsProxyResponseBodyData,
        raw_response: httpx.Response,
        body: Optional[str] = None,
    ):
        fallback = body or raw_response.text
        message = str(data.error.message) or fallback
        super().__init__(message, raw_response, body)
        self.data = data


class CategoriesTypedDict(TypedDict):
    r"""A list of the categories, and whether they are flagged or not"""

    hate: bool
    r"""Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste."""
    hate_threatening: bool
    r"""Hateful content that also includes violence or serious harm towards the targeted group."""
    harassment: bool
    r"""Content that expresses, incites, or promotes harassing language towards any target."""
    harassment_threatening: bool
    r"""Harassment content that also includes violence or serious harm towards any target."""
    illicit: bool
    r"""Content that includes instructions or advice that facilitate the planning or execution of wrongdoing."""
    illicit_violent: bool
    r"""Content that includes instructions or advice that facilitate the planning or execution of wrongdoing that also includes violence."""
    self_harm: bool
    r"""Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders."""
    self_harm_intent: bool
    r"""Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm."""
    self_harm_instructions: bool
    r"""Content that encourages performing acts of self-harm, or that gives instructions or advice on how to commit such acts."""
    sexual: bool
    r"""Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services."""
    sexual_minors: bool
    r"""Sexual content that includes an individual who is under 18 years old."""
    violence: bool
    r"""Content that depicts death, violence, or physical injury."""
    violence_graphic: bool
    r"""Content that depicts death, violence, or physical injury in graphic detail."""


class Categories(BaseModel):
    r"""A list of the categories, and whether they are flagged or not"""

    hate: bool
    r"""Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste."""

    hate_threatening: Annotated[bool, pydantic.Field(alias="hate/threatening")]
    r"""Hateful content that also includes violence or serious harm towards the targeted group."""

    harassment: bool
    r"""Content that expresses, incites, or promotes harassing language towards any target."""

    harassment_threatening: Annotated[
        bool, pydantic.Field(alias="harassment/threatening")
    ]
    r"""Harassment content that also includes violence or serious harm towards any target."""

    illicit: bool
    r"""Content that includes instructions or advice that facilitate the planning or execution of wrongdoing."""

    illicit_violent: Annotated[bool, pydantic.Field(alias="illicit/violent")]
    r"""Content that includes instructions or advice that facilitate the planning or execution of wrongdoing that also includes violence."""

    self_harm: Annotated[bool, pydantic.Field(alias="self-harm")]
    r"""Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders."""

    self_harm_intent: Annotated[bool, pydantic.Field(alias="self-harm/intent")]
    r"""Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm."""

    self_harm_instructions: Annotated[
        bool, pydantic.Field(alias="self-harm/instructions")
    ]
    r"""Content that encourages performing acts of self-harm, or that gives instructions or advice on how to commit such acts."""

    sexual: bool
    r"""Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services."""

    sexual_minors: Annotated[bool, pydantic.Field(alias="sexual/minors")]
    r"""Sexual content that includes an individual who is under 18 years old."""

    violence: bool
    r"""Content that depicts death, violence, or physical injury."""

    violence_graphic: Annotated[bool, pydantic.Field(alias="violence/graphic")]
    r"""Content that depicts death, violence, or physical injury in graphic detail."""


class CategoryScoresTypedDict(TypedDict):
    r"""A list of the categories along with their scores as predicted by model"""

    hate: float
    r"""The score for the category hate"""
    hate_threatening: float
    r"""The score for the category hate/threatening"""
    harassment: float
    r"""The score for the category harassment"""
    harassment_threatening: float
    r"""The score for the category harassment/threatening"""
    illicit: float
    r"""The score for the category illicit"""
    illicit_violent: float
    r"""The score for the category illicit/violent"""
    self_harm: float
    r"""The score for the category self-harm"""
    self_harm_intent: float
    r"""The score for the category self-harm/intent"""
    self_harm_instructions: float
    r"""The score for the category self-harm/instructions"""
    sexual: float
    r"""The score for the category sexual"""
    sexual_minors: float
    r"""The score for the category sexual/minors"""
    violence: float
    r"""The score for the category violence"""
    violence_graphic: float
    r"""The score for the category violence/graphic"""


class CategoryScores(BaseModel):
    r"""A list of the categories along with their scores as predicted by model"""

    hate: float
    r"""The score for the category hate"""

    hate_threatening: Annotated[float, pydantic.Field(alias="hate/threatening")]
    r"""The score for the category hate/threatening"""

    harassment: float
    r"""The score for the category harassment"""

    harassment_threatening: Annotated[
        float, pydantic.Field(alias="harassment/threatening")
    ]
    r"""The score for the category harassment/threatening"""

    illicit: float
    r"""The score for the category illicit"""

    illicit_violent: Annotated[float, pydantic.Field(alias="illicit/violent")]
    r"""The score for the category illicit/violent"""

    self_harm: Annotated[float, pydantic.Field(alias="self-harm")]
    r"""The score for the category self-harm"""

    self_harm_intent: Annotated[float, pydantic.Field(alias="self-harm/intent")]
    r"""The score for the category self-harm/intent"""

    self_harm_instructions: Annotated[
        float, pydantic.Field(alias="self-harm/instructions")
    ]
    r"""The score for the category self-harm/instructions"""

    sexual: float
    r"""The score for the category sexual"""

    sexual_minors: Annotated[float, pydantic.Field(alias="sexual/minors")]
    r"""The score for the category sexual/minors"""

    violence: float
    r"""The score for the category violence"""

    violence_graphic: Annotated[float, pydantic.Field(alias="violence/graphic")]
    r"""The score for the category violence/graphic"""


class CategoryAppliedInputTypesTypedDict(TypedDict):
    r"""A list of the categories along with the input type(s) that the score applies to"""

    hate: List[str]
    r"""The applied input type(s) for the category hate"""
    hate_threatening: List[str]
    r"""The applied input type(s) for the category hate/threatening"""
    harassment: List[str]
    r"""The applied input type(s) for the category harassment"""
    harassment_threatening: List[str]
    r"""The applied input type(s) for the category harassment/threatening"""
    illicit: List[str]
    r"""The applied input type(s) for the category illicit"""
    illicit_violent: List[str]
    r"""The applied input type(s) for the category illicit/violent"""
    self_harm: List[str]
    r"""The applied input type(s) for the category self-harm"""
    self_harm_intent: List[str]
    r"""The applied input type(s) for the category self-harm/intent"""
    self_harm_instructions: List[str]
    r"""The applied input type(s) for the category self-harm/instructions"""
    sexual: List[str]
    r"""The applied input type(s) for the category sexual"""
    sexual_minors: List[str]
    r"""The applied input type(s) for the category sexual/minors"""
    violence: List[str]
    r"""The applied input type(s) for the category violence"""
    violence_graphic: List[str]
    r"""The applied input type(s) for the category violence/graphic"""


class CategoryAppliedInputTypes(BaseModel):
    r"""A list of the categories along with the input type(s) that the score applies to"""

    hate: List[str]
    r"""The applied input type(s) for the category hate"""

    hate_threatening: Annotated[List[str], pydantic.Field(alias="hate/threatening")]
    r"""The applied input type(s) for the category hate/threatening"""

    harassment: List[str]
    r"""The applied input type(s) for the category harassment"""

    harassment_threatening: Annotated[
        List[str], pydantic.Field(alias="harassment/threatening")
    ]
    r"""The applied input type(s) for the category harassment/threatening"""

    illicit: List[str]
    r"""The applied input type(s) for the category illicit"""

    illicit_violent: Annotated[List[str], pydantic.Field(alias="illicit/violent")]
    r"""The applied input type(s) for the category illicit/violent"""

    self_harm: Annotated[List[str], pydantic.Field(alias="self-harm")]
    r"""The applied input type(s) for the category self-harm"""

    self_harm_intent: Annotated[List[str], pydantic.Field(alias="self-harm/intent")]
    r"""The applied input type(s) for the category self-harm/intent"""

    self_harm_instructions: Annotated[
        List[str], pydantic.Field(alias="self-harm/instructions")
    ]
    r"""The applied input type(s) for the category self-harm/instructions"""

    sexual: List[str]
    r"""The applied input type(s) for the category sexual"""

    sexual_minors: Annotated[List[str], pydantic.Field(alias="sexual/minors")]
    r"""The applied input type(s) for the category sexual/minors"""

    violence: List[str]
    r"""The applied input type(s) for the category violence"""

    violence_graphic: Annotated[List[str], pydantic.Field(alias="violence/graphic")]
    r"""The applied input type(s) for the category violence/graphic"""


class ResultsTypedDict(TypedDict):
    flagged: bool
    r"""Whether any of the categories are flagged"""
    categories: CategoriesTypedDict
    r"""A list of the categories, and whether they are flagged or not"""
    category_scores: CategoryScoresTypedDict
    r"""A list of the categories along with their scores as predicted by model"""
    category_applied_input_types: NotRequired[CategoryAppliedInputTypesTypedDict]
    r"""A list of the categories along with the input type(s) that the score applies to"""


class Results(BaseModel):
    flagged: bool
    r"""Whether any of the categories are flagged"""

    categories: Categories
    r"""A list of the categories, and whether they are flagged or not"""

    category_scores: CategoryScores
    r"""A list of the categories along with their scores as predicted by model"""

    category_applied_input_types: Optional[CategoryAppliedInputTypes] = None
    r"""A list of the categories along with the input type(s) that the score applies to"""


class PostV2ProxyModerationsResponseBodyTypedDict(TypedDict):
    r"""Returns moderation classification results"""

    id: str
    r"""The unique identifier for the moderation request"""
    model: str
    r"""The model used to generate the moderation results"""
    results: List[ResultsTypedDict]
    r"""A list of moderation objects"""


class PostV2ProxyModerationsResponseBody(BaseModel):
    r"""Returns moderation classification results"""

    id: str
    r"""The unique identifier for the moderation request"""

    model: str
    r"""The model used to generate the moderation results"""

    results: List[Results]
    r"""A list of moderation objects"""
