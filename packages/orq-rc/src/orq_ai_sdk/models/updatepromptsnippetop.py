"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
import dateutil.parser
from orq_ai_sdk import utils
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import (
    FieldMetadata,
    HeaderMetadata,
    PathParamMetadata,
    RequestMetadata,
)
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class UpdatePromptSnippetGlobalsTypedDict(TypedDict):
    contact_id: NotRequired[str]


class UpdatePromptSnippetGlobals(BaseModel):
    contact_id: Annotated[
        Optional[str],
        pydantic.Field(alias="contactId"),
        FieldMetadata(header=HeaderMetadata(style="simple", explode=False)),
    ] = None


UpdatePromptSnippetModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

UpdatePromptSnippetFormat = Literal["url", "b64_json", "text", "json_object"]
r"""Only supported on `image` models."""

UpdatePromptSnippetQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

UpdatePromptSnippetResponseFormatPromptSnippetsType = Literal["json_object"]


class UpdatePromptSnippetResponseFormat2TypedDict(TypedDict):
    type: UpdatePromptSnippetResponseFormatPromptSnippetsType


class UpdatePromptSnippetResponseFormat2(BaseModel):
    type: UpdatePromptSnippetResponseFormatPromptSnippetsType


UpdatePromptSnippetResponseFormatType = Literal["json_schema"]


class ResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class ResponseFormatJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class UpdatePromptSnippetResponseFormat1TypedDict(TypedDict):
    type: UpdatePromptSnippetResponseFormatType
    json_schema: ResponseFormatJSONSchemaTypedDict


class UpdatePromptSnippetResponseFormat1(BaseModel):
    type: UpdatePromptSnippetResponseFormatType

    json_schema: ResponseFormatJSONSchema


UpdatePromptSnippetResponseFormatTypedDict = TypeAliasType(
    "UpdatePromptSnippetResponseFormatTypedDict",
    Union[
        UpdatePromptSnippetResponseFormat2TypedDict,
        UpdatePromptSnippetResponseFormat1TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


UpdatePromptSnippetResponseFormat = TypeAliasType(
    "UpdatePromptSnippetResponseFormat",
    Union[UpdatePromptSnippetResponseFormat2, UpdatePromptSnippetResponseFormat1],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


UpdatePromptSnippetPhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

UpdatePromptSnippetEncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""

UpdatePromptSnippetReasoningEffort = Literal["low", "medium", "high"]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class UpdatePromptSnippetModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[UpdatePromptSnippetFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[UpdatePromptSnippetQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[Nullable[UpdatePromptSnippetResponseFormatTypedDict]]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[UpdatePromptSnippetPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[UpdatePromptSnippetEncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[UpdatePromptSnippetReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class UpdatePromptSnippetModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[
        Optional[UpdatePromptSnippetFormat], pydantic.Field(alias="format")
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[UpdatePromptSnippetQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[UpdatePromptSnippetResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[UpdatePromptSnippetPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[UpdatePromptSnippetEncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[UpdatePromptSnippetReasoningEffort],
        pydantic.Field(alias="reasoningEffort"),
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdatePromptSnippetProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
]

UpdatePromptSnippetRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

UpdatePromptSnippet2PromptSnippetsType = Literal["image_url"]


class UpdatePromptSnippet2ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class UpdatePromptSnippet2ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class UpdatePromptSnippet22TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: UpdatePromptSnippet2PromptSnippetsType
    image_url: UpdatePromptSnippet2ImageURLTypedDict


class UpdatePromptSnippet22(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: UpdatePromptSnippet2PromptSnippetsType

    image_url: UpdatePromptSnippet2ImageURL


UpdatePromptSnippet2Type = Literal["text"]


class UpdatePromptSnippet21TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: UpdatePromptSnippet2Type
    text: str


class UpdatePromptSnippet21(BaseModel):
    r"""Text content part of a prompt message"""

    type: UpdatePromptSnippet2Type

    text: str


UpdatePromptSnippetContent2TypedDict = TypeAliasType(
    "UpdatePromptSnippetContent2TypedDict",
    Union[UpdatePromptSnippet21TypedDict, UpdatePromptSnippet22TypedDict],
)


UpdatePromptSnippetContent2 = TypeAliasType(
    "UpdatePromptSnippetContent2", Union[UpdatePromptSnippet21, UpdatePromptSnippet22]
)


UpdatePromptSnippetContentTypedDict = TypeAliasType(
    "UpdatePromptSnippetContentTypedDict",
    Union[str, List[UpdatePromptSnippetContent2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


UpdatePromptSnippetContent = TypeAliasType(
    "UpdatePromptSnippetContent", Union[str, List[UpdatePromptSnippetContent2]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


UpdatePromptSnippetType = Literal["function"]


class UpdatePromptSnippetFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class UpdatePromptSnippetFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class UpdatePromptSnippetToolCallsTypedDict(TypedDict):
    type: UpdatePromptSnippetType
    function: UpdatePromptSnippetFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class UpdatePromptSnippetToolCalls(BaseModel):
    type: UpdatePromptSnippetType

    function: UpdatePromptSnippetFunction

    id: Optional[str] = None

    index: Optional[float] = None


class UpdatePromptSnippetMessagesTypedDict(TypedDict):
    role: UpdatePromptSnippetRole
    r"""The role of the prompt message"""
    content: UpdatePromptSnippetContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[UpdatePromptSnippetToolCallsTypedDict]]


class UpdatePromptSnippetMessages(BaseModel):
    role: UpdatePromptSnippetRole
    r"""The role of the prompt message"""

    content: UpdatePromptSnippetContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[UpdatePromptSnippetToolCalls]] = None


class UpdatePromptSnippetPromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[UpdatePromptSnippetMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_type: NotRequired[UpdatePromptSnippetModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[UpdatePromptSnippetModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[UpdatePromptSnippetProvider]
    version: NotRequired[str]


class UpdatePromptSnippetPromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[UpdatePromptSnippetMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_type: Optional[UpdatePromptSnippetModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[UpdatePromptSnippetModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[UpdatePromptSnippetProvider] = None

    version: Optional[str] = None


UpdatePromptSnippetUseCases = Literal[
    "Agents",
    "Agents simulations",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Documents QA",
    "Conversation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "SQL",
    "Summarization",
    "Tagging",
]

UpdatePromptSnippetLanguage = Literal[
    "Chinese", "Dutch", "English", "French", "German", "Russian", "Spanish"
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptSnippetMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[UpdatePromptSnippetUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[UpdatePromptSnippetLanguage]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptSnippetMetadata(BaseModel):
    use_cases: Optional[List[UpdatePromptSnippetUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: Optional[UpdatePromptSnippetLanguage] = None
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptSnippetRequestBodyTypedDict(TypedDict):
    description: NotRequired[Nullable[str]]
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""
    prompt_config: NotRequired[UpdatePromptSnippetPromptConfigTypedDict]
    r"""A list of messages compatible with the openAI schema"""
    metadata: NotRequired[UpdatePromptSnippetMetadataTypedDict]
    path: NotRequired[str]
    r"""The path where the entity is stored in the project structure. The first element of the path always represents the project name. Any subsequent path element after the project will be created as a folder in the project if it does not exists."""


class UpdatePromptSnippetRequestBody(BaseModel):
    description: OptionalNullable[str] = UNSET
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    prompt_config: Optional[UpdatePromptSnippetPromptConfig] = None
    r"""A list of messages compatible with the openAI schema"""

    metadata: Optional[UpdatePromptSnippetMetadata] = None

    path: Optional[str] = None
    r"""The path where the entity is stored in the project structure. The first element of the path always represents the project name. Any subsequent path element after the project will be created as a folder in the project if it does not exists."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "prompt_config", "metadata", "path"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class UpdatePromptSnippetRequestTypedDict(TypedDict):
    id: str
    r"""Prompt ID"""
    request_body: NotRequired[UpdatePromptSnippetRequestBodyTypedDict]


class UpdatePromptSnippetRequest(BaseModel):
    id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""Prompt ID"""

    request_body: Annotated[
        Optional[UpdatePromptSnippetRequestBody],
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ] = None


class UpdatePromptSnippetPromptSnippetsResponseBodyData(BaseModel):
    message: str


class UpdatePromptSnippetPromptSnippetsResponseBody(Exception):
    r"""Prompt snippet not found."""

    data: UpdatePromptSnippetPromptSnippetsResponseBodyData

    def __init__(self, data: UpdatePromptSnippetPromptSnippetsResponseBodyData):
        self.data = data

    def __str__(self) -> str:
        return utils.marshal_json(
            self.data, UpdatePromptSnippetPromptSnippetsResponseBodyData
        )


UpdatePromptSnippetOwner2 = Literal["vendor"]

UpdatePromptSnippetOwnerTypedDict = TypeAliasType(
    "UpdatePromptSnippetOwnerTypedDict", Union[str, UpdatePromptSnippetOwner2]
)


UpdatePromptSnippetOwner = TypeAliasType(
    "UpdatePromptSnippetOwner", Union[str, UpdatePromptSnippetOwner2]
)


UpdatePromptSnippetPromptSnippetsModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

UpdatePromptSnippetPromptSnippetsFormat = Literal[
    "url", "b64_json", "text", "json_object"
]
r"""Only supported on `image` models."""

UpdatePromptSnippetPromptSnippetsQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

UpdatePromptSnippetResponseFormatPromptSnippetsResponse200Type = Literal["json_object"]


class UpdatePromptSnippetResponseFormatPromptSnippets2TypedDict(TypedDict):
    type: UpdatePromptSnippetResponseFormatPromptSnippetsResponse200Type


class UpdatePromptSnippetResponseFormatPromptSnippets2(BaseModel):
    type: UpdatePromptSnippetResponseFormatPromptSnippetsResponse200Type


UpdatePromptSnippetResponseFormatPromptSnippetsResponseType = Literal["json_schema"]


class UpdatePromptSnippetResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class UpdatePromptSnippetResponseFormatJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class UpdatePromptSnippetResponseFormatPromptSnippets1TypedDict(TypedDict):
    type: UpdatePromptSnippetResponseFormatPromptSnippetsResponseType
    json_schema: UpdatePromptSnippetResponseFormatJSONSchemaTypedDict


class UpdatePromptSnippetResponseFormatPromptSnippets1(BaseModel):
    type: UpdatePromptSnippetResponseFormatPromptSnippetsResponseType

    json_schema: UpdatePromptSnippetResponseFormatJSONSchema


UpdatePromptSnippetPromptSnippetsResponseFormatTypedDict = TypeAliasType(
    "UpdatePromptSnippetPromptSnippetsResponseFormatTypedDict",
    Union[
        UpdatePromptSnippetResponseFormatPromptSnippets2TypedDict,
        UpdatePromptSnippetResponseFormatPromptSnippets1TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


UpdatePromptSnippetPromptSnippetsResponseFormat = TypeAliasType(
    "UpdatePromptSnippetPromptSnippetsResponseFormat",
    Union[
        UpdatePromptSnippetResponseFormatPromptSnippets2,
        UpdatePromptSnippetResponseFormatPromptSnippets1,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


UpdatePromptSnippetPromptSnippetsPhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

UpdatePromptSnippetPromptSnippetsEncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""

UpdatePromptSnippetPromptSnippetsReasoningEffort = Literal["low", "medium", "high"]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class UpdatePromptSnippetPromptSnippetsModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[UpdatePromptSnippetPromptSnippetsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[UpdatePromptSnippetPromptSnippetsQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[UpdatePromptSnippetPromptSnippetsResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[UpdatePromptSnippetPromptSnippetsPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[UpdatePromptSnippetPromptSnippetsEncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[UpdatePromptSnippetPromptSnippetsReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class UpdatePromptSnippetPromptSnippetsModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[
        Optional[UpdatePromptSnippetPromptSnippetsFormat],
        pydantic.Field(alias="format"),
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[UpdatePromptSnippetPromptSnippetsQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[UpdatePromptSnippetPromptSnippetsResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[UpdatePromptSnippetPromptSnippetsPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[UpdatePromptSnippetPromptSnippetsEncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[UpdatePromptSnippetPromptSnippetsReasoningEffort],
        pydantic.Field(alias="reasoningEffort"),
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdatePromptSnippetPromptSnippetsProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
]

UpdatePromptSnippetPromptSnippetsRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

UpdatePromptSnippet2PromptSnippetsResponse200Type = Literal["image_url"]


class UpdatePromptSnippet2PromptSnippetsImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class UpdatePromptSnippet2PromptSnippetsImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class UpdatePromptSnippet2PromptSnippets2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: UpdatePromptSnippet2PromptSnippetsResponse200Type
    image_url: UpdatePromptSnippet2PromptSnippetsImageURLTypedDict


class UpdatePromptSnippet2PromptSnippets2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: UpdatePromptSnippet2PromptSnippetsResponse200Type

    image_url: UpdatePromptSnippet2PromptSnippetsImageURL


UpdatePromptSnippet2PromptSnippetsResponseType = Literal["text"]


class UpdatePromptSnippet2PromptSnippets1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: UpdatePromptSnippet2PromptSnippetsResponseType
    text: str


class UpdatePromptSnippet2PromptSnippets1(BaseModel):
    r"""Text content part of a prompt message"""

    type: UpdatePromptSnippet2PromptSnippetsResponseType

    text: str


UpdatePromptSnippetContentPromptSnippets2TypedDict = TypeAliasType(
    "UpdatePromptSnippetContentPromptSnippets2TypedDict",
    Union[
        UpdatePromptSnippet2PromptSnippets1TypedDict,
        UpdatePromptSnippet2PromptSnippets2TypedDict,
    ],
)


UpdatePromptSnippetContentPromptSnippets2 = TypeAliasType(
    "UpdatePromptSnippetContentPromptSnippets2",
    Union[UpdatePromptSnippet2PromptSnippets1, UpdatePromptSnippet2PromptSnippets2],
)


UpdatePromptSnippetPromptSnippetsContentTypedDict = TypeAliasType(
    "UpdatePromptSnippetPromptSnippetsContentTypedDict",
    Union[str, List[UpdatePromptSnippetContentPromptSnippets2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


UpdatePromptSnippetPromptSnippetsContent = TypeAliasType(
    "UpdatePromptSnippetPromptSnippetsContent",
    Union[str, List[UpdatePromptSnippetContentPromptSnippets2]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


UpdatePromptSnippetPromptSnippetsResponseType = Literal["function"]


class UpdatePromptSnippetPromptSnippetsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class UpdatePromptSnippetPromptSnippetsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class UpdatePromptSnippetPromptSnippetsToolCallsTypedDict(TypedDict):
    type: UpdatePromptSnippetPromptSnippetsResponseType
    function: UpdatePromptSnippetPromptSnippetsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class UpdatePromptSnippetPromptSnippetsToolCalls(BaseModel):
    type: UpdatePromptSnippetPromptSnippetsResponseType

    function: UpdatePromptSnippetPromptSnippetsFunction

    id: Optional[str] = None

    index: Optional[float] = None


class UpdatePromptSnippetPromptSnippetsMessagesTypedDict(TypedDict):
    role: UpdatePromptSnippetPromptSnippetsRole
    r"""The role of the prompt message"""
    content: UpdatePromptSnippetPromptSnippetsContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[UpdatePromptSnippetPromptSnippetsToolCallsTypedDict]]


class UpdatePromptSnippetPromptSnippetsMessages(BaseModel):
    role: UpdatePromptSnippetPromptSnippetsRole
    r"""The role of the prompt message"""

    content: UpdatePromptSnippetPromptSnippetsContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[UpdatePromptSnippetPromptSnippetsToolCalls]] = None


class UpdatePromptSnippetPromptSnippetsPromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[UpdatePromptSnippetPromptSnippetsMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[UpdatePromptSnippetPromptSnippetsModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[
        UpdatePromptSnippetPromptSnippetsModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[UpdatePromptSnippetPromptSnippetsProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class UpdatePromptSnippetPromptSnippetsPromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[UpdatePromptSnippetPromptSnippetsMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[UpdatePromptSnippetPromptSnippetsModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[UpdatePromptSnippetPromptSnippetsModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[UpdatePromptSnippetPromptSnippetsProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdatePromptSnippetPromptSnippetsUseCases = Literal[
    "Agents",
    "Agents simulations",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Documents QA",
    "Conversation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "SQL",
    "Summarization",
    "Tagging",
]

UpdatePromptSnippetPromptSnippetsLanguage = Literal[
    "Chinese", "Dutch", "English", "French", "German", "Russian", "Spanish"
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptSnippetPromptSnippetsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[UpdatePromptSnippetPromptSnippetsUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[UpdatePromptSnippetPromptSnippetsLanguage]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptSnippetPromptSnippetsMetadata(BaseModel):
    use_cases: Optional[List[UpdatePromptSnippetPromptSnippetsUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: Optional[UpdatePromptSnippetPromptSnippetsLanguage] = None
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


UpdatePromptSnippetPromptSnippetsType = Literal["snippet"]

UpdatePromptSnippetPromptSnippetsResponseModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "vision",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderations",
]
r"""The type of the model"""

UpdatePromptSnippetPromptSnippetsResponse200Format = Literal[
    "url", "b64_json", "text", "json_object"
]
r"""Only supported on `image` models."""

UpdatePromptSnippetPromptSnippetsResponseQuality = Literal["standard", "hd"]
r"""Only supported on `image` models."""

UpdatePromptSnippetResponseFormatPromptSnippetsResponse200ApplicationJSONResponseBodyType = Literal[
    "json_object"
]


class UpdatePromptSnippetResponseFormatPromptSnippetsResponse2TypedDict(TypedDict):
    type: UpdatePromptSnippetResponseFormatPromptSnippetsResponse200ApplicationJSONResponseBodyType


class UpdatePromptSnippetResponseFormatPromptSnippetsResponse2(BaseModel):
    type: UpdatePromptSnippetResponseFormatPromptSnippetsResponse200ApplicationJSONResponseBodyType


UpdatePromptSnippetResponseFormatPromptSnippetsResponse200ApplicationJSONType = Literal[
    "json_schema"
]


class UpdatePromptSnippetResponseFormatPromptSnippetsJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class UpdatePromptSnippetResponseFormatPromptSnippetsJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class UpdatePromptSnippetResponseFormatPromptSnippetsResponse1TypedDict(TypedDict):
    type: UpdatePromptSnippetResponseFormatPromptSnippetsResponse200ApplicationJSONType
    json_schema: UpdatePromptSnippetResponseFormatPromptSnippetsJSONSchemaTypedDict


class UpdatePromptSnippetResponseFormatPromptSnippetsResponse1(BaseModel):
    type: UpdatePromptSnippetResponseFormatPromptSnippetsResponse200ApplicationJSONType

    json_schema: UpdatePromptSnippetResponseFormatPromptSnippetsJSONSchema


UpdatePromptSnippetPromptSnippetsResponseResponseFormatTypedDict = TypeAliasType(
    "UpdatePromptSnippetPromptSnippetsResponseResponseFormatTypedDict",
    Union[
        UpdatePromptSnippetResponseFormatPromptSnippetsResponse2TypedDict,
        UpdatePromptSnippetResponseFormatPromptSnippetsResponse1TypedDict,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


UpdatePromptSnippetPromptSnippetsResponseResponseFormat = TypeAliasType(
    "UpdatePromptSnippetPromptSnippetsResponseResponseFormat",
    Union[
        UpdatePromptSnippetResponseFormatPromptSnippetsResponse2,
        UpdatePromptSnippetResponseFormatPromptSnippetsResponse1,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


UpdatePromptSnippetPromptSnippetsResponsePhotoRealVersion = Literal["v1", "v2"]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

UpdatePromptSnippetPromptSnippetsResponseEncodingFormat = Literal["float", "base64"]
r"""The format to return the embeddings"""

UpdatePromptSnippetPromptSnippetsResponseReasoningEffort = Literal[
    "low", "medium", "high"
]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class UpdatePromptSnippetPromptSnippetsResponseModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[UpdatePromptSnippetPromptSnippetsResponse200Format]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[UpdatePromptSnippetPromptSnippetsResponseQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[UpdatePromptSnippetPromptSnippetsResponseResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[
        UpdatePromptSnippetPromptSnippetsResponsePhotoRealVersion
    ]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[
        UpdatePromptSnippetPromptSnippetsResponseEncodingFormat
    ]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[
        UpdatePromptSnippetPromptSnippetsResponseReasoningEffort
    ]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


class UpdatePromptSnippetPromptSnippetsResponseModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[
        Optional[UpdatePromptSnippetPromptSnippetsResponse200Format],
        pydantic.Field(alias="format"),
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[UpdatePromptSnippetPromptSnippetsResponseQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[UpdatePromptSnippetPromptSnippetsResponseResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[UpdatePromptSnippetPromptSnippetsResponsePhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[
        UpdatePromptSnippetPromptSnippetsResponseEncodingFormat
    ] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[UpdatePromptSnippetPromptSnippetsResponseReasoningEffort],
        pydantic.Field(alias="reasoningEffort"),
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdatePromptSnippetPromptSnippetsResponseProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
]

UpdatePromptSnippetPromptSnippetsResponseRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""

UpdatePromptSnippet2PromptSnippetsResponse200ApplicationJSONResponseBodyType = Literal[
    "image_url"
]


class UpdatePromptSnippet2PromptSnippetsResponseImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class UpdatePromptSnippet2PromptSnippetsResponseImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class UpdatePromptSnippet2PromptSnippetsResponse2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: UpdatePromptSnippet2PromptSnippetsResponse200ApplicationJSONResponseBodyType
    image_url: UpdatePromptSnippet2PromptSnippetsResponseImageURLTypedDict


class UpdatePromptSnippet2PromptSnippetsResponse2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: UpdatePromptSnippet2PromptSnippetsResponse200ApplicationJSONResponseBodyType

    image_url: UpdatePromptSnippet2PromptSnippetsResponseImageURL


UpdatePromptSnippet2PromptSnippetsResponse200ApplicationJSONType = Literal["text"]


class UpdatePromptSnippet2PromptSnippetsResponse1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: UpdatePromptSnippet2PromptSnippetsResponse200ApplicationJSONType
    text: str


class UpdatePromptSnippet2PromptSnippetsResponse1(BaseModel):
    r"""Text content part of a prompt message"""

    type: UpdatePromptSnippet2PromptSnippetsResponse200ApplicationJSONType

    text: str


UpdatePromptSnippetContentPromptSnippetsResponse2TypedDict = TypeAliasType(
    "UpdatePromptSnippetContentPromptSnippetsResponse2TypedDict",
    Union[
        UpdatePromptSnippet2PromptSnippetsResponse1TypedDict,
        UpdatePromptSnippet2PromptSnippetsResponse2TypedDict,
    ],
)


UpdatePromptSnippetContentPromptSnippetsResponse2 = TypeAliasType(
    "UpdatePromptSnippetContentPromptSnippetsResponse2",
    Union[
        UpdatePromptSnippet2PromptSnippetsResponse1,
        UpdatePromptSnippet2PromptSnippetsResponse2,
    ],
)


UpdatePromptSnippetPromptSnippetsResponseContentTypedDict = TypeAliasType(
    "UpdatePromptSnippetPromptSnippetsResponseContentTypedDict",
    Union[str, List[UpdatePromptSnippetContentPromptSnippetsResponse2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


UpdatePromptSnippetPromptSnippetsResponseContent = TypeAliasType(
    "UpdatePromptSnippetPromptSnippetsResponseContent",
    Union[str, List[UpdatePromptSnippetContentPromptSnippetsResponse2]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


UpdatePromptSnippetPromptSnippetsResponse200Type = Literal["function"]


class UpdatePromptSnippetPromptSnippetsResponseFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class UpdatePromptSnippetPromptSnippetsResponseFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class UpdatePromptSnippetPromptSnippetsResponseToolCallsTypedDict(TypedDict):
    type: UpdatePromptSnippetPromptSnippetsResponse200Type
    function: UpdatePromptSnippetPromptSnippetsResponseFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class UpdatePromptSnippetPromptSnippetsResponseToolCalls(BaseModel):
    type: UpdatePromptSnippetPromptSnippetsResponse200Type

    function: UpdatePromptSnippetPromptSnippetsResponseFunction

    id: Optional[str] = None

    index: Optional[float] = None


class UpdatePromptSnippetPromptSnippetsResponseMessagesTypedDict(TypedDict):
    role: UpdatePromptSnippetPromptSnippetsResponseRole
    r"""The role of the prompt message"""
    content: UpdatePromptSnippetPromptSnippetsResponseContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[
        List[UpdatePromptSnippetPromptSnippetsResponseToolCallsTypedDict]
    ]


class UpdatePromptSnippetPromptSnippetsResponseMessages(BaseModel):
    role: UpdatePromptSnippetPromptSnippetsResponseRole
    r"""The role of the prompt message"""

    content: UpdatePromptSnippetPromptSnippetsResponseContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[UpdatePromptSnippetPromptSnippetsResponseToolCalls]] = (
        None
    )


class UpdatePromptSnippetPromptSnippetsResponsePromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[UpdatePromptSnippetPromptSnippetsResponseMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[UpdatePromptSnippetPromptSnippetsResponseModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[
        UpdatePromptSnippetPromptSnippetsResponseModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[UpdatePromptSnippetPromptSnippetsResponseProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class UpdatePromptSnippetPromptSnippetsResponsePromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[UpdatePromptSnippetPromptSnippetsResponseMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[UpdatePromptSnippetPromptSnippetsResponseModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[
        UpdatePromptSnippetPromptSnippetsResponseModelParameters
    ] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[UpdatePromptSnippetPromptSnippetsResponseProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdatePromptSnippetPromptSnippetsResponseUseCases = Literal[
    "Agents",
    "Agents simulations",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Documents QA",
    "Conversation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "SQL",
    "Summarization",
    "Tagging",
]

UpdatePromptSnippetPromptSnippetsResponseLanguage = Literal[
    "Chinese", "Dutch", "English", "French", "German", "Russian", "Spanish"
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptSnippetPromptSnippetsResponseMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[UpdatePromptSnippetPromptSnippetsResponseUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[UpdatePromptSnippetPromptSnippetsResponseLanguage]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptSnippetPromptSnippetsResponseMetadata(BaseModel):
    use_cases: Optional[List[UpdatePromptSnippetPromptSnippetsResponseUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: Optional[UpdatePromptSnippetPromptSnippetsResponseLanguage] = None
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptSnippetVersionsTypedDict(TypedDict):
    r"""Prompt version model returned from the API"""

    id: str
    prompt_config: UpdatePromptSnippetPromptSnippetsResponsePromptConfigTypedDict
    r"""A list of messages compatible with the openAI schema"""
    metadata: UpdatePromptSnippetPromptSnippetsResponseMetadataTypedDict
    created_by_id: str
    updated_by_id: str
    timestamp: str
    description: NotRequired[Nullable[str]]
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""


class UpdatePromptSnippetVersions(BaseModel):
    r"""Prompt version model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    prompt_config: UpdatePromptSnippetPromptSnippetsResponsePromptConfig
    r"""A list of messages compatible with the openAI schema"""

    metadata: UpdatePromptSnippetPromptSnippetsResponseMetadata

    created_by_id: str

    updated_by_id: str

    timestamp: str

    description: OptionalNullable[str] = UNSET
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class UpdatePromptSnippetResponseBodyTypedDict(TypedDict):
    r"""Prompt snippet model returned from the API"""

    id: str
    owner: UpdatePromptSnippetOwnerTypedDict
    domain_id: str
    key: str
    prompt_config: UpdatePromptSnippetPromptSnippetsPromptConfigTypedDict
    r"""A list of messages compatible with the openAI schema"""
    metadata: UpdatePromptSnippetPromptSnippetsMetadataTypedDict
    created_by_id: str
    updated_by_id: str
    type: UpdatePromptSnippetPromptSnippetsType
    versions: List[UpdatePromptSnippetVersionsTypedDict]
    description: NotRequired[Nullable[str]]
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""
    created: NotRequired[datetime]
    r"""The date and time the resource was created"""
    updated: NotRequired[datetime]
    r"""The date and time the resource was last updated"""


class UpdatePromptSnippetResponseBody(BaseModel):
    r"""Prompt snippet model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    owner: UpdatePromptSnippetOwner

    domain_id: str

    key: str

    prompt_config: UpdatePromptSnippetPromptSnippetsPromptConfig
    r"""A list of messages compatible with the openAI schema"""

    metadata: UpdatePromptSnippetPromptSnippetsMetadata

    created_by_id: str

    updated_by_id: str

    type: UpdatePromptSnippetPromptSnippetsType

    versions: List[UpdatePromptSnippetVersions]

    description: OptionalNullable[str] = UNSET
    r"""The prompt snippet’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    created: Optional[datetime] = None
    r"""The date and time the resource was created"""

    updated: Optional[datetime] = dateutil.parser.isoparse("2025-02-10T17:12:36.462Z")
    r"""The date and time the resource was last updated"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "created", "updated"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
