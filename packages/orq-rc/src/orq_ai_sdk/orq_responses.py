"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from enum import Enum
from orq_ai_sdk import models, utils
from orq_ai_sdk._hooks import HookContext
from orq_ai_sdk.models import createresponseop as models_createresponseop
from orq_ai_sdk.types import OptionalNullable, UNSET
from orq_ai_sdk.utils import eventstreaming, get_security_from_env
from orq_ai_sdk.utils.unmarshal_json_response import unmarshal_json_response
from typing import Dict, List, Mapping, Optional, Union


class CreateAcceptEnum(str, Enum):
    APPLICATION_JSON = "application/json"
    TEXT_EVENT_STREAM = "text/event-stream"


class OrqResponses(BaseSDK):
    def create(
        self,
        *,
        model: str,
        input_: Union[
            models_createresponseop.CreateResponseInput,
            models_createresponseop.CreateResponseInputTypedDict,
        ],
        metadata: Optional[Dict[str, str]] = None,
        temperature: OptionalNullable[float] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        previous_response_id: OptionalNullable[str] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        reasoning: OptionalNullable[
            Union[
                models_createresponseop.Reasoning,
                models_createresponseop.ReasoningTypedDict,
            ]
        ] = UNSET,
        max_output_tokens: OptionalNullable[int] = UNSET,
        text: OptionalNullable[
            Union[
                models_createresponseop.CreateResponseText,
                models_createresponseop.CreateResponseTextTypedDict,
            ]
        ] = UNSET,
        include: OptionalNullable[List[models_createresponseop.Include]] = UNSET,
        parallel_tool_calls: OptionalNullable[bool] = UNSET,
        store: OptionalNullable[bool] = True,
        service_tier: OptionalNullable[models_createresponseop.ServiceTier] = UNSET,
        tools: Optional[
            Union[
                List[models_createresponseop.CreateResponseTools],
                List[models_createresponseop.CreateResponseToolsTypedDict],
            ]
        ] = None,
        tool_choice: Optional[
            Union[
                models_createresponseop.CreateResponseToolChoice,
                models_createresponseop.CreateResponseToolChoiceTypedDict,
            ]
        ] = None,
        stream: Optional[bool] = False,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        accept_header_override: Optional[CreateAcceptEnum] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.CreateResponseResponse:
        r"""Create response

        Creates a model response for the given input.

        :param model: ID of the model to use. You can use the List models API to see all of your available models.
        :param input: The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input.
        :param metadata: Developer-defined key-value pairs that will be included in response objects
        :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        :param previous_response_id: The ID of a previous response to continue the conversation from. The model will have access to the previous response context.
        :param instructions: Developer-provided instructions that the model should follow. Overwrites the default system message.
        :param reasoning: Configuration for reasoning models
        :param max_output_tokens: The maximum number of tokens that can be generated in the response
        :param text:
        :param include: Specifies which (potentially large) fields to include in the response. By default, the results of Code Interpreter and file searches are excluded. Available options:
            - code_interpreter_call.outputs: Include the outputs of Code Interpreter tool calls
            - computer_call_output.output.image_url: Include the image URLs from computer use tool calls
            - file_search_call.results: Include the results of file search tool calls
            - message.input_image.image_url: Include URLs of input images
            - message.output_text.logprobs: Include log probabilities for output text (when logprobs is enabled)
            - reasoning.encrypted_content: Include encrypted reasoning content for reasoning models
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use.
        :param store: Whether to store this response for use in distillations or evals.
        :param service_tier: Specifies the latency tier to use for processing the request. Defaults to \"auto\".
        :param tools: A list of tools the model may call. Use this to provide a list of functions the model may generate JSON inputs for.
        :param tool_choice: How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool.
        :param stream:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.CreateResponseRequestBody(
            model=model,
            metadata=metadata,
            temperature=temperature,
            top_p=top_p,
            previous_response_id=previous_response_id,
            instructions=instructions,
            reasoning=utils.get_pydantic_model(
                reasoning, OptionalNullable[models.Reasoning]
            ),
            max_output_tokens=max_output_tokens,
            text=utils.get_pydantic_model(
                text, OptionalNullable[models.CreateResponseText]
            ),
            input=utils.get_pydantic_model(input_, models.CreateResponseInput),
            include=include,
            parallel_tool_calls=parallel_tool_calls,
            store=store,
            service_tier=service_tier,
            tools=utils.get_pydantic_model(
                tools, Optional[List[models.CreateResponseTools]]
            ),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.CreateResponseToolChoice]
            ),
            stream=stream,
        )

        req = self._build_request(
            method="POST",
            path="/v2/router/responses",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value=accept_header_override.value
            if accept_header_override is not None
            else "application/json;q=1, text/event-stream;q=0",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.CreateResponseRequestBody
            ),
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="createResponse",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            return unmarshal_json_response(
                models.CreateResponseResponseBody, http_res, http_res_text
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.CreateResponseRouterResponsesResponseBody
                ),
                sentinel="[DONE]",
                client_ref=self,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        http_res_text = utils.stream_to_text(http_res)
        raise models.APIError("Unexpected response received", http_res, http_res_text)

    async def create_async(
        self,
        *,
        model: str,
        input_: Union[
            models_createresponseop.CreateResponseInput,
            models_createresponseop.CreateResponseInputTypedDict,
        ],
        metadata: Optional[Dict[str, str]] = None,
        temperature: OptionalNullable[float] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        previous_response_id: OptionalNullable[str] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        reasoning: OptionalNullable[
            Union[
                models_createresponseop.Reasoning,
                models_createresponseop.ReasoningTypedDict,
            ]
        ] = UNSET,
        max_output_tokens: OptionalNullable[int] = UNSET,
        text: OptionalNullable[
            Union[
                models_createresponseop.CreateResponseText,
                models_createresponseop.CreateResponseTextTypedDict,
            ]
        ] = UNSET,
        include: OptionalNullable[List[models_createresponseop.Include]] = UNSET,
        parallel_tool_calls: OptionalNullable[bool] = UNSET,
        store: OptionalNullable[bool] = True,
        service_tier: OptionalNullable[models_createresponseop.ServiceTier] = UNSET,
        tools: Optional[
            Union[
                List[models_createresponseop.CreateResponseTools],
                List[models_createresponseop.CreateResponseToolsTypedDict],
            ]
        ] = None,
        tool_choice: Optional[
            Union[
                models_createresponseop.CreateResponseToolChoice,
                models_createresponseop.CreateResponseToolChoiceTypedDict,
            ]
        ] = None,
        stream: Optional[bool] = False,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        accept_header_override: Optional[CreateAcceptEnum] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.CreateResponseResponse:
        r"""Create response

        Creates a model response for the given input.

        :param model: ID of the model to use. You can use the List models API to see all of your available models.
        :param input: The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input.
        :param metadata: Developer-defined key-value pairs that will be included in response objects
        :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        :param previous_response_id: The ID of a previous response to continue the conversation from. The model will have access to the previous response context.
        :param instructions: Developer-provided instructions that the model should follow. Overwrites the default system message.
        :param reasoning: Configuration for reasoning models
        :param max_output_tokens: The maximum number of tokens that can be generated in the response
        :param text:
        :param include: Specifies which (potentially large) fields to include in the response. By default, the results of Code Interpreter and file searches are excluded. Available options:
            - code_interpreter_call.outputs: Include the outputs of Code Interpreter tool calls
            - computer_call_output.output.image_url: Include the image URLs from computer use tool calls
            - file_search_call.results: Include the results of file search tool calls
            - message.input_image.image_url: Include URLs of input images
            - message.output_text.logprobs: Include log probabilities for output text (when logprobs is enabled)
            - reasoning.encrypted_content: Include encrypted reasoning content for reasoning models
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use.
        :param store: Whether to store this response for use in distillations or evals.
        :param service_tier: Specifies the latency tier to use for processing the request. Defaults to \"auto\".
        :param tools: A list of tools the model may call. Use this to provide a list of functions the model may generate JSON inputs for.
        :param tool_choice: How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool.
        :param stream:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.CreateResponseRequestBody(
            model=model,
            metadata=metadata,
            temperature=temperature,
            top_p=top_p,
            previous_response_id=previous_response_id,
            instructions=instructions,
            reasoning=utils.get_pydantic_model(
                reasoning, OptionalNullable[models.Reasoning]
            ),
            max_output_tokens=max_output_tokens,
            text=utils.get_pydantic_model(
                text, OptionalNullable[models.CreateResponseText]
            ),
            input=utils.get_pydantic_model(input_, models.CreateResponseInput),
            include=include,
            parallel_tool_calls=parallel_tool_calls,
            store=store,
            service_tier=service_tier,
            tools=utils.get_pydantic_model(
                tools, Optional[List[models.CreateResponseTools]]
            ),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.CreateResponseToolChoice]
            ),
            stream=stream,
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/router/responses",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value=accept_header_override.value
            if accept_header_override is not None
            else "application/json;q=1, text/event-stream;q=0",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.CreateResponseRequestBody
            ),
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="createResponse",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            return unmarshal_json_response(
                models.CreateResponseResponseBody, http_res, http_res_text
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.CreateResponseRouterResponsesResponseBody
                ),
                sentinel="[DONE]",
                client_ref=self,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.APIError("Unexpected response received", http_res, http_res_text)
