"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
import httpx
import io
from orq_ai_sdk.models import OrqError
from orq_ai_sdk.types import BaseModel, Nullable, UNSET_SENTINEL
from orq_ai_sdk.utils import FieldMetadata, MultipartFormMetadata
import pydantic
from pydantic import model_serializer
from typing import IO, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


PostV2ProxyAudioTranslationsResponseFormat = Literal[
    "json", "text", "srt", "verbose_json", "vtt"
]
r"""The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt."""

PostV2ProxyAudioTranslationsTimestampsGranularity = Literal["none", "word", "character"]
r"""The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word."""


class PostV2ProxyAudioTranslationsFileTypedDict(TypedDict):
    file_name: str
    content: Union[bytes, IO[bytes], io.BufferedReader]
    content_type: NotRequired[str]


class PostV2ProxyAudioTranslationsFile(BaseModel):
    file_name: Annotated[
        str, pydantic.Field(alias="fileName"), FieldMetadata(multipart=True)
    ]

    content: Annotated[
        Union[bytes, IO[bytes], io.BufferedReader],
        pydantic.Field(alias=""),
        FieldMetadata(multipart=MultipartFormMetadata(content=True)),
    ]

    content_type: Annotated[
        Optional[str],
        pydantic.Field(alias="Content-Type"),
        FieldMetadata(multipart=True),
    ] = None


class PostV2ProxyAudioTranslationsRequestBodyTypedDict(TypedDict):
    r"""Translates audio into English."""

    file: PostV2ProxyAudioTranslationsFileTypedDict
    r"""The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm."""
    model: NotRequired[str]
    r"""ID of the model to use"""
    prompt: NotRequired[str]
    r"""An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language."""
    enable_logging: NotRequired[bool]
    r"""When enable_logging is set to false, zero retention mode is used. This disables history features like request stitching and is only available to enterprise customers."""
    diarize: NotRequired[bool]
    r"""Whether to annotate which speaker is currently talking in the uploaded file."""
    response_format: NotRequired[PostV2ProxyAudioTranslationsResponseFormat]
    r"""The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt."""
    tag_audio_events: NotRequired[bool]
    r"""Whether to tag audio events like (laughter), (footsteps), etc. in the transcription."""
    num_speakers: NotRequired[float]
    r"""The maximum amount of speakers talking in the uploaded file. Helps with predicting who speaks when, the maximum is 32."""
    timestamps_granularity: NotRequired[
        PostV2ProxyAudioTranslationsTimestampsGranularity
    ]
    r"""The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word."""
    temperature: NotRequired[float]
    r"""The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit."""


class PostV2ProxyAudioTranslationsRequestBody(BaseModel):
    r"""Translates audio into English."""

    file: Annotated[
        PostV2ProxyAudioTranslationsFile,
        FieldMetadata(multipart=MultipartFormMetadata(file=True)),
    ]
    r"""The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm."""

    model: Annotated[Optional[str], FieldMetadata(multipart=True)] = "openai/whisper-1"
    r"""ID of the model to use"""

    prompt: Annotated[Optional[str], FieldMetadata(multipart=True)] = None
    r"""An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language."""

    enable_logging: Annotated[Optional[bool], FieldMetadata(multipart=True)] = True
    r"""When enable_logging is set to false, zero retention mode is used. This disables history features like request stitching and is only available to enterprise customers."""

    diarize: Annotated[Optional[bool], FieldMetadata(multipart=True)] = False
    r"""Whether to annotate which speaker is currently talking in the uploaded file."""

    response_format: Annotated[
        Optional[PostV2ProxyAudioTranslationsResponseFormat],
        FieldMetadata(multipart=True),
    ] = None
    r"""The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt."""

    tag_audio_events: Annotated[Optional[bool], FieldMetadata(multipart=True)] = True
    r"""Whether to tag audio events like (laughter), (footsteps), etc. in the transcription."""

    num_speakers: Annotated[Optional[float], FieldMetadata(multipart=True)] = None
    r"""The maximum amount of speakers talking in the uploaded file. Helps with predicting who speaks when, the maximum is 32."""

    timestamps_granularity: Annotated[
        Optional[PostV2ProxyAudioTranslationsTimestampsGranularity],
        FieldMetadata(multipart=True),
    ] = "word"
    r"""The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word."""

    temperature: Annotated[Optional[float], FieldMetadata(multipart=True)] = None
    r"""The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit."""


class PostV2ProxyAudioTranslationsErrorTypedDict(TypedDict):
    message: str
    type: str
    param: Nullable[str]
    code: str


class PostV2ProxyAudioTranslationsError(BaseModel):
    message: str

    type: str

    param: Nullable[str]

    code: str

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["param"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyAudioTranslationsProxyResponseBodyData(BaseModel):
    error: PostV2ProxyAudioTranslationsError


class PostV2ProxyAudioTranslationsProxyResponseBody(OrqError):
    r"""Returns validation error"""

    data: PostV2ProxyAudioTranslationsProxyResponseBodyData

    def __init__(
        self,
        data: PostV2ProxyAudioTranslationsProxyResponseBodyData,
        raw_response: httpx.Response,
        body: Optional[str] = None,
    ):
        fallback = body or raw_response.text
        message = str(data.error.message) or fallback
        super().__init__(message, raw_response, body)
        self.data = data


class ResponseBodyWordsTypedDict(TypedDict):
    word: NotRequired[str]
    start: NotRequired[float]
    end: NotRequired[float]


class ResponseBodyWords(BaseModel):
    word: Optional[str] = None

    start: Optional[float] = None

    end: Optional[float] = None


class ResponseBodySegmentsTypedDict(TypedDict):
    id: float
    seek: float
    start: float
    end: float
    text: str
    tokens: List[float]
    temperature: float
    avg_logprob: float
    compression_ratio: float
    no_speech_prob: float


class ResponseBodySegments(BaseModel):
    id: float

    seek: float

    start: float

    end: float

    text: str

    tokens: List[float]

    temperature: float

    avg_logprob: float

    compression_ratio: float

    no_speech_prob: float


class PostV2ProxyAudioTranslationsResponseBody2TypedDict(TypedDict):
    text: str
    task: NotRequired[str]
    language: NotRequired[str]
    duration: NotRequired[float]
    words: NotRequired[List[ResponseBodyWordsTypedDict]]
    segments: NotRequired[List[ResponseBodySegmentsTypedDict]]


class PostV2ProxyAudioTranslationsResponseBody2(BaseModel):
    text: str

    task: Optional[str] = None

    language: Optional[str] = None

    duration: Optional[float] = None

    words: Optional[List[ResponseBodyWords]] = None

    segments: Optional[List[ResponseBodySegments]] = None


class PostV2ProxyAudioTranslationsResponseBody1TypedDict(TypedDict):
    text: str


class PostV2ProxyAudioTranslationsResponseBody1(BaseModel):
    text: str


PostV2ProxyAudioTranslationsResponseBodyTypedDict = TypeAliasType(
    "PostV2ProxyAudioTranslationsResponseBodyTypedDict",
    Union[
        PostV2ProxyAudioTranslationsResponseBody1TypedDict,
        PostV2ProxyAudioTranslationsResponseBody2TypedDict,
        str,
    ],
)
r"""Returns the translated text"""


PostV2ProxyAudioTranslationsResponseBody = TypeAliasType(
    "PostV2ProxyAudioTranslationsResponseBody",
    Union[
        PostV2ProxyAudioTranslationsResponseBody1,
        PostV2ProxyAudioTranslationsResponseBody2,
        str,
    ],
)
r"""Returns the translated text"""
