"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from dataclasses import dataclass, field
import httpx
from orq_ai_sdk.models import OrqError
from orq_ai_sdk.types import BaseModel, Nullable, UNSET_SENTINEL
import pydantic
from pydantic import model_serializer
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


InputTypedDict = TypeAliasType("InputTypedDict", Union[str, List[str]])
r"""Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models."""


Input = TypeAliasType("Input", Union[str, List[str]])
r"""Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models."""


class CreateModerationRequestBodyTypedDict(TypedDict):
    r"""Classifies if text violates content policy"""

    input: InputTypedDict
    r"""Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models."""
    model: str
    r"""The content moderation model you would like to use. Defaults to omni-moderation-latest"""


class CreateModerationRequestBody(BaseModel):
    r"""Classifies if text violates content policy"""

    input: Input
    r"""Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models."""

    model: str
    r"""The content moderation model you would like to use. Defaults to omni-moderation-latest"""


class CreateModerationErrorTypedDict(TypedDict):
    message: str
    type: str
    param: Nullable[str]
    code: str


class CreateModerationError(BaseModel):
    message: str

    type: str

    param: Nullable[str]

    code: str

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateModerationRouterModerationsResponseBodyData(BaseModel):
    error: CreateModerationError


@dataclass(unsafe_hash=True)
class CreateModerationRouterModerationsResponseBody(OrqError):
    r"""Returns validation error"""

    data: CreateModerationRouterModerationsResponseBodyData = field(hash=False)

    def __init__(
        self,
        data: CreateModerationRouterModerationsResponseBodyData,
        raw_response: httpx.Response,
        body: Optional[str] = None,
    ):
        fallback = body or raw_response.text
        message = str(data.error.message) or fallback
        super().__init__(message, raw_response, body)
        object.__setattr__(self, "data", data)


class ResultsCategoriesTypedDict(TypedDict):
    r"""A list of the categories, and whether they are flagged or not"""

    sexual: bool
    r"""Sexual content detected"""
    hate_and_discrimination: bool
    r"""Hate and discrimination content detected"""
    violence_and_threats: bool
    r"""Violence and threats content detected"""
    dangerous_and_criminal_content: bool
    r"""Dangerous and criminal content detected"""
    selfharm: bool
    r"""Self-harm content detected"""
    health: bool
    r"""Unqualified health advice detected"""
    financial: bool
    r"""Unqualified financial advice detected"""
    law: bool
    r"""Unqualified legal advice detected"""
    pii: bool
    r"""Personally identifiable information detected"""


class ResultsCategories(BaseModel):
    r"""A list of the categories, and whether they are flagged or not"""

    sexual: bool
    r"""Sexual content detected"""

    hate_and_discrimination: bool
    r"""Hate and discrimination content detected"""

    violence_and_threats: bool
    r"""Violence and threats content detected"""

    dangerous_and_criminal_content: bool
    r"""Dangerous and criminal content detected"""

    selfharm: bool
    r"""Self-harm content detected"""

    health: bool
    r"""Unqualified health advice detected"""

    financial: bool
    r"""Unqualified financial advice detected"""

    law: bool
    r"""Unqualified legal advice detected"""

    pii: bool
    r"""Personally identifiable information detected"""


class ResultsCategoryScoresTypedDict(TypedDict):
    r"""A list of the categories along with their scores as predicted by model"""

    sexual: float
    r"""The score for sexual content"""
    hate_and_discrimination: float
    r"""The score for hate and discrimination content"""
    violence_and_threats: float
    r"""The score for violence and threats content"""
    dangerous_and_criminal_content: float
    r"""The score for dangerous and criminal content"""
    selfharm: float
    r"""The score for self-harm content"""
    health: float
    r"""The score for unqualified health advice"""
    financial: float
    r"""The score for unqualified financial advice"""
    law: float
    r"""The score for unqualified legal advice"""
    pii: float
    r"""The score for personally identifiable information"""


class ResultsCategoryScores(BaseModel):
    r"""A list of the categories along with their scores as predicted by model"""

    sexual: float
    r"""The score for sexual content"""

    hate_and_discrimination: float
    r"""The score for hate and discrimination content"""

    violence_and_threats: float
    r"""The score for violence and threats content"""

    dangerous_and_criminal_content: float
    r"""The score for dangerous and criminal content"""

    selfharm: float
    r"""The score for self-harm content"""

    health: float
    r"""The score for unqualified health advice"""

    financial: float
    r"""The score for unqualified financial advice"""

    law: float
    r"""The score for unqualified legal advice"""

    pii: float
    r"""The score for personally identifiable information"""


class Results2TypedDict(TypedDict):
    categories: ResultsCategoriesTypedDict
    r"""A list of the categories, and whether they are flagged or not"""
    category_scores: ResultsCategoryScoresTypedDict
    r"""A list of the categories along with their scores as predicted by model"""


class Results2(BaseModel):
    categories: ResultsCategories
    r"""A list of the categories, and whether they are flagged or not"""

    category_scores: ResultsCategoryScores
    r"""A list of the categories along with their scores as predicted by model"""


class CategoriesTypedDict(TypedDict):
    r"""A list of the categories, and whether they are flagged or not"""

    hate: bool
    r"""Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste."""
    hate_threatening: bool
    r"""Hateful content that also includes violence or serious harm towards the targeted group."""
    harassment: bool
    r"""Content that expresses, incites, or promotes harassing language towards any target."""
    harassment_threatening: bool
    r"""Harassment content that also includes violence or serious harm towards any target."""
    illicit: bool
    r"""Content that includes instructions or advice that facilitate the planning or execution of wrongdoing."""
    illicit_violent: bool
    r"""Content that includes instructions or advice that facilitate the planning or execution of wrongdoing that also includes violence."""
    self_harm: bool
    r"""Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders."""
    self_harm_intent: bool
    r"""Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm."""
    self_harm_instructions: bool
    r"""Content that encourages performing acts of self-harm, or that gives instructions or advice on how to commit such acts."""
    sexual: bool
    r"""Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services."""
    sexual_minors: bool
    r"""Sexual content that includes an individual who is under 18 years old."""
    violence: bool
    r"""Content that depicts death, violence, or physical injury."""
    violence_graphic: bool
    r"""Content that depicts death, violence, or physical injury in graphic detail."""


class Categories(BaseModel):
    r"""A list of the categories, and whether they are flagged or not"""

    hate: bool
    r"""Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste."""

    hate_threatening: Annotated[bool, pydantic.Field(alias="hate/threatening")]
    r"""Hateful content that also includes violence or serious harm towards the targeted group."""

    harassment: bool
    r"""Content that expresses, incites, or promotes harassing language towards any target."""

    harassment_threatening: Annotated[
        bool, pydantic.Field(alias="harassment/threatening")
    ]
    r"""Harassment content that also includes violence or serious harm towards any target."""

    illicit: bool
    r"""Content that includes instructions or advice that facilitate the planning or execution of wrongdoing."""

    illicit_violent: Annotated[bool, pydantic.Field(alias="illicit/violent")]
    r"""Content that includes instructions or advice that facilitate the planning or execution of wrongdoing that also includes violence."""

    self_harm: Annotated[bool, pydantic.Field(alias="self-harm")]
    r"""Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders."""

    self_harm_intent: Annotated[bool, pydantic.Field(alias="self-harm/intent")]
    r"""Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm."""

    self_harm_instructions: Annotated[
        bool, pydantic.Field(alias="self-harm/instructions")
    ]
    r"""Content that encourages performing acts of self-harm, or that gives instructions or advice on how to commit such acts."""

    sexual: bool
    r"""Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services."""

    sexual_minors: Annotated[bool, pydantic.Field(alias="sexual/minors")]
    r"""Sexual content that includes an individual who is under 18 years old."""

    violence: bool
    r"""Content that depicts death, violence, or physical injury."""

    violence_graphic: Annotated[bool, pydantic.Field(alias="violence/graphic")]
    r"""Content that depicts death, violence, or physical injury in graphic detail."""


class CategoryScoresTypedDict(TypedDict):
    r"""A list of the categories along with their scores as predicted by model"""

    hate: float
    r"""The score for the category hate"""
    hate_threatening: float
    r"""The score for the category hate/threatening"""
    harassment: float
    r"""The score for the category harassment"""
    harassment_threatening: float
    r"""The score for the category harassment/threatening"""
    illicit: float
    r"""The score for the category illicit"""
    illicit_violent: float
    r"""The score for the category illicit/violent"""
    self_harm: float
    r"""The score for the category self-harm"""
    self_harm_intent: float
    r"""The score for the category self-harm/intent"""
    self_harm_instructions: float
    r"""The score for the category self-harm/instructions"""
    sexual: float
    r"""The score for the category sexual"""
    sexual_minors: float
    r"""The score for the category sexual/minors"""
    violence: float
    r"""The score for the category violence"""
    violence_graphic: float
    r"""The score for the category violence/graphic"""


class CategoryScores(BaseModel):
    r"""A list of the categories along with their scores as predicted by model"""

    hate: float
    r"""The score for the category hate"""

    hate_threatening: Annotated[float, pydantic.Field(alias="hate/threatening")]
    r"""The score for the category hate/threatening"""

    harassment: float
    r"""The score for the category harassment"""

    harassment_threatening: Annotated[
        float, pydantic.Field(alias="harassment/threatening")
    ]
    r"""The score for the category harassment/threatening"""

    illicit: float
    r"""The score for the category illicit"""

    illicit_violent: Annotated[float, pydantic.Field(alias="illicit/violent")]
    r"""The score for the category illicit/violent"""

    self_harm: Annotated[float, pydantic.Field(alias="self-harm")]
    r"""The score for the category self-harm"""

    self_harm_intent: Annotated[float, pydantic.Field(alias="self-harm/intent")]
    r"""The score for the category self-harm/intent"""

    self_harm_instructions: Annotated[
        float, pydantic.Field(alias="self-harm/instructions")
    ]
    r"""The score for the category self-harm/instructions"""

    sexual: float
    r"""The score for the category sexual"""

    sexual_minors: Annotated[float, pydantic.Field(alias="sexual/minors")]
    r"""The score for the category sexual/minors"""

    violence: float
    r"""The score for the category violence"""

    violence_graphic: Annotated[float, pydantic.Field(alias="violence/graphic")]
    r"""The score for the category violence/graphic"""


class CategoryAppliedInputTypesTypedDict(TypedDict):
    r"""A list of the categories along with the input type(s) that the score applies to"""

    hate: List[str]
    r"""The applied input type(s) for the category hate"""
    hate_threatening: List[str]
    r"""The applied input type(s) for the category hate/threatening"""
    harassment: List[str]
    r"""The applied input type(s) for the category harassment"""
    harassment_threatening: List[str]
    r"""The applied input type(s) for the category harassment/threatening"""
    illicit: List[str]
    r"""The applied input type(s) for the category illicit"""
    illicit_violent: List[str]
    r"""The applied input type(s) for the category illicit/violent"""
    self_harm: List[str]
    r"""The applied input type(s) for the category self-harm"""
    self_harm_intent: List[str]
    r"""The applied input type(s) for the category self-harm/intent"""
    self_harm_instructions: List[str]
    r"""The applied input type(s) for the category self-harm/instructions"""
    sexual: List[str]
    r"""The applied input type(s) for the category sexual"""
    sexual_minors: List[str]
    r"""The applied input type(s) for the category sexual/minors"""
    violence: List[str]
    r"""The applied input type(s) for the category violence"""
    violence_graphic: List[str]
    r"""The applied input type(s) for the category violence/graphic"""


class CategoryAppliedInputTypes(BaseModel):
    r"""A list of the categories along with the input type(s) that the score applies to"""

    hate: List[str]
    r"""The applied input type(s) for the category hate"""

    hate_threatening: Annotated[List[str], pydantic.Field(alias="hate/threatening")]
    r"""The applied input type(s) for the category hate/threatening"""

    harassment: List[str]
    r"""The applied input type(s) for the category harassment"""

    harassment_threatening: Annotated[
        List[str], pydantic.Field(alias="harassment/threatening")
    ]
    r"""The applied input type(s) for the category harassment/threatening"""

    illicit: List[str]
    r"""The applied input type(s) for the category illicit"""

    illicit_violent: Annotated[List[str], pydantic.Field(alias="illicit/violent")]
    r"""The applied input type(s) for the category illicit/violent"""

    self_harm: Annotated[List[str], pydantic.Field(alias="self-harm")]
    r"""The applied input type(s) for the category self-harm"""

    self_harm_intent: Annotated[List[str], pydantic.Field(alias="self-harm/intent")]
    r"""The applied input type(s) for the category self-harm/intent"""

    self_harm_instructions: Annotated[
        List[str], pydantic.Field(alias="self-harm/instructions")
    ]
    r"""The applied input type(s) for the category self-harm/instructions"""

    sexual: List[str]
    r"""The applied input type(s) for the category sexual"""

    sexual_minors: Annotated[List[str], pydantic.Field(alias="sexual/minors")]
    r"""The applied input type(s) for the category sexual/minors"""

    violence: List[str]
    r"""The applied input type(s) for the category violence"""

    violence_graphic: Annotated[List[str], pydantic.Field(alias="violence/graphic")]
    r"""The applied input type(s) for the category violence/graphic"""


class Results1TypedDict(TypedDict):
    flagged: bool
    r"""Whether any of the categories are flagged"""
    categories: CategoriesTypedDict
    r"""A list of the categories, and whether they are flagged or not"""
    category_scores: CategoryScoresTypedDict
    r"""A list of the categories along with their scores as predicted by model"""
    category_applied_input_types: NotRequired[CategoryAppliedInputTypesTypedDict]
    r"""A list of the categories along with the input type(s) that the score applies to"""


class Results1(BaseModel):
    flagged: bool
    r"""Whether any of the categories are flagged"""

    categories: Categories
    r"""A list of the categories, and whether they are flagged or not"""

    category_scores: CategoryScores
    r"""A list of the categories along with their scores as predicted by model"""

    category_applied_input_types: Optional[CategoryAppliedInputTypes] = None
    r"""A list of the categories along with the input type(s) that the score applies to"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["category_applied_input_types"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ResultsTypedDict = TypeAliasType(
    "ResultsTypedDict", Union[Results2TypedDict, Results1TypedDict]
)


Results = TypeAliasType("Results", Union[Results2, Results1])


class CreateModerationResponseBodyTypedDict(TypedDict):
    r"""Returns moderation classification results"""

    id: str
    r"""The unique identifier for the moderation request"""
    model: str
    r"""The model used to generate the moderation results"""
    results: List[ResultsTypedDict]
    r"""A list of moderation objects"""


class CreateModerationResponseBody(BaseModel):
    r"""Returns moderation classification results"""

    id: str
    r"""The unique identifier for the moderation request"""

    model: str
    r"""The model used to generate the moderation results"""

    results: List[Results]
    r"""A list of moderation objects"""


try:
    Categories.model_rebuild()
except NameError:
    pass
try:
    CategoryScores.model_rebuild()
except NameError:
    pass
try:
    CategoryAppliedInputTypes.model_rebuild()
except NameError:
    pass
