"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .audiocontentpartschema import (
    AudioContentPartSchema,
    AudioContentPartSchemaTypedDict,
)
from .filecontentpartschema import FileContentPartSchema, FileContentPartSchemaTypedDict
from .imagecontentpartschema import (
    ImageContentPartSchema,
    ImageContentPartSchemaTypedDict,
)
from .reasoningpartschema import ReasoningPartSchema, ReasoningPartSchemaTypedDict
from .redactedreasoningpartschema import (
    RedactedReasoningPartSchema,
    RedactedReasoningPartSchemaTypedDict,
)
from .refusalpartschema import RefusalPartSchema, RefusalPartSchemaTypedDict
from dataclasses import dataclass, field
import httpx
from orq_ai_sdk.models import OrqError
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import (
    FieldMetadata,
    PathParamMetadata,
    RequestMetadata,
    get_discriminator,
)
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import (
    Annotated,
    NotRequired,
    TypeAliasType,
    TypedDict,
    deprecated,
)


UpdatePromptFormat = Literal[
    "url",
    "b64_json",
    "text",
    "json_object",
]
r"""Only supported on `image` models."""


ResponseFormat6 = Literal[
    "json",
    "text",
    "srt",
    "verbose_json",
    "vtt",
]


ResponseFormat5 = Literal[
    "url",
    "base64_json",
]


UpdatePromptResponseFormat4 = Literal[
    "mp3",
    "opus",
    "aac",
    "flac",
    "wav",
    "pcm",
]


UpdatePromptResponseFormatPromptsRequestType = Literal["text",]


class UpdatePromptResponseFormat3TypedDict(TypedDict):
    type: UpdatePromptResponseFormatPromptsRequestType


class UpdatePromptResponseFormat3(BaseModel):
    type: UpdatePromptResponseFormatPromptsRequestType


UpdatePromptResponseFormatPromptsType = Literal["json_object",]


class UpdatePromptResponseFormat2TypedDict(TypedDict):
    type: UpdatePromptResponseFormatPromptsType


class UpdatePromptResponseFormat2(BaseModel):
    type: UpdatePromptResponseFormatPromptsType


UpdatePromptResponseFormatType = Literal["json_schema",]


class UpdatePromptResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    schema_: Dict[str, Any]
    description: NotRequired[str]
    strict: NotRequired[bool]


class UpdatePromptResponseFormatJSONSchema(BaseModel):
    name: str

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]

    description: Optional[str] = None

    strict: Optional[bool] = None


class UpdatePromptResponseFormat1TypedDict(TypedDict):
    type: UpdatePromptResponseFormatType
    json_schema: UpdatePromptResponseFormatJSONSchemaTypedDict
    display_name: NotRequired[str]


class UpdatePromptResponseFormat1(BaseModel):
    type: UpdatePromptResponseFormatType

    json_schema: UpdatePromptResponseFormatJSONSchema

    display_name: Optional[str] = None


UpdatePromptResponseFormatTypedDict = TypeAliasType(
    "UpdatePromptResponseFormatTypedDict",
    Union[
        UpdatePromptResponseFormat2TypedDict,
        UpdatePromptResponseFormat3TypedDict,
        UpdatePromptResponseFormat1TypedDict,
        UpdatePromptResponseFormat4,
        ResponseFormat5,
        ResponseFormat6,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


UpdatePromptResponseFormat = TypeAliasType(
    "UpdatePromptResponseFormat",
    Union[
        UpdatePromptResponseFormat2,
        UpdatePromptResponseFormat3,
        UpdatePromptResponseFormat1,
        UpdatePromptResponseFormat4,
        ResponseFormat5,
        ResponseFormat6,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


UpdatePromptPhotoRealVersion = Literal[
    "v1",
    "v2",
]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""


UpdatePromptEncodingFormat = Literal[
    "float",
    "base64",
]
r"""The format to return the embeddings"""


UpdatePromptReasoningEffort = Literal[
    "none",
    "disable",
    "minimal",
    "low",
    "medium",
    "high",
]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


UpdatePromptVerbosity = Literal[
    "low",
    "medium",
    "high",
]
r"""Controls the verbosity of the model output."""


UpdatePromptThinkingLevel = Literal[
    "low",
    "high",
]
r"""The level of thinking to use for the model. Only supported by `Google AI`"""


class UpdatePromptModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[UpdatePromptFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[str]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[Nullable[UpdatePromptResponseFormatTypedDict]]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[UpdatePromptPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[UpdatePromptEncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[UpdatePromptReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""
    budget_tokens: NotRequired[float]
    r"""Gives the model enhanced reasoning capabilities for complex tasks. A value of 0 disables thinking. The minimum budget tokens for thinking are 1024. The Budget Tokens should never exceed the Max Tokens parameter. Only supported by `Anthropic`"""
    verbosity: NotRequired[UpdatePromptVerbosity]
    r"""Controls the verbosity of the model output."""
    thinking_level: NotRequired[UpdatePromptThinkingLevel]
    r"""The level of thinking to use for the model. Only supported by `Google AI`"""


class UpdatePromptModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[Optional[UpdatePromptFormat], pydantic.Field(alias="format")] = (
        None
    )
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[str] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[UpdatePromptResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[UpdatePromptPhotoRealVersion], pydantic.Field(alias="photoRealVersion")
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[UpdatePromptEncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[UpdatePromptReasoningEffort], pydantic.Field(alias="reasoningEffort")
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    budget_tokens: Annotated[Optional[float], pydantic.Field(alias="budgetTokens")] = (
        None
    )
    r"""Gives the model enhanced reasoning capabilities for complex tasks. A value of 0 disables thinking. The minimum budget tokens for thinking are 1024. The Budget Tokens should never exceed the Max Tokens parameter. Only supported by `Anthropic`"""

    verbosity: Optional[UpdatePromptVerbosity] = None
    r"""Controls the verbosity of the model output."""

    thinking_level: Annotated[
        Optional[UpdatePromptThinkingLevel], pydantic.Field(alias="thinkingLevel")
    ] = None
    r"""The level of thinking to use for the model. Only supported by `Google AI`"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
            "budgetTokens",
            "verbosity",
            "thinkingLevel",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


Provider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
    "litellm",
    "openailike",
    "cerebras",
    "bytedance",
    "mistral",
]


UpdatePromptRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""


UpdatePrompt2PromptsRequestType = Literal["file",]
r"""The type of the content part. Always `file`."""


class UpdatePrompt2FileTypedDict(TypedDict):
    file_data: NotRequired[str]
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""
    uri: NotRequired[str]
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""
    mime_type: NotRequired[str]
    r"""MIME type of the file (e.g., application/pdf, image/png)"""
    filename: NotRequired[str]
    r"""The name of the file, used when passing the file to the model as a string."""


class UpdatePrompt2File(BaseModel):
    file_data: Optional[str] = None
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""

    uri: Optional[str] = None
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""MIME type of the file (e.g., application/pdf, image/png)"""

    filename: Optional[str] = None
    r"""The name of the file, used when passing the file to the model as a string."""


class UpdatePrompt23TypedDict(TypedDict):
    type: UpdatePrompt2PromptsRequestType
    r"""The type of the content part. Always `file`."""
    file: UpdatePrompt2FileTypedDict


class UpdatePrompt23(BaseModel):
    type: UpdatePrompt2PromptsRequestType
    r"""The type of the content part. Always `file`."""

    file: UpdatePrompt2File


UpdatePrompt2PromptsType = Literal["image_url",]


class UpdatePrompt2ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class UpdatePrompt2ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class UpdatePrompt22TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: UpdatePrompt2PromptsType
    image_url: UpdatePrompt2ImageURLTypedDict


class UpdatePrompt22(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: UpdatePrompt2PromptsType

    image_url: UpdatePrompt2ImageURL


UpdatePrompt2Type = Literal["text",]


class UpdatePrompt21TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: UpdatePrompt2Type
    text: str


class UpdatePrompt21(BaseModel):
    r"""Text content part of a prompt message"""

    type: UpdatePrompt2Type

    text: str


UpdatePromptContent2TypedDict = TypeAliasType(
    "UpdatePromptContent2TypedDict",
    Union[UpdatePrompt21TypedDict, UpdatePrompt22TypedDict, UpdatePrompt23TypedDict],
)


UpdatePromptContent2 = Annotated[
    Union[
        Annotated[UpdatePrompt21, Tag("text")],
        Annotated[UpdatePrompt22, Tag("image_url")],
        Annotated[UpdatePrompt23, Tag("file")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


UpdatePromptContentTypedDict = TypeAliasType(
    "UpdatePromptContentTypedDict", Union[str, List[UpdatePromptContent2TypedDict]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""


UpdatePromptContent = TypeAliasType(
    "UpdatePromptContent", Union[str, List[UpdatePromptContent2]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""


UpdatePromptType = Literal["function",]


class UpdatePromptFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class UpdatePromptFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class UpdatePromptToolCallsTypedDict(TypedDict):
    type: UpdatePromptType
    function: UpdatePromptFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class UpdatePromptToolCalls(BaseModel):
    type: UpdatePromptType

    function: UpdatePromptFunction

    id: Optional[str] = None

    index: Optional[float] = None


class UpdatePromptMessagesTypedDict(TypedDict):
    role: UpdatePromptRole
    r"""The role of the prompt message"""
    content: Nullable[UpdatePromptContentTypedDict]
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""
    tool_calls: NotRequired[List[UpdatePromptToolCallsTypedDict]]
    tool_call_id: NotRequired[str]


class UpdatePromptMessages(BaseModel):
    role: UpdatePromptRole
    r"""The role of the prompt message"""

    content: Nullable[UpdatePromptContent]
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""

    tool_calls: Optional[List[UpdatePromptToolCalls]] = None

    tool_call_id: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["tool_calls", "tool_call_id"]
        nullable_fields = ["content"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


ModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderation",
    "vision",
]
r"""The modality of the model"""


@deprecated(
    "warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
)
class PromptConfigTypedDict(TypedDict):
    r"""[DEPRECATED]. Please use the `prompt` property instead."""

    messages: List[UpdatePromptMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_parameters: NotRequired[UpdatePromptModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[Provider]
    version: NotRequired[str]
    model_db_id: NotRequired[Nullable[str]]
    model_type: NotRequired[Nullable[ModelType]]
    r"""The modality of the model"""
    is_private: NotRequired[bool]


@deprecated(
    "warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
)
class PromptConfig(BaseModel):
    r"""[DEPRECATED]. Please use the `prompt` property instead."""

    messages: List[UpdatePromptMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_parameters: Optional[UpdatePromptModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[Provider] = None

    version: Optional[str] = None

    model_db_id: OptionalNullable[str] = UNSET

    model_type: OptionalNullable[ModelType] = UNSET
    r"""The modality of the model"""

    is_private: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_parameters",
            "provider",
            "version",
            "model_db_id",
            "model_type",
            "is_private",
        ]
        nullable_fields = ["model_db_id", "model_type"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdatePromptUseCases = Literal[
    "Agents simulations",
    "Agents",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Conversation",
    "Documents QA",
    "Evaluation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "Sentiment analysis",
    "SQL",
    "Summarization",
    "Tagging",
    "Translation (document)",
    "Translation (sentences)",
]


UpdatePromptLanguage = Literal[
    "Chinese",
    "Dutch",
    "English",
    "French",
    "German",
    "Russian",
    "Spanish",
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[UpdatePromptUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[Nullable[UpdatePromptLanguage]]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptMetadata(BaseModel):
    use_cases: Optional[List[UpdatePromptUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: OptionalNullable[UpdatePromptLanguage] = UNSET
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["use_cases", "language"]
        nullable_fields = ["language"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdatePromptMessagesPromptsRequestRequestBodyRole = Literal["tool",]
r"""The role of the messages author, in this case tool."""


UpdatePrompt2PromptsRequestRequestBodyPromptMessages4Type = Literal["text",]


UpdatePrompt2PromptsRequestRequestBodyPromptMessages4ContentType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


UpdatePrompt2PromptsRequestRequestBodyTTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class UpdatePrompt2PromptsRequestRequestBodyCacheControlTypedDict(TypedDict):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessages4ContentType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[UpdatePrompt2PromptsRequestRequestBodyTTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePrompt2PromptsRequestRequestBodyCacheControl(BaseModel):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessages4ContentType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[UpdatePrompt2PromptsRequestRequestBodyTTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePrompt2PromptsRequestRequestBody1TypedDict(TypedDict):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessages4Type
    text: str
    cache_control: NotRequired[
        UpdatePrompt2PromptsRequestRequestBodyCacheControlTypedDict
    ]


class UpdatePrompt2PromptsRequestRequestBody1(BaseModel):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessages4Type

    text: str

    cache_control: Optional[UpdatePrompt2PromptsRequestRequestBodyCacheControl] = None


UpdatePromptContentPromptsRequestRequestBodyPrompt2TypedDict = (
    UpdatePrompt2PromptsRequestRequestBody1TypedDict
)


UpdatePromptContentPromptsRequestRequestBodyPrompt2 = (
    UpdatePrompt2PromptsRequestRequestBody1
)


UpdatePromptMessagesPromptsRequestRequestBodyContentTypedDict = TypeAliasType(
    "UpdatePromptMessagesPromptsRequestRequestBodyContentTypedDict",
    Union[str, List[UpdatePromptContentPromptsRequestRequestBodyPrompt2TypedDict]],
)
r"""The contents of the tool message."""


UpdatePromptMessagesPromptsRequestRequestBodyContent = TypeAliasType(
    "UpdatePromptMessagesPromptsRequestRequestBodyContent",
    Union[str, List[UpdatePromptContentPromptsRequestRequestBodyPrompt2]],
)
r"""The contents of the tool message."""


UpdatePromptMessagesPromptsType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


UpdatePromptMessagesTTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class UpdatePromptMessagesCacheControlTypedDict(TypedDict):
    type: UpdatePromptMessagesPromptsType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[UpdatePromptMessagesTTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePromptMessagesCacheControl(BaseModel):
    type: UpdatePromptMessagesPromptsType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[UpdatePromptMessagesTTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePromptMessagesToolMessageTypedDict(TypedDict):
    role: UpdatePromptMessagesPromptsRequestRequestBodyRole
    r"""The role of the messages author, in this case tool."""
    content: UpdatePromptMessagesPromptsRequestRequestBodyContentTypedDict
    r"""The contents of the tool message."""
    tool_call_id: str
    r"""Tool call that this message is responding to."""
    cache_control: NotRequired[UpdatePromptMessagesCacheControlTypedDict]


class UpdatePromptMessagesToolMessage(BaseModel):
    role: UpdatePromptMessagesPromptsRequestRequestBodyRole
    r"""The role of the messages author, in this case tool."""

    content: UpdatePromptMessagesPromptsRequestRequestBodyContent
    r"""The contents of the tool message."""

    tool_call_id: str
    r"""Tool call that this message is responding to."""

    cache_control: Optional[UpdatePromptMessagesCacheControl] = None


UpdatePrompt2PromptsRequestRequestBodyPromptMessages3Type = Literal["text",]


UpdatePrompt2PromptsRequestRequestBodyPromptMessages3ContentType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


UpdatePrompt2PromptsRequestTTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class UpdatePrompt2PromptsRequestCacheControlTypedDict(TypedDict):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessages3ContentType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[UpdatePrompt2PromptsRequestTTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePrompt2PromptsRequestCacheControl(BaseModel):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessages3ContentType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[UpdatePrompt2PromptsRequestTTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePrompt2PromptsRequest1TypedDict(TypedDict):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessages3Type
    text: str
    cache_control: NotRequired[UpdatePrompt2PromptsRequestCacheControlTypedDict]


class UpdatePrompt2PromptsRequest1(BaseModel):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessages3Type

    text: str

    cache_control: Optional[UpdatePrompt2PromptsRequestCacheControl] = None


UpdatePromptContentPromptsRequestRequestBody2TypedDict = TypeAliasType(
    "UpdatePromptContentPromptsRequestRequestBody2TypedDict",
    Union[
        RefusalPartSchemaTypedDict,
        RedactedReasoningPartSchemaTypedDict,
        UpdatePrompt2PromptsRequest1TypedDict,
        ReasoningPartSchemaTypedDict,
    ],
)


UpdatePromptContentPromptsRequestRequestBody2 = Annotated[
    Union[
        Annotated[UpdatePrompt2PromptsRequest1, Tag("text")],
        Annotated[RefusalPartSchema, Tag("refusal")],
        Annotated[ReasoningPartSchema, Tag("reasoning")],
        Annotated[RedactedReasoningPartSchema, Tag("redacted_reasoning")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


UpdatePromptMessagesPromptsRequestContentTypedDict = TypeAliasType(
    "UpdatePromptMessagesPromptsRequestContentTypedDict",
    Union[str, List[UpdatePromptContentPromptsRequestRequestBody2TypedDict]],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


UpdatePromptMessagesPromptsRequestContent = TypeAliasType(
    "UpdatePromptMessagesPromptsRequestContent",
    Union[str, List[UpdatePromptContentPromptsRequestRequestBody2]],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


UpdatePromptMessagesPromptsRequestRole = Literal["assistant",]
r"""The role of the messages author, in this case `assistant`."""


class UpdatePromptMessagesAudioTypedDict(TypedDict):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


class UpdatePromptMessagesAudio(BaseModel):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


UpdatePromptMessagesType = Literal["function",]
r"""The type of the tool. Currently, only `function` is supported."""


class UpdatePromptMessagesFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class UpdatePromptMessagesFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class UpdatePromptMessagesToolCallsTypedDict(TypedDict):
    id: str
    r"""The ID of the tool call."""
    type: UpdatePromptMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""
    function: UpdatePromptMessagesFunctionTypedDict
    thought_signature: NotRequired[str]
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models when continuing a conversation after a tool call."""


class UpdatePromptMessagesToolCalls(BaseModel):
    id: str
    r"""The ID of the tool call."""

    type: UpdatePromptMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""

    function: UpdatePromptMessagesFunction

    thought_signature: Optional[str] = None
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models when continuing a conversation after a tool call."""


class UpdatePromptMessagesAssistantMessageTypedDict(TypedDict):
    role: UpdatePromptMessagesPromptsRequestRole
    r"""The role of the messages author, in this case `assistant`."""
    content: NotRequired[Nullable[UpdatePromptMessagesPromptsRequestContentTypedDict]]
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""
    refusal: NotRequired[Nullable[str]]
    r"""The refusal message by the assistant."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""
    audio: NotRequired[Nullable[UpdatePromptMessagesAudioTypedDict]]
    r"""Data about a previous audio response from the model."""
    tool_calls: NotRequired[List[UpdatePromptMessagesToolCallsTypedDict]]
    r"""The tool calls generated by the model, such as function calls."""


class UpdatePromptMessagesAssistantMessage(BaseModel):
    role: UpdatePromptMessagesPromptsRequestRole
    r"""The role of the messages author, in this case `assistant`."""

    content: OptionalNullable[UpdatePromptMessagesPromptsRequestContent] = UNSET
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""

    refusal: OptionalNullable[str] = UNSET
    r"""The refusal message by the assistant."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    audio: OptionalNullable[UpdatePromptMessagesAudio] = UNSET
    r"""Data about a previous audio response from the model."""

    tool_calls: Optional[List[UpdatePromptMessagesToolCalls]] = None
    r"""The tool calls generated by the model, such as function calls."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["content", "refusal", "name", "audio", "tool_calls"]
        nullable_fields = ["content", "refusal", "audio"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdatePromptMessagesPromptsRole = Literal["user",]
r"""The role of the messages author, in this case `user`."""


UpdatePrompt2PromptsRequestRequestBodyPromptMessagesType = Literal["file",]
r"""The type of the content part. Always `file`."""


UpdatePrompt2PromptsRequestRequestBodyPromptMessages2Type = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


UpdatePrompt2PromptsTTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class UpdatePrompt2PromptsCacheControlTypedDict(TypedDict):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessages2Type
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[UpdatePrompt2PromptsTTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePrompt2PromptsCacheControl(BaseModel):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessages2Type
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[UpdatePrompt2PromptsTTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePrompt24TypedDict(TypedDict):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessagesType
    r"""The type of the content part. Always `file`."""
    file: FileContentPartSchemaTypedDict
    r"""File data for the content part. Must contain either file_data or uri, but not both."""
    cache_control: NotRequired[UpdatePrompt2PromptsCacheControlTypedDict]


class UpdatePrompt24(BaseModel):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptMessagesType
    r"""The type of the content part. Always `file`."""

    file: FileContentPartSchema
    r"""File data for the content part. Must contain either file_data or uri, but not both."""

    cache_control: Optional[UpdatePrompt2PromptsCacheControl] = None


UpdatePrompt2PromptsRequestRequestBodyType = Literal["text",]


UpdatePrompt2PromptsRequestRequestBodyPromptType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


UpdatePrompt2TTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class UpdatePrompt2CacheControlTypedDict(TypedDict):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[UpdatePrompt2TTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePrompt2CacheControl(BaseModel):
    type: UpdatePrompt2PromptsRequestRequestBodyPromptType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[UpdatePrompt2TTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePrompt2Prompts1TypedDict(TypedDict):
    type: UpdatePrompt2PromptsRequestRequestBodyType
    text: str
    cache_control: NotRequired[UpdatePrompt2CacheControlTypedDict]


class UpdatePrompt2Prompts1(BaseModel):
    type: UpdatePrompt2PromptsRequestRequestBodyType

    text: str

    cache_control: Optional[UpdatePrompt2CacheControl] = None


UpdatePromptContentPromptsRequest2TypedDict = TypeAliasType(
    "UpdatePromptContentPromptsRequest2TypedDict",
    Union[
        AudioContentPartSchemaTypedDict,
        UpdatePrompt2Prompts1TypedDict,
        ImageContentPartSchemaTypedDict,
        UpdatePrompt24TypedDict,
    ],
)


UpdatePromptContentPromptsRequest2 = Annotated[
    Union[
        Annotated[UpdatePrompt2Prompts1, Tag("text")],
        Annotated[ImageContentPartSchema, Tag("image_url")],
        Annotated[AudioContentPartSchema, Tag("input_audio")],
        Annotated[UpdatePrompt24, Tag("file")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


UpdatePromptMessagesPromptsContentTypedDict = TypeAliasType(
    "UpdatePromptMessagesPromptsContentTypedDict",
    Union[str, List[UpdatePromptContentPromptsRequest2TypedDict]],
)
r"""The contents of the user message."""


UpdatePromptMessagesPromptsContent = TypeAliasType(
    "UpdatePromptMessagesPromptsContent",
    Union[str, List[UpdatePromptContentPromptsRequest2]],
)
r"""The contents of the user message."""


class UpdatePromptMessagesUserMessageTypedDict(TypedDict):
    role: UpdatePromptMessagesPromptsRole
    r"""The role of the messages author, in this case `user`."""
    content: UpdatePromptMessagesPromptsContentTypedDict
    r"""The contents of the user message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class UpdatePromptMessagesUserMessage(BaseModel):
    role: UpdatePromptMessagesPromptsRole
    r"""The role of the messages author, in this case `user`."""

    content: UpdatePromptMessagesPromptsContent
    r"""The contents of the user message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


UpdatePromptMessagesRole = Literal["system",]
r"""The role of the messages author, in this case `system`."""


UpdatePromptContentType = Literal["text",]


UpdatePromptContentPromptsType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


UpdatePromptContentTTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class UpdatePromptContentCacheControlTypedDict(TypedDict):
    type: UpdatePromptContentPromptsType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[UpdatePromptContentTTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePromptContentCacheControl(BaseModel):
    type: UpdatePromptContentPromptsType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[UpdatePromptContentTTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class UpdatePromptContentPrompts2TypedDict(TypedDict):
    type: UpdatePromptContentType
    text: str
    cache_control: NotRequired[UpdatePromptContentCacheControlTypedDict]


class UpdatePromptContentPrompts2(BaseModel):
    type: UpdatePromptContentType

    text: str

    cache_control: Optional[UpdatePromptContentCacheControl] = None


UpdatePromptMessagesContentTypedDict = TypeAliasType(
    "UpdatePromptMessagesContentTypedDict",
    Union[str, List[UpdatePromptContentPrompts2TypedDict]],
)
r"""The contents of the system message."""


UpdatePromptMessagesContent = TypeAliasType(
    "UpdatePromptMessagesContent", Union[str, List[UpdatePromptContentPrompts2]]
)
r"""The contents of the system message."""


class UpdatePromptMessagesSystemMessageTypedDict(TypedDict):
    r"""Developer-provided instructions that the model should follow, regardless of messages sent by the user."""

    role: UpdatePromptMessagesRole
    r"""The role of the messages author, in this case `system`."""
    content: UpdatePromptMessagesContentTypedDict
    r"""The contents of the system message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class UpdatePromptMessagesSystemMessage(BaseModel):
    r"""Developer-provided instructions that the model should follow, regardless of messages sent by the user."""

    role: UpdatePromptMessagesRole
    r"""The role of the messages author, in this case `system`."""

    content: UpdatePromptMessagesContent
    r"""The contents of the system message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


UpdatePromptPromptsMessagesTypedDict = TypeAliasType(
    "UpdatePromptPromptsMessagesTypedDict",
    Union[
        UpdatePromptMessagesSystemMessageTypedDict,
        UpdatePromptMessagesUserMessageTypedDict,
        UpdatePromptMessagesToolMessageTypedDict,
        UpdatePromptMessagesAssistantMessageTypedDict,
    ],
)


UpdatePromptPromptsMessages = Annotated[
    Union[
        Annotated[UpdatePromptMessagesSystemMessage, Tag("system")],
        Annotated[UpdatePromptMessagesUserMessage, Tag("user")],
        Annotated[UpdatePromptMessagesAssistantMessage, Tag("assistant")],
        Annotated[UpdatePromptMessagesToolMessage, Tag("tool")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


UpdatePromptResponseFormatPromptsRequestRequestBodyPrompt3Type = Literal["json_schema",]


class UpdatePromptResponseFormatPromptsRequestJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class UpdatePromptResponseFormatPromptsRequestJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = False
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class UpdatePromptResponseFormatPromptsJSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: UpdatePromptResponseFormatPromptsRequestRequestBodyPrompt3Type
    json_schema: UpdatePromptResponseFormatPromptsRequestJSONSchemaTypedDict


class UpdatePromptResponseFormatPromptsJSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: UpdatePromptResponseFormatPromptsRequestRequestBodyPrompt3Type

    json_schema: UpdatePromptResponseFormatPromptsRequestJSONSchema


UpdatePromptResponseFormatPromptsRequestRequestBodyPromptType = Literal["json_object",]


class UpdatePromptResponseFormatJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: UpdatePromptResponseFormatPromptsRequestRequestBodyPromptType


class UpdatePromptResponseFormatJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: UpdatePromptResponseFormatPromptsRequestRequestBodyPromptType


UpdatePromptResponseFormatPromptsRequestRequestBodyType = Literal["text",]


class UpdatePromptResponseFormatTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: UpdatePromptResponseFormatPromptsRequestRequestBodyType


class UpdatePromptResponseFormatText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: UpdatePromptResponseFormatPromptsRequestRequestBodyType


UpdatePromptPromptsResponseFormatTypedDict = TypeAliasType(
    "UpdatePromptPromptsResponseFormatTypedDict",
    Union[
        UpdatePromptResponseFormatTextTypedDict,
        UpdatePromptResponseFormatJSONObjectTypedDict,
        UpdatePromptResponseFormatPromptsJSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


UpdatePromptPromptsResponseFormat = Annotated[
    Union[
        Annotated[UpdatePromptResponseFormatText, Tag("text")],
        Annotated[UpdatePromptResponseFormatJSONObject, Tag("json_object")],
        Annotated[UpdatePromptResponseFormatPromptsJSONSchema, Tag("json_schema")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An object specifying the format that the model must output"""


class UpdatePromptPromptInputTypedDict(TypedDict):
    r"""Prompt configuration with model and messages. Use this to update the prompt."""

    messages: NotRequired[List[UpdatePromptPromptsMessagesTypedDict]]
    r"""Array of messages that make up the conversation. Each message has a role (system, user, assistant, or tool) and content."""
    model: NotRequired[str]
    r"""Model ID used to generate the response, like `openai/gpt-4o` or `anthropic/claude-3-5-sonnet-20241022`. The full list of models can be found at https://docs.orq.ai/docs/ai-gateway-supported-models. Only chat models are supported."""
    is_private: NotRequired[bool]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    response_format: NotRequired[UpdatePromptPromptsResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""


class UpdatePromptPromptInput(BaseModel):
    r"""Prompt configuration with model and messages. Use this to update the prompt."""

    messages: Optional[List[UpdatePromptPromptsMessages]] = None
    r"""Array of messages that make up the conversation. Each message has a role (system, user, assistant, or tool) and content."""

    model: Optional[str] = None
    r"""Model ID used to generate the response, like `openai/gpt-4o` or `anthropic/claude-3-5-sonnet-20241022`. The full list of models can be found at https://docs.orq.ai/docs/ai-gateway-supported-models. Only chat models are supported."""

    is_private: Optional[bool] = False

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    response_format: Optional[UpdatePromptPromptsResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "messages",
            "model",
            "is_private",
            "temperature",
            "max_tokens",
            "response_format",
        ]
        nullable_fields = ["temperature", "max_tokens"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class UpdatePromptRequestBodyTypedDict(TypedDict):
    owner: NotRequired[str]
    domain_id: NotRequired[str]
    created: NotRequired[str]
    updated: NotRequired[str]
    created_by_id: NotRequired[Nullable[str]]
    updated_by_id: NotRequired[Nullable[str]]
    display_name: NotRequired[str]
    r"""The prompts name, meant to be displayable in the UI."""
    description: NotRequired[Nullable[str]]
    r"""The prompts description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""
    prompt_config: NotRequired[PromptConfigTypedDict]
    r"""[DEPRECATED]. Please use the `prompt` property instead."""
    metadata: NotRequired[UpdatePromptMetadataTypedDict]
    path: NotRequired[str]
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """
    prompt: NotRequired[UpdatePromptPromptInputTypedDict]
    r"""Prompt configuration with model and messages. Use this to update the prompt."""


class UpdatePromptRequestBody(BaseModel):
    owner: Optional[str] = None

    domain_id: Optional[str] = None

    created: Optional[str] = None

    updated: Optional[str] = None

    created_by_id: OptionalNullable[str] = UNSET

    updated_by_id: OptionalNullable[str] = UNSET

    display_name: Optional[str] = None
    r"""The prompts name, meant to be displayable in the UI."""

    description: OptionalNullable[str] = UNSET
    r"""The prompts description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    prompt_config: Annotated[
        Optional[PromptConfig],
        pydantic.Field(
            deprecated="warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
        ),
    ] = None
    r"""[DEPRECATED]. Please use the `prompt` property instead."""

    metadata: Optional[UpdatePromptMetadata] = None

    path: Optional[str] = None
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """

    prompt: Optional[UpdatePromptPromptInput] = None
    r"""Prompt configuration with model and messages. Use this to update the prompt."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "owner",
            "domain_id",
            "created",
            "updated",
            "created_by_id",
            "updated_by_id",
            "display_name",
            "description",
            "prompt_config",
            "metadata",
            "path",
            "prompt",
        ]
        nullable_fields = ["created_by_id", "updated_by_id", "description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class UpdatePromptRequestTypedDict(TypedDict):
    id: str
    r"""Unique identifier of the prompt"""
    request_body: NotRequired[UpdatePromptRequestBodyTypedDict]


class UpdatePromptRequest(BaseModel):
    id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""Unique identifier of the prompt"""

    request_body: Annotated[
        Optional[UpdatePromptRequestBody],
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ] = None


class UpdatePromptResponseBodyData(BaseModel):
    message: str


@dataclass(unsafe_hash=True)
class UpdatePromptResponseBody(OrqError):
    r"""Prompt not found."""

    data: UpdatePromptResponseBodyData = field(hash=False)

    def __init__(
        self,
        data: UpdatePromptResponseBodyData,
        raw_response: httpx.Response,
        body: Optional[str] = None,
    ):
        fallback = body or raw_response.text
        message = str(data.message) or fallback
        super().__init__(message, raw_response, body)
        object.__setattr__(self, "data", data)


UpdatePromptPromptsType = Literal["prompt",]


UpdatePromptModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderation",
    "vision",
]
r"""The modality of the model"""


UpdatePromptPromptsFormat = Literal[
    "url",
    "b64_json",
    "text",
    "json_object",
]
r"""Only supported on `image` models."""


UpdatePromptResponseFormat6 = Literal[
    "json",
    "text",
    "srt",
    "verbose_json",
    "vtt",
]


UpdatePromptResponseFormat5 = Literal[
    "url",
    "base64_json",
]


UpdatePromptResponseFormatPrompts4 = Literal[
    "mp3",
    "opus",
    "aac",
    "flac",
    "wav",
    "pcm",
]


UpdatePromptResponseFormatPromptsResponse200ApplicationJSONType = Literal["text",]


class UpdatePromptResponseFormatPrompts3TypedDict(TypedDict):
    type: UpdatePromptResponseFormatPromptsResponse200ApplicationJSONType


class UpdatePromptResponseFormatPrompts3(BaseModel):
    type: UpdatePromptResponseFormatPromptsResponse200ApplicationJSONType


UpdatePromptResponseFormatPromptsResponse200Type = Literal["json_object",]


class UpdatePromptResponseFormatPrompts2TypedDict(TypedDict):
    type: UpdatePromptResponseFormatPromptsResponse200Type


class UpdatePromptResponseFormatPrompts2(BaseModel):
    type: UpdatePromptResponseFormatPromptsResponse200Type


UpdatePromptResponseFormatPromptsResponseType = Literal["json_schema",]


class UpdatePromptResponseFormatPromptsResponseJSONSchemaTypedDict(TypedDict):
    name: str
    schema_: Dict[str, Any]
    description: NotRequired[str]
    strict: NotRequired[bool]


class UpdatePromptResponseFormatPromptsResponseJSONSchema(BaseModel):
    name: str

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]

    description: Optional[str] = None

    strict: Optional[bool] = None


class UpdatePromptResponseFormatPrompts1TypedDict(TypedDict):
    type: UpdatePromptResponseFormatPromptsResponseType
    json_schema: UpdatePromptResponseFormatPromptsResponseJSONSchemaTypedDict
    display_name: NotRequired[str]


class UpdatePromptResponseFormatPrompts1(BaseModel):
    type: UpdatePromptResponseFormatPromptsResponseType

    json_schema: UpdatePromptResponseFormatPromptsResponseJSONSchema

    display_name: Optional[str] = None


UpdatePromptPromptsResponseResponseFormatTypedDict = TypeAliasType(
    "UpdatePromptPromptsResponseResponseFormatTypedDict",
    Union[
        UpdatePromptResponseFormatPrompts2TypedDict,
        UpdatePromptResponseFormatPrompts3TypedDict,
        UpdatePromptResponseFormatPrompts1TypedDict,
        UpdatePromptResponseFormatPrompts4,
        UpdatePromptResponseFormat5,
        UpdatePromptResponseFormat6,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


UpdatePromptPromptsResponseResponseFormat = TypeAliasType(
    "UpdatePromptPromptsResponseResponseFormat",
    Union[
        UpdatePromptResponseFormatPrompts2,
        UpdatePromptResponseFormatPrompts3,
        UpdatePromptResponseFormatPrompts1,
        UpdatePromptResponseFormatPrompts4,
        UpdatePromptResponseFormat5,
        UpdatePromptResponseFormat6,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


UpdatePromptPromptsPhotoRealVersion = Literal[
    "v1",
    "v2",
]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""


UpdatePromptPromptsEncodingFormat = Literal[
    "float",
    "base64",
]
r"""The format to return the embeddings"""


UpdatePromptPromptsReasoningEffort = Literal[
    "none",
    "disable",
    "minimal",
    "low",
    "medium",
    "high",
]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


UpdatePromptPromptsVerbosity = Literal[
    "low",
    "medium",
    "high",
]
r"""Controls the verbosity of the model output."""


UpdatePromptPromptsThinkingLevel = Literal[
    "low",
    "high",
]
r"""The level of thinking to use for the model. Only supported by `Google AI`"""


class UpdatePromptPromptsModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[UpdatePromptPromptsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[str]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[UpdatePromptPromptsResponseResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[UpdatePromptPromptsPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[UpdatePromptPromptsEncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[UpdatePromptPromptsReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""
    budget_tokens: NotRequired[float]
    r"""Gives the model enhanced reasoning capabilities for complex tasks. A value of 0 disables thinking. The minimum budget tokens for thinking are 1024. The Budget Tokens should never exceed the Max Tokens parameter. Only supported by `Anthropic`"""
    verbosity: NotRequired[UpdatePromptPromptsVerbosity]
    r"""Controls the verbosity of the model output."""
    thinking_level: NotRequired[UpdatePromptPromptsThinkingLevel]
    r"""The level of thinking to use for the model. Only supported by `Google AI`"""


class UpdatePromptPromptsModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[
        Optional[UpdatePromptPromptsFormat], pydantic.Field(alias="format")
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[str] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[UpdatePromptPromptsResponseResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[UpdatePromptPromptsPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[UpdatePromptPromptsEncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[UpdatePromptPromptsReasoningEffort],
        pydantic.Field(alias="reasoningEffort"),
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    budget_tokens: Annotated[Optional[float], pydantic.Field(alias="budgetTokens")] = (
        None
    )
    r"""Gives the model enhanced reasoning capabilities for complex tasks. A value of 0 disables thinking. The minimum budget tokens for thinking are 1024. The Budget Tokens should never exceed the Max Tokens parameter. Only supported by `Anthropic`"""

    verbosity: Optional[UpdatePromptPromptsVerbosity] = None
    r"""Controls the verbosity of the model output."""

    thinking_level: Annotated[
        Optional[UpdatePromptPromptsThinkingLevel],
        pydantic.Field(alias="thinkingLevel"),
    ] = None
    r"""The level of thinking to use for the model. Only supported by `Google AI`"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
            "reasoningEffort",
            "budgetTokens",
            "verbosity",
            "thinkingLevel",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdatePromptProvider = Literal[
    "cohere",
    "openai",
    "anthropic",
    "huggingface",
    "replicate",
    "google",
    "google-ai",
    "azure",
    "aws",
    "anyscale",
    "perplexity",
    "groq",
    "fal",
    "leonardoai",
    "nvidia",
    "jina",
    "togetherai",
    "elevenlabs",
    "litellm",
    "openailike",
    "cerebras",
    "bytedance",
    "mistral",
]


UpdatePromptPromptsRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""


UpdatePrompt2PromptsResponse200ApplicationJSONType = Literal["file",]
r"""The type of the content part. Always `file`."""


class UpdatePrompt2PromptsFileTypedDict(TypedDict):
    file_data: NotRequired[str]
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""
    uri: NotRequired[str]
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""
    mime_type: NotRequired[str]
    r"""MIME type of the file (e.g., application/pdf, image/png)"""
    filename: NotRequired[str]
    r"""The name of the file, used when passing the file to the model as a string."""


class UpdatePrompt2PromptsFile(BaseModel):
    file_data: Optional[str] = None
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""

    uri: Optional[str] = None
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""MIME type of the file (e.g., application/pdf, image/png)"""

    filename: Optional[str] = None
    r"""The name of the file, used when passing the file to the model as a string."""


class UpdatePrompt2Prompts3TypedDict(TypedDict):
    type: UpdatePrompt2PromptsResponse200ApplicationJSONType
    r"""The type of the content part. Always `file`."""
    file: UpdatePrompt2PromptsFileTypedDict


class UpdatePrompt2Prompts3(BaseModel):
    type: UpdatePrompt2PromptsResponse200ApplicationJSONType
    r"""The type of the content part. Always `file`."""

    file: UpdatePrompt2PromptsFile


UpdatePrompt2PromptsResponse200Type = Literal["image_url",]


class UpdatePrompt2PromptsImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class UpdatePrompt2PromptsImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class UpdatePrompt2Prompts2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: UpdatePrompt2PromptsResponse200Type
    image_url: UpdatePrompt2PromptsImageURLTypedDict


class UpdatePrompt2Prompts2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: UpdatePrompt2PromptsResponse200Type

    image_url: UpdatePrompt2PromptsImageURL


UpdatePrompt2PromptsResponseType = Literal["text",]


class UpdatePrompt2PromptsResponse1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: UpdatePrompt2PromptsResponseType
    text: str


class UpdatePrompt2PromptsResponse1(BaseModel):
    r"""Text content part of a prompt message"""

    type: UpdatePrompt2PromptsResponseType

    text: str


UpdatePromptContentPromptsResponse2TypedDict = TypeAliasType(
    "UpdatePromptContentPromptsResponse2TypedDict",
    Union[
        UpdatePrompt2PromptsResponse1TypedDict,
        UpdatePrompt2Prompts2TypedDict,
        UpdatePrompt2Prompts3TypedDict,
    ],
)


UpdatePromptContentPromptsResponse2 = Annotated[
    Union[
        Annotated[UpdatePrompt2PromptsResponse1, Tag("text")],
        Annotated[UpdatePrompt2Prompts2, Tag("image_url")],
        Annotated[UpdatePrompt2Prompts3, Tag("file")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


UpdatePromptPromptsContentTypedDict = TypeAliasType(
    "UpdatePromptPromptsContentTypedDict",
    Union[str, List[UpdatePromptContentPromptsResponse2TypedDict]],
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""


UpdatePromptPromptsContent = TypeAliasType(
    "UpdatePromptPromptsContent", Union[str, List[UpdatePromptContentPromptsResponse2]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""


UpdatePromptPromptsResponseType = Literal["function",]


class UpdatePromptPromptsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class UpdatePromptPromptsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class UpdatePromptPromptsToolCallsTypedDict(TypedDict):
    type: UpdatePromptPromptsResponseType
    function: UpdatePromptPromptsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class UpdatePromptPromptsToolCalls(BaseModel):
    type: UpdatePromptPromptsResponseType

    function: UpdatePromptPromptsFunction

    id: Optional[str] = None

    index: Optional[float] = None


class UpdatePromptPromptsResponseMessagesTypedDict(TypedDict):
    role: UpdatePromptPromptsRole
    r"""The role of the prompt message"""
    content: Nullable[UpdatePromptPromptsContentTypedDict]
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""
    tool_calls: NotRequired[List[UpdatePromptPromptsToolCallsTypedDict]]
    tool_call_id: NotRequired[str]


class UpdatePromptPromptsResponseMessages(BaseModel):
    role: UpdatePromptPromptsRole
    r"""The role of the prompt message"""

    content: Nullable[UpdatePromptPromptsContent]
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""

    tool_calls: Optional[List[UpdatePromptPromptsToolCalls]] = None

    tool_call_id: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["tool_calls", "tool_call_id"]
        nullable_fields = ["content"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class UpdatePromptPromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[UpdatePromptPromptsResponseMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    model_type: NotRequired[Nullable[UpdatePromptModelType]]
    r"""The modality of the model"""
    model_parameters: NotRequired[UpdatePromptPromptsModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[UpdatePromptProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The ID of the integration to use"""
    version: NotRequired[str]


class UpdatePromptPromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[UpdatePromptPromptsResponseMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    model_type: OptionalNullable[UpdatePromptModelType] = UNSET
    r"""The modality of the model"""

    model_parameters: Optional[UpdatePromptPromptsModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[UpdatePromptProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The ID of the integration to use"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["model_db_id", "model_type", "integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdatePromptPromptsUseCases = Literal[
    "Agents simulations",
    "Agents",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Conversation",
    "Documents QA",
    "Evaluation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "Sentiment analysis",
    "SQL",
    "Summarization",
    "Tagging",
    "Translation (document)",
    "Translation (sentences)",
]


UpdatePromptPromptsLanguage = Literal[
    "Chinese",
    "Dutch",
    "English",
    "French",
    "German",
    "Russian",
    "Spanish",
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptPromptsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[UpdatePromptPromptsUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[Nullable[UpdatePromptPromptsLanguage]]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class UpdatePromptPromptsMetadata(BaseModel):
    use_cases: Optional[List[UpdatePromptPromptsUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: OptionalNullable[UpdatePromptPromptsLanguage] = UNSET
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["use_cases", "language"]
        nullable_fields = ["language"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class UpdatePromptPromptTypedDict(TypedDict):
    r"""A prompt entity with configuration, metadata, and versioning."""

    id: str
    type: UpdatePromptPromptsType
    owner: str
    domain_id: str
    created: str
    updated: str
    display_name: str
    r"""The prompts name, meant to be displayable in the UI."""
    prompt_config: UpdatePromptPromptConfigTypedDict
    r"""A list of messages compatible with the openAI schema"""
    created_by_id: NotRequired[Nullable[str]]
    updated_by_id: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]
    r"""The prompts description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""
    metadata: NotRequired[UpdatePromptPromptsMetadataTypedDict]


class UpdatePromptPrompt(BaseModel):
    r"""A prompt entity with configuration, metadata, and versioning."""

    id: Annotated[str, pydantic.Field(alias="_id")]

    type: UpdatePromptPromptsType

    owner: str

    domain_id: str

    created: str

    updated: str

    display_name: str
    r"""The prompts name, meant to be displayable in the UI."""

    prompt_config: UpdatePromptPromptConfig
    r"""A list of messages compatible with the openAI schema"""

    created_by_id: OptionalNullable[str] = UNSET

    updated_by_id: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET
    r"""The prompts description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    metadata: Optional[UpdatePromptPromptsMetadata] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["created_by_id", "updated_by_id", "description", "metadata"]
        nullable_fields = ["created_by_id", "updated_by_id", "description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
