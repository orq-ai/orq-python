"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .publiccontact import PublicContact, PublicContactTypedDict
from dataclasses import dataclass, field
import httpx
import io
from orq_ai_sdk.models import OrqError
from orq_ai_sdk.types import BaseModel, Nullable, UNSET_SENTINEL
from orq_ai_sdk.utils import FieldMetadata, MultipartFormMetadata
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, IO, List, Literal, Optional, Union
from typing_extensions import (
    Annotated,
    NotRequired,
    TypeAliasType,
    TypedDict,
    deprecated,
)


CreateTranscriptionResponseFormat = Literal[
    "json",
    "text",
    "srt",
    "verbose_json",
    "vtt",
]
r"""The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt."""


TimestampsGranularity = Literal[
    "none",
    "word",
    "character",
]
r"""The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word."""


TimestampGranularities = Literal[
    "word",
    "segment",
]


class CreateTranscriptionFallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class CreateTranscriptionFallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


class CreateTranscriptionRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class CreateTranscriptionRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


@deprecated(
    "warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
)
class CreateTranscriptionContactTypedDict(TypedDict):
    r"""@deprecated Use identity instead. Information about the contact making the request."""

    id: str
    r"""Unique identifier for the contact"""
    display_name: NotRequired[str]
    r"""Display name of the contact"""
    email: NotRequired[str]
    r"""Email address of the contact"""
    metadata: NotRequired[List[Dict[str, Any]]]
    r"""A hash of key/value pairs containing any other data about the contact"""
    logo_url: NotRequired[str]
    r"""URL to the contact's avatar or logo"""
    tags: NotRequired[List[str]]
    r"""A list of tags associated with the contact"""


@deprecated(
    "warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
)
class CreateTranscriptionContact(BaseModel):
    r"""@deprecated Use identity instead. Information about the contact making the request."""

    id: str
    r"""Unique identifier for the contact"""

    display_name: Optional[str] = None
    r"""Display name of the contact"""

    email: Optional[str] = None
    r"""Email address of the contact"""

    metadata: Optional[List[Dict[str, Any]]] = None
    r"""A hash of key/value pairs containing any other data about the contact"""

    logo_url: Optional[str] = None
    r"""URL to the contact's avatar or logo"""

    tags: Optional[List[str]] = None
    r"""A list of tags associated with the contact"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["display_name", "email", "metadata", "logo_url", "tags"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateTranscriptionLoadBalancerType = Literal["weight_based",]


class CreateTranscriptionLoadBalancerModelsTypedDict(TypedDict):
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class CreateTranscriptionLoadBalancerModels(BaseModel):
    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateTranscriptionLoadBalancer1TypedDict(TypedDict):
    type: CreateTranscriptionLoadBalancerType
    models: List[CreateTranscriptionLoadBalancerModelsTypedDict]


class CreateTranscriptionLoadBalancer1(BaseModel):
    type: CreateTranscriptionLoadBalancerType

    models: List[CreateTranscriptionLoadBalancerModels]


CreateTranscriptionLoadBalancerTypedDict = CreateTranscriptionLoadBalancer1TypedDict
r"""Array of models with weights for load balancing requests"""


CreateTranscriptionLoadBalancer = CreateTranscriptionLoadBalancer1
r"""Array of models with weights for load balancing requests"""


class CreateTranscriptionTimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class CreateTranscriptionTimeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class CreateTranscriptionOrqTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    fallbacks: NotRequired[List[CreateTranscriptionFallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    retry: NotRequired[CreateTranscriptionRetryTypedDict]
    r"""Retry configuration for the request"""
    identity: NotRequired[PublicContactTypedDict]
    r"""Information about the identity making the request. If the identity does not exist, it will be created automatically."""
    contact: NotRequired[CreateTranscriptionContactTypedDict]
    load_balancer: NotRequired[CreateTranscriptionLoadBalancerTypedDict]
    r"""Array of models with weights for load balancing requests"""
    timeout: NotRequired[CreateTranscriptionTimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


class CreateTranscriptionOrq(BaseModel):
    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    fallbacks: Optional[List[CreateTranscriptionFallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    retry: Optional[CreateTranscriptionRetry] = None
    r"""Retry configuration for the request"""

    identity: Optional[PublicContact] = None
    r"""Information about the identity making the request. If the identity does not exist, it will be created automatically."""

    contact: Optional[CreateTranscriptionContact] = None

    load_balancer: Optional[CreateTranscriptionLoadBalancer] = None
    r"""Array of models with weights for load balancing requests"""

    timeout: Optional[CreateTranscriptionTimeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "name",
                "fallbacks",
                "retry",
                "identity",
                "contact",
                "load_balancer",
                "timeout",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateTranscriptionFileTypedDict(TypedDict):
    file_name: str
    content: Union[bytes, IO[bytes], io.BufferedReader]
    content_type: NotRequired[str]


class CreateTranscriptionFile(BaseModel):
    file_name: Annotated[
        str, pydantic.Field(alias="fileName"), FieldMetadata(multipart=True)
    ]

    content: Annotated[
        Union[bytes, IO[bytes], io.BufferedReader],
        pydantic.Field(alias=""),
        FieldMetadata(multipart=MultipartFormMetadata(content=True)),
    ]

    content_type: Annotated[
        Optional[str],
        pydantic.Field(alias="Content-Type"),
        FieldMetadata(multipart=True),
    ] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["contentType"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateTranscriptionRequestBodyTypedDict(TypedDict):
    r"""Transcribes audio into the input language."""

    model: str
    r"""ID of the model to use"""
    prompt: NotRequired[str]
    r"""An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language."""
    enable_logging: NotRequired[bool]
    r"""When enable_logging is set to false, zero retention mode is used. This disables history features like request stitching and is only available to enterprise customers."""
    diarize: NotRequired[bool]
    r"""Whether to annotate which speaker is currently talking in the uploaded file."""
    response_format: NotRequired[CreateTranscriptionResponseFormat]
    r"""The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt."""
    tag_audio_events: NotRequired[bool]
    r"""Whether to tag audio events like (laughter), (footsteps), etc. in the transcription."""
    num_speakers: NotRequired[float]
    r"""The maximum amount of speakers talking in the uploaded file. Helps with predicting who speaks when, the maximum is 32."""
    timestamps_granularity: NotRequired[TimestampsGranularity]
    r"""The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word."""
    temperature: NotRequired[float]
    r"""The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit."""
    language: NotRequired[str]
    r"""The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency."""
    timestamp_granularities: NotRequired[List[TimestampGranularities]]
    r"""The timestamp granularities to populate for this transcription. response_format must be set to verbose_json to use timestamp granularities. Either or both of these options are supported: \"word\" or \"segment\". Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency."""
    orq: NotRequired[CreateTranscriptionOrqTypedDict]
    file: NotRequired[CreateTranscriptionFileTypedDict]
    r"""The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm."""


class CreateTranscriptionRequestBody(BaseModel):
    r"""Transcribes audio into the input language."""

    model: Annotated[str, FieldMetadata(multipart=True)]
    r"""ID of the model to use"""

    prompt: Annotated[Optional[str], FieldMetadata(multipart=True)] = None
    r"""An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language."""

    enable_logging: Annotated[Optional[bool], FieldMetadata(multipart=True)] = True
    r"""When enable_logging is set to false, zero retention mode is used. This disables history features like request stitching and is only available to enterprise customers."""

    diarize: Annotated[Optional[bool], FieldMetadata(multipart=True)] = False
    r"""Whether to annotate which speaker is currently talking in the uploaded file."""

    response_format: Annotated[
        Optional[CreateTranscriptionResponseFormat], FieldMetadata(multipart=True)
    ] = None
    r"""The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt."""

    tag_audio_events: Annotated[Optional[bool], FieldMetadata(multipart=True)] = True
    r"""Whether to tag audio events like (laughter), (footsteps), etc. in the transcription."""

    num_speakers: Annotated[Optional[float], FieldMetadata(multipart=True)] = None
    r"""The maximum amount of speakers talking in the uploaded file. Helps with predicting who speaks when, the maximum is 32."""

    timestamps_granularity: Annotated[
        Optional[TimestampsGranularity], FieldMetadata(multipart=True)
    ] = "word"
    r"""The granularity of the timestamps in the transcription. Word provides word-level timestamps and character provides character-level timestamps per word."""

    temperature: Annotated[Optional[float], FieldMetadata(multipart=True)] = None
    r"""The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit."""

    language: Annotated[Optional[str], FieldMetadata(multipart=True)] = None
    r"""The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency."""

    timestamp_granularities: Annotated[
        Optional[List[TimestampGranularities]], FieldMetadata(multipart=True)
    ] = None
    r"""The timestamp granularities to populate for this transcription. response_format must be set to verbose_json to use timestamp granularities. Either or both of these options are supported: \"word\" or \"segment\". Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency."""

    orq: Annotated[
        Optional[CreateTranscriptionOrq],
        FieldMetadata(multipart=MultipartFormMetadata(json=True)),
    ] = None

    file: Annotated[
        Optional[CreateTranscriptionFile],
        FieldMetadata(multipart=MultipartFormMetadata(file=True)),
    ] = None
    r"""The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "prompt",
                "enable_logging",
                "diarize",
                "response_format",
                "tag_audio_events",
                "num_speakers",
                "timestamps_granularity",
                "temperature",
                "language",
                "timestamp_granularities",
                "orq",
                "file",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateTranscriptionErrorTypedDict(TypedDict):
    message: str
    type: str
    param: Nullable[str]
    code: str


class CreateTranscriptionError(BaseModel):
    message: str

    type: str

    param: Nullable[str]

    code: str

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateTranscriptionRouterAudioTranscriptionsResponseBodyData(BaseModel):
    error: CreateTranscriptionError


@dataclass(unsafe_hash=True)
class CreateTranscriptionRouterAudioTranscriptionsResponseBody(OrqError):
    r"""Returns validation error"""

    data: CreateTranscriptionRouterAudioTranscriptionsResponseBodyData = field(
        hash=False
    )

    def __init__(
        self,
        data: CreateTranscriptionRouterAudioTranscriptionsResponseBodyData,
        raw_response: httpx.Response,
        body: Optional[str] = None,
    ):
        fallback = body or raw_response.text
        message = str(data.error.message) or fallback
        super().__init__(message, raw_response, body)
        object.__setattr__(self, "data", data)


class WordsTypedDict(TypedDict):
    word: NotRequired[str]
    start: NotRequired[float]
    end: NotRequired[float]


class Words(BaseModel):
    word: Optional[str] = None

    start: Optional[float] = None

    end: Optional[float] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["word", "start", "end"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class SegmentsTypedDict(TypedDict):
    id: float
    seek: float
    start: float
    end: float
    text: str
    tokens: List[float]
    temperature: float
    avg_logprob: float
    compression_ratio: float
    no_speech_prob: float


class Segments(BaseModel):
    id: float

    seek: float

    start: float

    end: float

    text: str

    tokens: List[float]

    temperature: float

    avg_logprob: float

    compression_ratio: float

    no_speech_prob: float


class CreateTranscriptionResponseBody2TypedDict(TypedDict):
    text: str
    task: NotRequired[str]
    language: NotRequired[str]
    duration: NotRequired[float]
    words: NotRequired[List[WordsTypedDict]]
    segments: NotRequired[List[SegmentsTypedDict]]


class CreateTranscriptionResponseBody2(BaseModel):
    text: str

    task: Optional[str] = None

    language: Optional[str] = None

    duration: Optional[float] = None

    words: Optional[List[Words]] = None

    segments: Optional[List[Segments]] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["task", "language", "duration", "words", "segments"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateTranscriptionResponseBody1TypedDict(TypedDict):
    text: str


class CreateTranscriptionResponseBody1(BaseModel):
    text: str


CreateTranscriptionResponseBodyTypedDict = TypeAliasType(
    "CreateTranscriptionResponseBodyTypedDict",
    Union[
        CreateTranscriptionResponseBody1TypedDict,
        CreateTranscriptionResponseBody2TypedDict,
        str,
    ],
)
r"""Returns the transcription or verbose transcription"""


CreateTranscriptionResponseBody = TypeAliasType(
    "CreateTranscriptionResponseBody",
    Union[CreateTranscriptionResponseBody1, CreateTranscriptionResponseBody2, str],
)
r"""Returns the transcription or verbose transcription"""
