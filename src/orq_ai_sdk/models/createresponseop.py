"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import eventstreaming
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


Effort = Literal["low", "medium", "high"]
r"""The effort level for reasoning (o3-mini model only)"""


class ReasoningTypedDict(TypedDict):
    r"""Configuration for reasoning models"""

    effort: NotRequired[Effort]
    r"""The effort level for reasoning (o3-mini model only)"""


class Reasoning(BaseModel):
    r"""Configuration for reasoning models"""

    effort: Optional[Effort] = None
    r"""The effort level for reasoning (o3-mini model only)"""


CreateResponseFormatProxyType = Literal["json_schema"]
r"""Ensures the response matches a supplied JSON schema"""


class Format3TypedDict(TypedDict):
    type: CreateResponseFormatProxyType
    r"""Ensures the response matches a supplied JSON schema"""
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    schema_: Dict[str, Any]
    r"""The JSON schema to validate the response against"""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    strict: NotRequired[Nullable[bool]]
    r"""Whether to enable strict `schema` adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when `strict` is `true`"""


class Format3(BaseModel):
    type: CreateResponseFormatProxyType
    r"""Ensures the response matches a supplied JSON schema"""

    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]
    r"""The JSON schema to validate the response against"""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    strict: OptionalNullable[bool] = True
    r"""Whether to enable strict `schema` adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when `strict` is `true`"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "strict"]
        nullable_fields = ["strict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateResponseFormatType = Literal["json_object"]
r"""Ensures the response is a valid JSON object"""


class Format2TypedDict(TypedDict):
    type: CreateResponseFormatType
    r"""Ensures the response is a valid JSON object"""


class Format2(BaseModel):
    type: CreateResponseFormatType
    r"""Ensures the response is a valid JSON object"""


FormatType = Literal["text"]
r"""Plain text response format"""


class Format1TypedDict(TypedDict):
    type: FormatType
    r"""Plain text response format"""


class Format1(BaseModel):
    type: FormatType
    r"""Plain text response format"""


CreateResponseFormatTypedDict = TypeAliasType(
    "CreateResponseFormatTypedDict",
    Union[Format1TypedDict, Format2TypedDict, Format3TypedDict],
)


CreateResponseFormat = TypeAliasType(
    "CreateResponseFormat", Union[Format1, Format2, Format3]
)


class TextTypedDict(TypedDict):
    format_: CreateResponseFormatTypedDict


class Text(BaseModel):
    format_: Annotated[CreateResponseFormat, pydantic.Field(alias="format")]


CreateResponse2ProxyRequestRequestBodyType = Literal["function_call_output"]
r"""The type of input item"""


class CreateResponse2Proxy2TypedDict(TypedDict):
    r"""Represents the output of a function tool call, provided as input to the model."""

    type: CreateResponse2ProxyRequestRequestBodyType
    r"""The type of input item"""
    call_id: str
    r"""The ID of the function call this output is for"""
    output: str
    r"""The output from the function call"""


class CreateResponse2Proxy2(BaseModel):
    r"""Represents the output of a function tool call, provided as input to the model."""

    type: CreateResponse2ProxyRequestRequestBodyType
    r"""The type of input item"""

    call_id: str
    r"""The ID of the function call this output is for"""

    output: str
    r"""The output from the function call"""


TwoRole = Literal["user", "assistant", "system", "developer"]
r"""The role of the message author"""

CreateResponse2ProxyRequestType = Literal["input_file"]
r"""The type of input content part"""


class CreateResponse23TypedDict(TypedDict):
    r"""A file input content part."""

    type: CreateResponse2ProxyRequestType
    r"""The type of input content part"""
    file_data: NotRequired[str]
    r"""Base64 encoded file data"""
    file_id: NotRequired[str]
    r"""File ID from the Files API"""
    filename: NotRequired[str]
    r"""Name of the file"""
    file_url: NotRequired[str]
    r"""URL of the file to fetch"""


class CreateResponse23(BaseModel):
    r"""A file input content part."""

    type: CreateResponse2ProxyRequestType
    r"""The type of input content part"""

    file_data: Optional[str] = None
    r"""Base64 encoded file data"""

    file_id: Optional[str] = None
    r"""File ID from the Files API"""

    filename: Optional[str] = None
    r"""Name of the file"""

    file_url: Optional[str] = None
    r"""URL of the file to fetch"""


CreateResponse2ProxyType = Literal["input_image"]
r"""The type of input content part"""

CreateResponse2Detail = Literal["high", "low", "auto"]
r"""Level of detail for image analysis"""


class CreateResponse22TypedDict(TypedDict):
    r"""An image input content part."""

    type: CreateResponse2ProxyType
    r"""The type of input content part"""
    detail: NotRequired[CreateResponse2Detail]
    r"""Level of detail for image analysis"""
    file_id: NotRequired[Nullable[str]]
    r"""File ID for the image"""
    image_url: NotRequired[Nullable[str]]
    r"""URL of the image (can be http URL or data URL)"""


class CreateResponse22(BaseModel):
    r"""An image input content part."""

    type: CreateResponse2ProxyType
    r"""The type of input content part"""

    detail: Optional[CreateResponse2Detail] = "auto"
    r"""Level of detail for image analysis"""

    file_id: OptionalNullable[str] = UNSET
    r"""File ID for the image"""

    image_url: OptionalNullable[str] = UNSET
    r"""URL of the image (can be http URL or data URL)"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["detail", "file_id", "image_url"]
        nullable_fields = ["file_id", "image_url"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateResponse2Type = Literal["input_text"]
r"""The type of input content"""


class CreateResponse2Proxy1TypedDict(TypedDict):
    r"""A text input content part"""

    type: CreateResponse2Type
    r"""The type of input content"""
    text: str
    r"""The text content"""


class CreateResponse2Proxy1(BaseModel):
    r"""A text input content part"""

    type: CreateResponse2Type
    r"""The type of input content"""

    text: str
    r"""The text content"""


CreateResponseContent2TypedDict = TypeAliasType(
    "CreateResponseContent2TypedDict",
    Union[
        CreateResponse2Proxy1TypedDict,
        CreateResponse22TypedDict,
        CreateResponse23TypedDict,
    ],
)


CreateResponseContent2 = TypeAliasType(
    "CreateResponseContent2",
    Union[CreateResponse2Proxy1, CreateResponse22, CreateResponse23],
)


TwoContentTypedDict = TypeAliasType(
    "TwoContentTypedDict", Union[str, List[CreateResponseContent2TypedDict]]
)
r"""The content of the message, either a string or an array of content parts"""


TwoContent = TypeAliasType("TwoContent", Union[str, List[CreateResponseContent2]])
r"""The content of the message, either a string or an array of content parts"""


class CreateResponse21TypedDict(TypedDict):
    r"""Represents a message in the conversation, with a role and content (string or rich content parts)."""

    role: TwoRole
    r"""The role of the message author"""
    content: TwoContentTypedDict
    r"""The content of the message, either a string or an array of content parts"""


class CreateResponse21(BaseModel):
    r"""Represents a message in the conversation, with a role and content (string or rich content parts)."""

    role: TwoRole
    r"""The role of the message author"""

    content: TwoContent
    r"""The content of the message, either a string or an array of content parts"""


Input2TypedDict = TypeAliasType(
    "Input2TypedDict", Union[CreateResponse21TypedDict, CreateResponse2Proxy2TypedDict]
)


Input2 = TypeAliasType("Input2", Union[CreateResponse21, CreateResponse2Proxy2])


CreateResponseInputTypedDict = TypeAliasType(
    "CreateResponseInputTypedDict", Union[str, List[Input2TypedDict]]
)
r"""The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input."""


CreateResponseInput = TypeAliasType("CreateResponseInput", Union[str, List[Input2]])
r"""The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input."""


Include = Literal[
    "code_interpreter_call.outputs",
    "computer_call_output.output.image_url",
    "file_search_call.results",
    "message.input_image.image_url",
    "message.output_text.logprobs",
    "reasoning.encrypted_content",
]

CreateResponseToolsProxyRequestRequestBodyType = Literal["file_search"]
r"""The type of tool"""

Ranker = Literal["auto", "default_2024_08_21"]
r"""The ranking algorithm"""


class RankingOptionsTypedDict(TypedDict):
    r"""Options for ranking search results"""

    ranker: NotRequired[Ranker]
    r"""The ranking algorithm"""
    score_threshold: NotRequired[float]
    r"""Minimum relevance score"""


class RankingOptions(BaseModel):
    r"""Options for ranking search results"""

    ranker: Optional[Ranker] = "auto"
    r"""The ranking algorithm"""

    score_threshold: Optional[float] = 0
    r"""Minimum relevance score"""


class Tools3TypedDict(TypedDict):
    r"""Configuration for file search tool"""

    type: CreateResponseToolsProxyRequestRequestBodyType
    r"""The type of tool"""
    vector_store_ids: NotRequired[List[str]]
    r"""The vector stores to search"""
    max_num_results: NotRequired[int]
    r"""Maximum number of results to return"""
    filters: NotRequired[Any]
    r"""Filters to apply to the search"""
    ranking_options: NotRequired[RankingOptionsTypedDict]
    r"""Options for ranking search results"""


class Tools3(BaseModel):
    r"""Configuration for file search tool"""

    type: CreateResponseToolsProxyRequestRequestBodyType
    r"""The type of tool"""

    vector_store_ids: Optional[List[str]] = None
    r"""The vector stores to search"""

    max_num_results: Optional[int] = 20
    r"""Maximum number of results to return"""

    filters: Optional[Any] = None
    r"""Filters to apply to the search"""

    ranking_options: Optional[RankingOptions] = None
    r"""Options for ranking search results"""


CreateResponseToolsProxyType = Literal["web_search_preview"]
r"""The type of tool"""

SearchContextSize = Literal["small", "medium", "large"]
r"""Amount of context to retrieve for each search result"""

CreateResponseToolsProxyRequestType = Literal["approximate", "exact"]
r"""The type of location"""


class UserLocationTypedDict(TypedDict):
    r"""User location for search localization"""

    type: NotRequired[CreateResponseToolsProxyRequestType]
    r"""The type of location"""
    city: NotRequired[Nullable[str]]
    r"""The city name"""
    country: NotRequired[str]
    r"""The country code"""
    region: NotRequired[Nullable[str]]
    r"""The region/state"""
    timezone: NotRequired[Nullable[str]]
    r"""The timezone"""


class UserLocation(BaseModel):
    r"""User location for search localization"""

    type: Optional[CreateResponseToolsProxyRequestType] = None
    r"""The type of location"""

    city: OptionalNullable[str] = UNSET
    r"""The city name"""

    country: Optional[str] = None
    r"""The country code"""

    region: OptionalNullable[str] = UNSET
    r"""The region/state"""

    timezone: OptionalNullable[str] = UNSET
    r"""The timezone"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "city", "country", "region", "timezone"]
        nullable_fields = ["city", "region", "timezone"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class Tools2TypedDict(TypedDict):
    r"""Configuration for web search tool"""

    type: CreateResponseToolsProxyType
    r"""The type of tool"""
    domains: NotRequired[List[str]]
    r"""List of domains to restrict search to"""
    search_context_size: NotRequired[SearchContextSize]
    r"""Amount of context to retrieve for each search result"""
    user_location: NotRequired[UserLocationTypedDict]
    r"""User location for search localization"""


class Tools2(BaseModel):
    r"""Configuration for web search tool"""

    type: CreateResponseToolsProxyType
    r"""The type of tool"""

    domains: Optional[List[str]] = None
    r"""List of domains to restrict search to"""

    search_context_size: Optional[SearchContextSize] = "medium"
    r"""Amount of context to retrieve for each search result"""

    user_location: Optional[UserLocation] = None
    r"""User location for search localization"""


ToolsType = Literal["function"]
r"""The type of tool"""

CreateResponseToolsType = Literal["object"]
r"""The type of the parameters object"""


class PropertiesTypedDict(TypedDict):
    type: str
    description: NotRequired[str]
    enum: NotRequired[List[str]]


class Properties(BaseModel):
    type: str

    description: Optional[str] = None

    enum: Optional[List[str]] = None


class ToolsParametersTypedDict(TypedDict):
    r"""The parameters the function accepts"""

    type: CreateResponseToolsType
    r"""The type of the parameters object"""
    properties: Dict[str, PropertiesTypedDict]
    r"""The parameters the function accepts, described as a JSON Schema object"""
    required: NotRequired[List[str]]
    r"""List of required parameter names"""
    additional_properties: NotRequired[bool]
    r"""Whether to allow properties not defined in the schema"""


class ToolsParameters(BaseModel):
    r"""The parameters the function accepts"""

    type: CreateResponseToolsType
    r"""The type of the parameters object"""

    properties: Dict[str, Properties]
    r"""The parameters the function accepts, described as a JSON Schema object"""

    required: Optional[List[str]] = None
    r"""List of required parameter names"""

    additional_properties: Annotated[
        Optional[bool], pydantic.Field(alias="additionalProperties")
    ] = None
    r"""Whether to allow properties not defined in the schema"""


class Tools1TypedDict(TypedDict):
    r"""A function tool definition"""

    type: ToolsType
    r"""The type of tool"""
    name: str
    r"""The name of the function to be called"""
    parameters: ToolsParametersTypedDict
    r"""The parameters the function accepts"""
    description: NotRequired[Nullable[str]]
    r"""A description of what the function does"""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating function calls"""


class Tools1(BaseModel):
    r"""A function tool definition"""

    type: ToolsType
    r"""The type of tool"""

    name: str
    r"""The name of the function to be called"""

    parameters: ToolsParameters
    r"""The parameters the function accepts"""

    description: OptionalNullable[str] = UNSET
    r"""A description of what the function does"""

    strict: Optional[bool] = True
    r"""Whether to enable strict schema adherence when generating function calls"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "strict"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateResponseToolsTypedDict = TypeAliasType(
    "CreateResponseToolsTypedDict",
    Union[Tools2TypedDict, Tools1TypedDict, Tools3TypedDict],
)


CreateResponseTools = TypeAliasType(
    "CreateResponseTools", Union[Tools2, Tools1, Tools3]
)


CreateResponseToolChoiceProxyRequestType = Literal["mcp"]


class ToolChoice4TypedDict(TypedDict):
    type: CreateResponseToolChoiceProxyRequestType
    server_label: str
    name: NotRequired[Nullable[str]]


class ToolChoice4(BaseModel):
    type: CreateResponseToolChoiceProxyRequestType

    server_label: str

    name: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["name"]
        nullable_fields = ["name"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateResponseToolChoiceProxyType = Literal["function"]


class ToolChoice3TypedDict(TypedDict):
    type: CreateResponseToolChoiceProxyType
    name: str


class ToolChoice3(BaseModel):
    type: CreateResponseToolChoiceProxyType

    name: str


CreateResponseToolChoiceType = Literal[
    "file_search",
    "web_search_preview",
    "computer_use_preview",
    "code_interpreter",
    "image_generation",
]


class CreateResponseToolChoice2TypedDict(TypedDict):
    type: CreateResponseToolChoiceType


class CreateResponseToolChoice2(BaseModel):
    type: CreateResponseToolChoiceType


CreateResponseToolChoice1 = Literal["none", "auto", "required"]
r"""Controls which (if any) tool is called by the model. `none` means the model will not call any tool. `auto` means the model can pick between generating a message or calling a tool. `required` means the model must call a tool."""

CreateResponseToolChoiceTypedDict = TypeAliasType(
    "CreateResponseToolChoiceTypedDict",
    Union[
        CreateResponseToolChoice2TypedDict,
        ToolChoice3TypedDict,
        ToolChoice4TypedDict,
        CreateResponseToolChoice1,
    ],
)
r"""How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool."""


CreateResponseToolChoice = TypeAliasType(
    "CreateResponseToolChoice",
    Union[
        CreateResponseToolChoice2, ToolChoice3, ToolChoice4, CreateResponseToolChoice1
    ],
)
r"""How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool."""


class CreateResponseRequestBodyTypedDict(TypedDict):
    model: str
    r"""ID of the model to use. You can use the List models API to see all of your available models."""
    input: CreateResponseInputTypedDict
    r"""The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input."""
    metadata: NotRequired[Dict[str, str]]
    r"""Developer-defined key-value pairs that will be included in response objects"""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."""
    previous_response_id: NotRequired[Nullable[str]]
    r"""The ID of a previous response to continue the conversation from. The model will have access to the previous response context."""
    instructions: NotRequired[Nullable[str]]
    r"""Developer-provided instructions that the model should follow. Overwrites the default system message."""
    reasoning: NotRequired[Nullable[ReasoningTypedDict]]
    r"""Configuration for reasoning models"""
    max_output_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens that can be generated in the response"""
    text: NotRequired[Nullable[TextTypedDict]]
    include: NotRequired[Nullable[List[Include]]]
    r"""Specifies which (potentially large) fields to include in the response. By default, the results of Code Interpreter and file searches are excluded. Available options:
    - code_interpreter_call.outputs: Include the outputs of Code Interpreter tool calls
    - computer_call_output.output.image_url: Include the image URLs from computer use tool calls
    - file_search_call.results: Include the results of file search tool calls
    - message.input_image.image_url: Include URLs of input images
    - message.output_text.logprobs: Include log probabilities for output text (when logprobs is enabled)
    - reasoning.encrypted_content: Include encrypted reasoning content for reasoning models
    """
    parallel_tool_calls: NotRequired[Nullable[bool]]
    r"""Whether to enable parallel function calling during tool use."""
    store: NotRequired[Nullable[bool]]
    r"""Whether to store this response for use in distillations or evals."""
    tools: NotRequired[List[CreateResponseToolsTypedDict]]
    r"""A list of tools the model may call. Use this to provide a list of functions the model may generate JSON inputs for."""
    tool_choice: NotRequired[CreateResponseToolChoiceTypedDict]
    r"""How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool."""
    stream: NotRequired[bool]


class CreateResponseRequestBody(BaseModel):
    model: str
    r"""ID of the model to use. You can use the List models API to see all of your available models."""

    input: CreateResponseInput
    r"""The actual user input(s) for the model. Can be a simple string, or an array of structured input items (messages, tool outputs) representing a conversation history or complex input."""

    metadata: Optional[Dict[str, str]] = None
    r"""Developer-defined key-value pairs that will be included in response objects"""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."""

    previous_response_id: OptionalNullable[str] = UNSET
    r"""The ID of a previous response to continue the conversation from. The model will have access to the previous response context."""

    instructions: OptionalNullable[str] = UNSET
    r"""Developer-provided instructions that the model should follow. Overwrites the default system message."""

    reasoning: OptionalNullable[Reasoning] = UNSET
    r"""Configuration for reasoning models"""

    max_output_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens that can be generated in the response"""

    text: OptionalNullable[Text] = UNSET

    include: OptionalNullable[List[Include]] = UNSET
    r"""Specifies which (potentially large) fields to include in the response. By default, the results of Code Interpreter and file searches are excluded. Available options:
    - code_interpreter_call.outputs: Include the outputs of Code Interpreter tool calls
    - computer_call_output.output.image_url: Include the image URLs from computer use tool calls
    - file_search_call.results: Include the results of file search tool calls
    - message.input_image.image_url: Include URLs of input images
    - message.output_text.logprobs: Include log probabilities for output text (when logprobs is enabled)
    - reasoning.encrypted_content: Include encrypted reasoning content for reasoning models
    """

    parallel_tool_calls: OptionalNullable[bool] = UNSET
    r"""Whether to enable parallel function calling during tool use."""

    store: OptionalNullable[bool] = True
    r"""Whether to store this response for use in distillations or evals."""

    tools: Optional[List[CreateResponseTools]] = None
    r"""A list of tools the model may call. Use this to provide a list of functions the model may generate JSON inputs for."""

    tool_choice: Optional[CreateResponseToolChoice] = None
    r"""How the model should select which tool (or tools) to use when generating a response. Can be a string (`none`, `auto`, `required`) or an object to force a specific tool."""

    stream: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "metadata",
            "temperature",
            "top_p",
            "previous_response_id",
            "instructions",
            "reasoning",
            "max_output_tokens",
            "text",
            "include",
            "parallel_tool_calls",
            "store",
            "tools",
            "tool_choice",
            "stream",
        ]
        nullable_fields = [
            "temperature",
            "top_p",
            "previous_response_id",
            "instructions",
            "reasoning",
            "max_output_tokens",
            "text",
            "include",
            "parallel_tool_calls",
            "store",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateResponseDataTypedDict(TypedDict):
    r"""One server-sent event emitted while the response streams"""

    type: str
    r"""The type of streaming event"""


class CreateResponseData(BaseModel):
    r"""One server-sent event emitted while the response streams"""

    type: str
    r"""The type of streaming event"""


class CreateResponseProxyResponseBodyTypedDict(TypedDict):
    r"""One server-sent event emitted while the response streams"""

    data: NotRequired[CreateResponseDataTypedDict]
    r"""One server-sent event emitted while the response streams"""


class CreateResponseProxyResponseBody(BaseModel):
    r"""One server-sent event emitted while the response streams"""

    data: Optional[CreateResponseData] = None
    r"""One server-sent event emitted while the response streams"""


CreateResponseObject = Literal["response"]
r"""The object type, which is always \"response\" """

CreateResponseStatus = Literal["completed", "failed", "in_progress", "incomplete"]
r"""The status of the response"""


class ErrorTypedDict(TypedDict):
    r"""The error that occurred, if any"""

    code: str
    r"""The error code"""
    message: str
    r"""The error message"""


class Error(BaseModel):
    r"""The error that occurred, if any"""

    code: str
    r"""The error code"""

    message: str
    r"""The error message"""


Reason = Literal["max_output_tokens", "content_filter"]
r"""The reason the response is incomplete"""


class IncompleteDetailsTypedDict(TypedDict):
    r"""Details about why the response is incomplete"""

    reason: Reason
    r"""The reason the response is incomplete"""


class IncompleteDetails(BaseModel):
    r"""Details about why the response is incomplete"""

    reason: Reason
    r"""The reason the response is incomplete"""


CreateResponseOutputProxyResponseType = Literal["function_call"]
r"""The type of output item"""

CreateResponseOutputProxyResponseStatus = Literal[
    "in_progress", "completed", "incomplete", "failed"
]
r"""The status of the function call"""


class Output4TypedDict(TypedDict):
    r"""A function tool call output"""

    id: str
    r"""The unique identifier for this output item"""
    type: CreateResponseOutputProxyResponseType
    r"""The type of output item"""
    call_id: str
    r"""The ID of the function call"""
    name: str
    r"""The name of the function being called"""
    arguments: str
    r"""The arguments to the function as a JSON string"""
    status: CreateResponseOutputProxyResponseStatus
    r"""The status of the function call"""


class Output4(BaseModel):
    r"""A function tool call output"""

    id: str
    r"""The unique identifier for this output item"""

    type: CreateResponseOutputProxyResponseType
    r"""The type of output item"""

    call_id: str
    r"""The ID of the function call"""

    name: str
    r"""The name of the function being called"""

    arguments: str
    r"""The arguments to the function as a JSON string"""

    status: CreateResponseOutputProxyResponseStatus
    r"""The status of the function call"""


CreateResponseOutputProxyType = Literal["file_search_call"]
r"""The type of output item"""

CreateResponseOutputProxyStatus = Literal[
    "in_progress", "completed", "incomplete", "failed"
]
r"""The status of the file search"""


class Output3TypedDict(TypedDict):
    r"""A file search tool call output"""

    id: str
    r"""The unique identifier for this output item"""
    type: CreateResponseOutputProxyType
    r"""The type of output item"""
    status: CreateResponseOutputProxyStatus
    r"""The status of the file search"""
    queries: NotRequired[List[str]]
    r"""The search queries used"""
    results: NotRequired[Any]
    r"""The file search results"""


class Output3(BaseModel):
    r"""A file search tool call output"""

    id: str
    r"""The unique identifier for this output item"""

    type: CreateResponseOutputProxyType
    r"""The type of output item"""

    status: CreateResponseOutputProxyStatus
    r"""The status of the file search"""

    queries: Optional[List[str]] = None
    r"""The search queries used"""

    results: Optional[Any] = None
    r"""The file search results"""


CreateResponseOutputType = Literal["web_search_call"]
r"""The type of output item"""

CreateResponseOutputStatus = Literal["in_progress", "completed", "incomplete", "failed"]
r"""The status of the web search"""


class Output2TypedDict(TypedDict):
    r"""A web search tool call output"""

    id: str
    r"""The unique identifier for this output item"""
    type: CreateResponseOutputType
    r"""The type of output item"""
    status: CreateResponseOutputStatus
    r"""The status of the web search"""


class Output2(BaseModel):
    r"""A web search tool call output"""

    id: str
    r"""The unique identifier for this output item"""

    type: CreateResponseOutputType
    r"""The type of output item"""

    status: CreateResponseOutputStatus
    r"""The status of the web search"""


OutputType = Literal["message"]
r"""The type of output item"""

OutputRole = Literal["assistant"]
r"""The role of the message author"""

OutputStatus = Literal["in_progress", "completed", "incomplete", "failed"]
r"""The status of the message"""

ContentType = Literal["output_text"]
r"""The type of content part"""

CreateResponseAnnotationsProxyType = Literal["file_citation"]


class CreateResponseAnnotations2TypedDict(TypedDict):
    r"""A citation to a file"""

    type: CreateResponseAnnotationsProxyType
    index: float
    r"""The index in the text where the citation appears"""
    file_id: str
    r"""The ID of the file being cited"""
    filename: str
    r"""The name of the file being cited"""


class CreateResponseAnnotations2(BaseModel):
    r"""A citation to a file"""

    type: CreateResponseAnnotationsProxyType

    index: float
    r"""The index in the text where the citation appears"""

    file_id: str
    r"""The ID of the file being cited"""

    filename: str
    r"""The name of the file being cited"""


CreateResponseAnnotationsType = Literal["url_citation"]


class CreateResponseAnnotations1TypedDict(TypedDict):
    r"""A citation to a URL"""

    type: CreateResponseAnnotationsType
    start_index: float
    r"""The start index of the citation in the text"""
    end_index: float
    r"""The end index of the citation in the text"""
    url: str
    r"""The URL being cited"""
    title: str
    r"""The title of the cited resource"""


class CreateResponseAnnotations1(BaseModel):
    r"""A citation to a URL"""

    type: CreateResponseAnnotationsType

    start_index: float
    r"""The start index of the citation in the text"""

    end_index: float
    r"""The end index of the citation in the text"""

    url: str
    r"""The URL being cited"""

    title: str
    r"""The title of the cited resource"""


ContentAnnotationsTypedDict = TypeAliasType(
    "ContentAnnotationsTypedDict",
    Union[CreateResponseAnnotations2TypedDict, CreateResponseAnnotations1TypedDict],
)
r"""An annotation in the output text"""


ContentAnnotations = TypeAliasType(
    "ContentAnnotations", Union[CreateResponseAnnotations2, CreateResponseAnnotations1]
)
r"""An annotation in the output text"""


class Content1TypedDict(TypedDict):
    r"""Text output from the model"""

    type: ContentType
    r"""The type of content part"""
    text: str
    r"""The text content"""
    annotations: NotRequired[List[ContentAnnotationsTypedDict]]
    r"""Annotations in the text such as citations"""
    logprobs: NotRequired[List[Any]]
    r"""Log probabilities of the output tokens if requested"""


class Content1(BaseModel):
    r"""Text output from the model"""

    type: ContentType
    r"""The type of content part"""

    text: str
    r"""The text content"""

    annotations: Optional[List[ContentAnnotations]] = None
    r"""Annotations in the text such as citations"""

    logprobs: Optional[List[Any]] = None
    r"""Log probabilities of the output tokens if requested"""


OutputContentTypedDict = Content1TypedDict


OutputContent = Content1


class Output1TypedDict(TypedDict):
    r"""An assistant message output"""

    id: str
    r"""The unique identifier for this message"""
    type: OutputType
    r"""The type of output item"""
    role: OutputRole
    r"""The role of the message author"""
    status: OutputStatus
    r"""The status of the message"""
    content: NotRequired[List[OutputContentTypedDict]]
    r"""The content parts of the message"""


class Output1(BaseModel):
    r"""An assistant message output"""

    id: str
    r"""The unique identifier for this message"""

    type: OutputType
    r"""The type of output item"""

    role: OutputRole
    r"""The role of the message author"""

    status: OutputStatus
    r"""The status of the message"""

    content: Optional[List[OutputContent]] = None
    r"""The content parts of the message"""


OutputTypedDict = TypeAliasType(
    "OutputTypedDict",
    Union[Output2TypedDict, Output1TypedDict, Output3TypedDict, Output4TypedDict],
)


Output = TypeAliasType("Output", Union[Output2, Output1, Output3, Output4])


class CreateResponseInputTokensDetailsTypedDict(TypedDict):
    r"""Breakdown of input token usage"""

    cached_tokens: NotRequired[int]
    r"""Number of tokens from cache"""


class CreateResponseInputTokensDetails(BaseModel):
    r"""Breakdown of input token usage"""

    cached_tokens: Optional[int] = None
    r"""Number of tokens from cache"""


class OutputTokensDetailsTypedDict(TypedDict):
    r"""Breakdown of output token usage"""

    reasoning_tokens: NotRequired[int]
    r"""Number of tokens used for reasoning (o3 models)"""
    accepted_prediction_tokens: NotRequired[int]
    r"""Number of tokens generated by automatic prediction that were accepted"""
    rejected_prediction_tokens: NotRequired[int]
    r"""Number of tokens generated by automatic prediction that were rejected"""


class OutputTokensDetails(BaseModel):
    r"""Breakdown of output token usage"""

    reasoning_tokens: Optional[int] = None
    r"""Number of tokens used for reasoning (o3 models)"""

    accepted_prediction_tokens: Optional[int] = None
    r"""Number of tokens generated by automatic prediction that were accepted"""

    rejected_prediction_tokens: Optional[int] = None
    r"""Number of tokens generated by automatic prediction that were rejected"""


class CreateResponseUsageTypedDict(TypedDict):
    r"""Usage statistics for the response"""

    input_tokens: NotRequired[float]
    r"""Number of tokens in the input"""
    output_tokens: NotRequired[float]
    r"""Number of tokens in the generated output"""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (input + output)"""
    input_tokens_details: NotRequired[CreateResponseInputTokensDetailsTypedDict]
    r"""Breakdown of input token usage"""
    output_tokens_details: NotRequired[OutputTokensDetailsTypedDict]
    r"""Breakdown of output token usage"""


class CreateResponseUsage(BaseModel):
    r"""Usage statistics for the response"""

    input_tokens: Optional[float] = None
    r"""Number of tokens in the input"""

    output_tokens: Optional[float] = None
    r"""Number of tokens in the generated output"""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (input + output)"""

    input_tokens_details: Optional[CreateResponseInputTokensDetails] = None
    r"""Breakdown of input token usage"""

    output_tokens_details: Optional[OutputTokensDetails] = None
    r"""Breakdown of output token usage"""


CreateResponseToolChoiceProxyResponseType = Literal["function"]


class CreateResponseToolChoiceFunctionTypedDict(TypedDict):
    name: str


class CreateResponseToolChoiceFunction(BaseModel):
    name: str


class CreateResponseToolChoiceProxy2TypedDict(TypedDict):
    type: CreateResponseToolChoiceProxyResponseType
    function: CreateResponseToolChoiceFunctionTypedDict


class CreateResponseToolChoiceProxy2(BaseModel):
    type: CreateResponseToolChoiceProxyResponseType

    function: CreateResponseToolChoiceFunction


CreateResponseToolChoiceProxy1 = Literal["none", "auto", "required"]

CreateResponseProxyToolChoiceTypedDict = TypeAliasType(
    "CreateResponseProxyToolChoiceTypedDict",
    Union[CreateResponseToolChoiceProxy2TypedDict, CreateResponseToolChoiceProxy1],
)
r"""Controls which (if any) tool is called by the model"""


CreateResponseProxyToolChoice = TypeAliasType(
    "CreateResponseProxyToolChoice",
    Union[CreateResponseToolChoiceProxy2, CreateResponseToolChoiceProxy1],
)
r"""Controls which (if any) tool is called by the model"""


CreateResponseToolsProxyResponse200ApplicationJSONType = Literal["file_search"]
r"""The type of tool"""

ToolsRanker = Literal["auto", "default_2024_08_21"]
r"""The ranking algorithm"""


class ToolsRankingOptionsTypedDict(TypedDict):
    r"""Options for ranking search results"""

    ranker: NotRequired[ToolsRanker]
    r"""The ranking algorithm"""
    score_threshold: NotRequired[float]
    r"""Minimum relevance score"""


class ToolsRankingOptions(BaseModel):
    r"""Options for ranking search results"""

    ranker: Optional[ToolsRanker] = "auto"
    r"""The ranking algorithm"""

    score_threshold: Optional[float] = 0
    r"""Minimum relevance score"""


class CreateResponseTools3TypedDict(TypedDict):
    r"""Configuration for file search tool"""

    type: CreateResponseToolsProxyResponse200ApplicationJSONType
    r"""The type of tool"""
    vector_store_ids: NotRequired[List[str]]
    r"""The vector stores to search"""
    max_num_results: NotRequired[int]
    r"""Maximum number of results to return"""
    filters: NotRequired[Any]
    r"""Filters to apply to the search"""
    ranking_options: NotRequired[ToolsRankingOptionsTypedDict]
    r"""Options for ranking search results"""


class CreateResponseTools3(BaseModel):
    r"""Configuration for file search tool"""

    type: CreateResponseToolsProxyResponse200ApplicationJSONType
    r"""The type of tool"""

    vector_store_ids: Optional[List[str]] = None
    r"""The vector stores to search"""

    max_num_results: Optional[int] = 20
    r"""Maximum number of results to return"""

    filters: Optional[Any] = None
    r"""Filters to apply to the search"""

    ranking_options: Optional[ToolsRankingOptions] = None
    r"""Options for ranking search results"""


CreateResponseToolsProxyResponse200Type = Literal["web_search_preview"]
r"""The type of tool"""

ToolsSearchContextSize = Literal["small", "medium", "large"]
r"""Amount of context to retrieve for each search result"""

CreateResponseToolsProxyResponse200ApplicationJSONResponseBodyType = Literal[
    "approximate", "exact"
]
r"""The type of location"""


class ToolsUserLocationTypedDict(TypedDict):
    r"""User location for search localization"""

    type: NotRequired[
        CreateResponseToolsProxyResponse200ApplicationJSONResponseBodyType
    ]
    r"""The type of location"""
    city: NotRequired[Nullable[str]]
    r"""The city name"""
    country: NotRequired[str]
    r"""The country code"""
    region: NotRequired[Nullable[str]]
    r"""The region/state"""
    timezone: NotRequired[Nullable[str]]
    r"""The timezone"""


class ToolsUserLocation(BaseModel):
    r"""User location for search localization"""

    type: Optional[
        CreateResponseToolsProxyResponse200ApplicationJSONResponseBodyType
    ] = None
    r"""The type of location"""

    city: OptionalNullable[str] = UNSET
    r"""The city name"""

    country: Optional[str] = None
    r"""The country code"""

    region: OptionalNullable[str] = UNSET
    r"""The region/state"""

    timezone: OptionalNullable[str] = UNSET
    r"""The timezone"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "city", "country", "region", "timezone"]
        nullable_fields = ["city", "region", "timezone"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class CreateResponseTools2TypedDict(TypedDict):
    r"""Configuration for web search tool"""

    type: CreateResponseToolsProxyResponse200Type
    r"""The type of tool"""
    domains: NotRequired[List[str]]
    r"""List of domains to restrict search to"""
    search_context_size: NotRequired[ToolsSearchContextSize]
    r"""Amount of context to retrieve for each search result"""
    user_location: NotRequired[ToolsUserLocationTypedDict]
    r"""User location for search localization"""


class CreateResponseTools2(BaseModel):
    r"""Configuration for web search tool"""

    type: CreateResponseToolsProxyResponse200Type
    r"""The type of tool"""

    domains: Optional[List[str]] = None
    r"""List of domains to restrict search to"""

    search_context_size: Optional[ToolsSearchContextSize] = "medium"
    r"""Amount of context to retrieve for each search result"""

    user_location: Optional[ToolsUserLocation] = None
    r"""User location for search localization"""


CreateResponseToolsProxyResponseType = Literal["function"]
r"""The type of tool"""

CreateResponseToolsProxyResponse200ApplicationJSONResponseBody1Type = Literal["object"]
r"""The type of the parameters object"""


class ToolsPropertiesTypedDict(TypedDict):
    type: str
    description: NotRequired[str]
    enum: NotRequired[List[str]]


class ToolsProperties(BaseModel):
    type: str

    description: Optional[str] = None

    enum: Optional[List[str]] = None


class CreateResponseToolsParametersTypedDict(TypedDict):
    r"""The parameters the function accepts"""

    type: CreateResponseToolsProxyResponse200ApplicationJSONResponseBody1Type
    r"""The type of the parameters object"""
    properties: Dict[str, ToolsPropertiesTypedDict]
    r"""The parameters the function accepts, described as a JSON Schema object"""
    required: NotRequired[List[str]]
    r"""List of required parameter names"""
    additional_properties: NotRequired[bool]
    r"""Whether to allow properties not defined in the schema"""


class CreateResponseToolsParameters(BaseModel):
    r"""The parameters the function accepts"""

    type: CreateResponseToolsProxyResponse200ApplicationJSONResponseBody1Type
    r"""The type of the parameters object"""

    properties: Dict[str, ToolsProperties]
    r"""The parameters the function accepts, described as a JSON Schema object"""

    required: Optional[List[str]] = None
    r"""List of required parameter names"""

    additional_properties: Annotated[
        Optional[bool], pydantic.Field(alias="additionalProperties")
    ] = None
    r"""Whether to allow properties not defined in the schema"""


class CreateResponseTools1TypedDict(TypedDict):
    r"""A function tool definition"""

    type: CreateResponseToolsProxyResponseType
    r"""The type of tool"""
    name: str
    r"""The name of the function to be called"""
    parameters: CreateResponseToolsParametersTypedDict
    r"""The parameters the function accepts"""
    description: NotRequired[Nullable[str]]
    r"""A description of what the function does"""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating function calls"""


class CreateResponseTools1(BaseModel):
    r"""A function tool definition"""

    type: CreateResponseToolsProxyResponseType
    r"""The type of tool"""

    name: str
    r"""The name of the function to be called"""

    parameters: CreateResponseToolsParameters
    r"""The parameters the function accepts"""

    description: OptionalNullable[str] = UNSET
    r"""A description of what the function does"""

    strict: Optional[bool] = True
    r"""Whether to enable strict schema adherence when generating function calls"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "strict"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateResponseProxyToolsTypedDict = TypeAliasType(
    "CreateResponseProxyToolsTypedDict",
    Union[
        CreateResponseTools2TypedDict,
        CreateResponseTools1TypedDict,
        CreateResponseTools3TypedDict,
    ],
)


CreateResponseProxyTools = TypeAliasType(
    "CreateResponseProxyTools",
    Union[CreateResponseTools2, CreateResponseTools1, CreateResponseTools3],
)


class CreateResponseReasoningTypedDict(TypedDict):
    effort: NotRequired[Nullable[str]]
    summary: NotRequired[Nullable[str]]


class CreateResponseReasoning(BaseModel):
    effort: OptionalNullable[str] = UNSET

    summary: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["effort", "summary"]
        nullable_fields = ["effort", "summary"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateResponseFormatProxyResponse200ApplicationJSONType = Literal["json_schema"]
r"""Ensures the response matches a supplied JSON schema"""


class CreateResponseFormat3TypedDict(TypedDict):
    type: CreateResponseFormatProxyResponse200ApplicationJSONType
    r"""Ensures the response matches a supplied JSON schema"""
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    schema_: Dict[str, Any]
    r"""The JSON schema to validate the response against"""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    strict: NotRequired[Nullable[bool]]
    r"""Whether to enable strict `schema` adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when `strict` is `true`"""


class CreateResponseFormat3(BaseModel):
    type: CreateResponseFormatProxyResponse200ApplicationJSONType
    r"""Ensures the response matches a supplied JSON schema"""

    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]
    r"""The JSON schema to validate the response against"""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    strict: OptionalNullable[bool] = True
    r"""Whether to enable strict `schema` adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when `strict` is `true`"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "strict"]
        nullable_fields = ["strict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateResponseFormatProxyResponse200Type = Literal["json_object"]
r"""Ensures the response is a valid JSON object"""


class CreateResponseFormat2TypedDict(TypedDict):
    type: CreateResponseFormatProxyResponse200Type
    r"""Ensures the response is a valid JSON object"""


class CreateResponseFormat2(BaseModel):
    type: CreateResponseFormatProxyResponse200Type
    r"""Ensures the response is a valid JSON object"""


CreateResponseFormatProxyResponseType = Literal["text"]
r"""Plain text response format"""


class CreateResponseFormat1TypedDict(TypedDict):
    type: CreateResponseFormatProxyResponseType
    r"""Plain text response format"""


class CreateResponseFormat1(BaseModel):
    type: CreateResponseFormatProxyResponseType
    r"""Plain text response format"""


CreateResponseProxyFormatTypedDict = TypeAliasType(
    "CreateResponseProxyFormatTypedDict",
    Union[
        CreateResponseFormat1TypedDict,
        CreateResponseFormat2TypedDict,
        CreateResponseFormat3TypedDict,
    ],
)


CreateResponseProxyFormat = TypeAliasType(
    "CreateResponseProxyFormat",
    Union[CreateResponseFormat1, CreateResponseFormat2, CreateResponseFormat3],
)


class CreateResponseTextTypedDict(TypedDict):
    format_: CreateResponseProxyFormatTypedDict


class CreateResponseText(BaseModel):
    format_: Annotated[CreateResponseProxyFormat, pydantic.Field(alias="format")]


Truncation = Literal["auto", "disabled"]
r"""Controls how the model handles inputs longer than the maximum token length"""

ServiceTier = Literal["auto", "default"]
r"""The service tier used for processing the request"""


class CreateResponseResponseBodyTypedDict(TypedDict):
    r"""Represents the completed model response returned when `stream` is false"""

    id: str
    r"""The unique identifier for the response"""
    object: CreateResponseObject
    r"""The object type, which is always \"response\" """
    created_at: float
    r"""The Unix timestamp (in seconds) of when the response was created"""
    status: CreateResponseStatus
    r"""The status of the response"""
    error: Nullable[ErrorTypedDict]
    r"""The error that occurred, if any"""
    incomplete_details: Nullable[IncompleteDetailsTypedDict]
    r"""Details about why the response is incomplete"""
    model: str
    r"""The model used to generate the response"""
    output: List[OutputTypedDict]
    r"""The list of output items generated by the model"""
    parallel_tool_calls: bool
    instructions: NotRequired[Nullable[str]]
    r"""The instructions provided for the response"""
    output_text: NotRequired[Nullable[str]]
    r"""A convenience field with the concatenated text from all text content parts"""
    usage: NotRequired[CreateResponseUsageTypedDict]
    r"""Usage statistics for the response"""
    temperature: NotRequired[Nullable[float]]
    top_p: NotRequired[Nullable[float]]
    max_output_tokens: NotRequired[Nullable[int]]
    previous_response_id: NotRequired[Nullable[str]]
    metadata: NotRequired[Dict[str, str]]
    tool_choice: NotRequired[CreateResponseProxyToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model"""
    tools: NotRequired[List[CreateResponseProxyToolsTypedDict]]
    reasoning: NotRequired[Nullable[CreateResponseReasoningTypedDict]]
    store: NotRequired[bool]
    text: NotRequired[CreateResponseTextTypedDict]
    truncation: NotRequired[Nullable[Truncation]]
    r"""Controls how the model handles inputs longer than the maximum token length"""
    user: NotRequired[Nullable[str]]
    r"""A unique identifier representing your end-user"""
    service_tier: NotRequired[Nullable[ServiceTier]]
    r"""The service tier used for processing the request"""
    background: NotRequired[Nullable[bool]]
    r"""Whether the response was processed in the background"""
    top_logprobs: NotRequired[Nullable[int]]
    r"""The number of top log probabilities to return for each output token"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens"""


class CreateResponseResponseBody(BaseModel):
    r"""Represents the completed model response returned when `stream` is false"""

    id: str
    r"""The unique identifier for the response"""

    object: CreateResponseObject
    r"""The object type, which is always \"response\" """

    created_at: float
    r"""The Unix timestamp (in seconds) of when the response was created"""

    status: CreateResponseStatus
    r"""The status of the response"""

    error: Nullable[Error]
    r"""The error that occurred, if any"""

    incomplete_details: Nullable[IncompleteDetails]
    r"""Details about why the response is incomplete"""

    model: str
    r"""The model used to generate the response"""

    output: List[Output]
    r"""The list of output items generated by the model"""

    parallel_tool_calls: bool

    instructions: OptionalNullable[str] = UNSET
    r"""The instructions provided for the response"""

    output_text: OptionalNullable[str] = UNSET
    r"""A convenience field with the concatenated text from all text content parts"""

    usage: Optional[CreateResponseUsage] = None
    r"""Usage statistics for the response"""

    temperature: OptionalNullable[float] = UNSET

    top_p: OptionalNullable[float] = UNSET

    max_output_tokens: OptionalNullable[int] = UNSET

    previous_response_id: OptionalNullable[str] = UNSET

    metadata: Optional[Dict[str, str]] = None

    tool_choice: Optional[CreateResponseProxyToolChoice] = None
    r"""Controls which (if any) tool is called by the model"""

    tools: Optional[List[CreateResponseProxyTools]] = None

    reasoning: OptionalNullable[CreateResponseReasoning] = UNSET

    store: Optional[bool] = None

    text: Optional[CreateResponseText] = None

    truncation: OptionalNullable[Truncation] = "disabled"
    r"""Controls how the model handles inputs longer than the maximum token length"""

    user: OptionalNullable[str] = UNSET
    r"""A unique identifier representing your end-user"""

    service_tier: OptionalNullable[ServiceTier] = UNSET
    r"""The service tier used for processing the request"""

    background: OptionalNullable[bool] = UNSET
    r"""Whether the response was processed in the background"""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""The number of top log probabilities to return for each output token"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "instructions",
            "output_text",
            "usage",
            "temperature",
            "top_p",
            "max_output_tokens",
            "previous_response_id",
            "metadata",
            "tool_choice",
            "tools",
            "reasoning",
            "store",
            "text",
            "truncation",
            "user",
            "service_tier",
            "background",
            "top_logprobs",
            "logprobs",
        ]
        nullable_fields = [
            "error",
            "incomplete_details",
            "instructions",
            "output_text",
            "temperature",
            "top_p",
            "max_output_tokens",
            "previous_response_id",
            "reasoning",
            "truncation",
            "user",
            "service_tier",
            "background",
            "top_logprobs",
            "logprobs",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateResponseResponseTypedDict = TypeAliasType(
    "CreateResponseResponseTypedDict",
    Union[
        CreateResponseResponseBodyTypedDict,
        Union[
            eventstreaming.EventStream[CreateResponseProxyResponseBodyTypedDict],
            eventstreaming.EventStreamAsync[CreateResponseProxyResponseBodyTypedDict],
        ],
    ],
)


CreateResponseResponse = TypeAliasType(
    "CreateResponseResponse",
    Union[
        CreateResponseResponseBody,
        Union[
            eventstreaming.EventStream[CreateResponseProxyResponseBody],
            eventstreaming.EventStreamAsync[CreateResponseProxyResponseBody],
        ],
    ],
)
