"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .audiocontentpartschema import (
    AudioContentPartSchema,
    AudioContentPartSchemaTypedDict,
)
from .filecontentpartschema import FileContentPartSchema, FileContentPartSchemaTypedDict
from .imagecontentpartschema import (
    ImageContentPartSchema,
    ImageContentPartSchemaTypedDict,
)
from .publicidentity import PublicIdentity, PublicIdentityTypedDict
from .reasoningpartschema import ReasoningPartSchema, ReasoningPartSchemaTypedDict
from .redactedreasoningpartschema import (
    RedactedReasoningPartSchema,
    RedactedReasoningPartSchemaTypedDict,
)
from .refusalpartschema import RefusalPartSchema, RefusalPartSchemaTypedDict
from .textcontentpartschema import TextContentPartSchema, TextContentPartSchemaTypedDict
from datetime import datetime
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import FieldMetadata, HeaderMetadata, get_discriminator
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class DeploymentStreamGlobalsTypedDict(TypedDict):
    environment: NotRequired[str]
    contact_id: NotRequired[str]


class DeploymentStreamGlobals(BaseModel):
    environment: Annotated[
        Optional[str],
        FieldMetadata(header=HeaderMetadata(style="simple", explode=False)),
    ] = None

    contact_id: Annotated[
        Optional[str],
        pydantic.Field(alias="contactId"),
        FieldMetadata(header=HeaderMetadata(style="simple", explode=False)),
    ] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["environment", "contactId"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


DeploymentStreamPrefixMessagesDeploymentsRequestRequestBody5Role = Literal["tool",]
r"""The role of the messages author, in this case tool."""


DeploymentStreamContentDeploymentsRequest2TypedDict = TextContentPartSchemaTypedDict


DeploymentStreamContentDeploymentsRequest2 = TextContentPartSchema


DeploymentStreamPrefixMessagesDeploymentsRequestRequestBody5ContentTypedDict = (
    TypeAliasType(
        "DeploymentStreamPrefixMessagesDeploymentsRequestRequestBody5ContentTypedDict",
        Union[str, List[DeploymentStreamContentDeploymentsRequest2TypedDict]],
    )
)
r"""The contents of the tool message."""


DeploymentStreamPrefixMessagesDeploymentsRequestRequestBody5Content = TypeAliasType(
    "DeploymentStreamPrefixMessagesDeploymentsRequestRequestBody5Content",
    Union[str, List[DeploymentStreamContentDeploymentsRequest2]],
)
r"""The contents of the tool message."""


DeploymentStreamPrefixMessagesDeploymentsType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


DeploymentStreamPrefixMessagesTTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class DeploymentStreamPrefixMessagesCacheControlTypedDict(TypedDict):
    type: DeploymentStreamPrefixMessagesDeploymentsType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[DeploymentStreamPrefixMessagesTTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class DeploymentStreamPrefixMessagesCacheControl(BaseModel):
    type: DeploymentStreamPrefixMessagesDeploymentsType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[DeploymentStreamPrefixMessagesTTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamPrefixMessagesToolMessageTypedDict(TypedDict):
    role: DeploymentStreamPrefixMessagesDeploymentsRequestRequestBody5Role
    r"""The role of the messages author, in this case tool."""
    content: (
        DeploymentStreamPrefixMessagesDeploymentsRequestRequestBody5ContentTypedDict
    )
    r"""The contents of the tool message."""
    tool_call_id: Nullable[str]
    r"""Tool call that this message is responding to."""
    cache_control: NotRequired[DeploymentStreamPrefixMessagesCacheControlTypedDict]


class DeploymentStreamPrefixMessagesToolMessage(BaseModel):
    role: DeploymentStreamPrefixMessagesDeploymentsRequestRequestBody5Role
    r"""The role of the messages author, in this case tool."""

    content: DeploymentStreamPrefixMessagesDeploymentsRequestRequestBody5Content
    r"""The contents of the tool message."""

    tool_call_id: Nullable[str]
    r"""Tool call that this message is responding to."""

    cache_control: Optional[DeploymentStreamPrefixMessagesCacheControl] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["cache_control"])
        nullable_fields = set(["tool_call_id"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


DeploymentStreamContentDeployments2TypedDict = TypeAliasType(
    "DeploymentStreamContentDeployments2TypedDict",
    Union[
        RefusalPartSchemaTypedDict,
        RedactedReasoningPartSchemaTypedDict,
        TextContentPartSchemaTypedDict,
        ReasoningPartSchemaTypedDict,
    ],
)


DeploymentStreamContentDeployments2 = Annotated[
    Union[
        Annotated[TextContentPartSchema, Tag("text")],
        Annotated[RefusalPartSchema, Tag("refusal")],
        Annotated[ReasoningPartSchema, Tag("reasoning")],
        Annotated[RedactedReasoningPartSchema, Tag("redacted_reasoning")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


DeploymentStreamPrefixMessagesDeploymentsRequestRequestBodyContentTypedDict = (
    TypeAliasType(
        "DeploymentStreamPrefixMessagesDeploymentsRequestRequestBodyContentTypedDict",
        Union[str, List[DeploymentStreamContentDeployments2TypedDict]],
    )
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


DeploymentStreamPrefixMessagesDeploymentsRequestRequestBodyContent = TypeAliasType(
    "DeploymentStreamPrefixMessagesDeploymentsRequestRequestBodyContent",
    Union[str, List[DeploymentStreamContentDeployments2]],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


DeploymentStreamPrefixMessagesDeploymentsRequestRequestBodyRole = Literal["assistant",]
r"""The role of the messages author, in this case `assistant`."""


class DeploymentStreamPrefixMessagesAudioTypedDict(TypedDict):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


class DeploymentStreamPrefixMessagesAudio(BaseModel):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


DeploymentStreamPrefixMessagesType = Literal["function",]
r"""The type of the tool. Currently, only `function` is supported."""


class DeploymentStreamPrefixMessagesFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class DeploymentStreamPrefixMessagesFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name", "arguments"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamPrefixMessagesToolCallsTypedDict(TypedDict):
    id: str
    r"""The ID of the tool call."""
    type: DeploymentStreamPrefixMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""
    function: DeploymentStreamPrefixMessagesFunctionTypedDict
    thought_signature: NotRequired[str]
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models when continuing a conversation after a tool call."""


class DeploymentStreamPrefixMessagesToolCalls(BaseModel):
    id: str
    r"""The ID of the tool call."""

    type: DeploymentStreamPrefixMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""

    function: DeploymentStreamPrefixMessagesFunction

    thought_signature: Optional[str] = None
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models when continuing a conversation after a tool call."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["thought_signature"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamPrefixMessagesAssistantMessageTypedDict(TypedDict):
    role: DeploymentStreamPrefixMessagesDeploymentsRequestRequestBodyRole
    r"""The role of the messages author, in this case `assistant`."""
    content: NotRequired[
        Nullable[
            DeploymentStreamPrefixMessagesDeploymentsRequestRequestBodyContentTypedDict
        ]
    ]
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""
    refusal: NotRequired[Nullable[str]]
    r"""The refusal message by the assistant."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""
    audio: NotRequired[Nullable[DeploymentStreamPrefixMessagesAudioTypedDict]]
    r"""Data about a previous audio response from the model."""
    tool_calls: NotRequired[List[DeploymentStreamPrefixMessagesToolCallsTypedDict]]
    r"""The tool calls generated by the model, such as function calls."""


class DeploymentStreamPrefixMessagesAssistantMessage(BaseModel):
    role: DeploymentStreamPrefixMessagesDeploymentsRequestRequestBodyRole
    r"""The role of the messages author, in this case `assistant`."""

    content: OptionalNullable[
        DeploymentStreamPrefixMessagesDeploymentsRequestRequestBodyContent
    ] = UNSET
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""

    refusal: OptionalNullable[str] = UNSET
    r"""The refusal message by the assistant."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    audio: OptionalNullable[DeploymentStreamPrefixMessagesAudio] = UNSET
    r"""Data about a previous audio response from the model."""

    tool_calls: Optional[List[DeploymentStreamPrefixMessagesToolCalls]] = None
    r"""The tool calls generated by the model, such as function calls."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["content", "refusal", "name", "audio", "tool_calls"])
        nullable_fields = set(["content", "refusal", "audio"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


DeploymentStreamPrefixMessagesDeploymentsRequestRole = Literal["user",]
r"""The role of the messages author, in this case `user`."""


DeploymentStream2Type = Literal["file",]
r"""The type of the content part. Always `file`."""


DeploymentStream2DeploymentsType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


DeploymentStream2TTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class DeploymentStream2CacheControlTypedDict(TypedDict):
    type: DeploymentStream2DeploymentsType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[DeploymentStream2TTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class DeploymentStream2CacheControl(BaseModel):
    type: DeploymentStream2DeploymentsType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[DeploymentStream2TTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStream24TypedDict(TypedDict):
    type: DeploymentStream2Type
    r"""The type of the content part. Always `file`."""
    file: FileContentPartSchemaTypedDict
    r"""File data for the content part. Must contain either file_data or uri, but not both."""
    cache_control: NotRequired[DeploymentStream2CacheControlTypedDict]


class DeploymentStream24(BaseModel):
    type: DeploymentStream2Type
    r"""The type of the content part. Always `file`."""

    file: FileContentPartSchema
    r"""File data for the content part. Must contain either file_data or uri, but not both."""

    cache_control: Optional[DeploymentStream2CacheControl] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["cache_control"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


DeploymentStreamContent2TypedDict = TypeAliasType(
    "DeploymentStreamContent2TypedDict",
    Union[
        AudioContentPartSchemaTypedDict,
        TextContentPartSchemaTypedDict,
        ImageContentPartSchemaTypedDict,
        DeploymentStream24TypedDict,
    ],
)


DeploymentStreamContent2 = Annotated[
    Union[
        Annotated[TextContentPartSchema, Tag("text")],
        Annotated[ImageContentPartSchema, Tag("image_url")],
        Annotated[AudioContentPartSchema, Tag("input_audio")],
        Annotated[DeploymentStream24, Tag("file")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


DeploymentStreamPrefixMessagesDeploymentsRequestContentTypedDict = TypeAliasType(
    "DeploymentStreamPrefixMessagesDeploymentsRequestContentTypedDict",
    Union[str, List[DeploymentStreamContent2TypedDict]],
)
r"""The contents of the user message."""


DeploymentStreamPrefixMessagesDeploymentsRequestContent = TypeAliasType(
    "DeploymentStreamPrefixMessagesDeploymentsRequestContent",
    Union[str, List[DeploymentStreamContent2]],
)
r"""The contents of the user message."""


class DeploymentStreamPrefixMessagesUserMessageTypedDict(TypedDict):
    role: DeploymentStreamPrefixMessagesDeploymentsRequestRole
    r"""The role of the messages author, in this case `user`."""
    content: DeploymentStreamPrefixMessagesDeploymentsRequestContentTypedDict
    r"""The contents of the user message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class DeploymentStreamPrefixMessagesUserMessage(BaseModel):
    role: DeploymentStreamPrefixMessagesDeploymentsRequestRole
    r"""The role of the messages author, in this case `user`."""

    content: DeploymentStreamPrefixMessagesDeploymentsRequestContent
    r"""The contents of the user message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


DeploymentStreamPrefixMessagesDeploymentsRole = Literal["developer",]
r"""The role of the messages author, in this case  `developer`."""


DeploymentStreamPrefixMessagesDeploymentsContentTypedDict = TypeAliasType(
    "DeploymentStreamPrefixMessagesDeploymentsContentTypedDict",
    Union[str, List[TextContentPartSchemaTypedDict]],
)
r"""The contents of the developer message."""


DeploymentStreamPrefixMessagesDeploymentsContent = TypeAliasType(
    "DeploymentStreamPrefixMessagesDeploymentsContent",
    Union[str, List[TextContentPartSchema]],
)
r"""The contents of the developer message."""


class DeploymentStreamPrefixMessagesDeveloperMessageTypedDict(TypedDict):
    role: DeploymentStreamPrefixMessagesDeploymentsRole
    r"""The role of the messages author, in this case  `developer`."""
    content: DeploymentStreamPrefixMessagesDeploymentsContentTypedDict
    r"""The contents of the developer message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class DeploymentStreamPrefixMessagesDeveloperMessage(BaseModel):
    role: DeploymentStreamPrefixMessagesDeploymentsRole
    r"""The role of the messages author, in this case  `developer`."""

    content: DeploymentStreamPrefixMessagesDeploymentsContent
    r"""The contents of the developer message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


DeploymentStreamPrefixMessagesRole = Literal["system",]
r"""The role of the messages author, in this case `system`."""


DeploymentStreamPrefixMessagesContentTypedDict = TypeAliasType(
    "DeploymentStreamPrefixMessagesContentTypedDict",
    Union[str, List[TextContentPartSchemaTypedDict]],
)
r"""The contents of the system message."""


DeploymentStreamPrefixMessagesContent = TypeAliasType(
    "DeploymentStreamPrefixMessagesContent", Union[str, List[TextContentPartSchema]]
)
r"""The contents of the system message."""


class DeploymentStreamPrefixMessagesSystemMessageTypedDict(TypedDict):
    r"""Developer-provided instructions that the model should follow, regardless of messages sent by the user."""

    role: DeploymentStreamPrefixMessagesRole
    r"""The role of the messages author, in this case `system`."""
    content: DeploymentStreamPrefixMessagesContentTypedDict
    r"""The contents of the system message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class DeploymentStreamPrefixMessagesSystemMessage(BaseModel):
    r"""Developer-provided instructions that the model should follow, regardless of messages sent by the user."""

    role: DeploymentStreamPrefixMessagesRole
    r"""The role of the messages author, in this case `system`."""

    content: DeploymentStreamPrefixMessagesContent
    r"""The contents of the system message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


DeploymentStreamPrefixMessagesTypedDict = TypeAliasType(
    "DeploymentStreamPrefixMessagesTypedDict",
    Union[
        DeploymentStreamPrefixMessagesSystemMessageTypedDict,
        DeploymentStreamPrefixMessagesDeveloperMessageTypedDict,
        DeploymentStreamPrefixMessagesUserMessageTypedDict,
        DeploymentStreamPrefixMessagesToolMessageTypedDict,
        DeploymentStreamPrefixMessagesAssistantMessageTypedDict,
    ],
)


DeploymentStreamPrefixMessages = Annotated[
    Union[
        Annotated[DeploymentStreamPrefixMessagesSystemMessage, Tag("system")],
        Annotated[DeploymentStreamPrefixMessagesDeveloperMessage, Tag("developer")],
        Annotated[DeploymentStreamPrefixMessagesUserMessage, Tag("user")],
        Annotated[DeploymentStreamPrefixMessagesAssistantMessage, Tag("assistant")],
        Annotated[DeploymentStreamPrefixMessagesToolMessage, Tag("tool")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


DeploymentStreamMessagesDeploymentsRequestRequestBody5Role = Literal["tool",]
r"""The role of the messages author, in this case tool."""


DeploymentStreamContentDeploymentsRequestRequestBodyMessages52TypedDict = (
    TextContentPartSchemaTypedDict
)


DeploymentStreamContentDeploymentsRequestRequestBodyMessages52 = TextContentPartSchema


DeploymentStreamMessagesDeploymentsRequestRequestBody5ContentTypedDict = TypeAliasType(
    "DeploymentStreamMessagesDeploymentsRequestRequestBody5ContentTypedDict",
    Union[
        str,
        List[DeploymentStreamContentDeploymentsRequestRequestBodyMessages52TypedDict],
    ],
)
r"""The contents of the tool message."""


DeploymentStreamMessagesDeploymentsRequestRequestBody5Content = TypeAliasType(
    "DeploymentStreamMessagesDeploymentsRequestRequestBody5Content",
    Union[str, List[DeploymentStreamContentDeploymentsRequestRequestBodyMessages52]],
)
r"""The contents of the tool message."""


DeploymentStreamMessagesDeploymentsType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


DeploymentStreamMessagesTTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class DeploymentStreamMessagesCacheControlTypedDict(TypedDict):
    type: DeploymentStreamMessagesDeploymentsType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[DeploymentStreamMessagesTTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class DeploymentStreamMessagesCacheControl(BaseModel):
    type: DeploymentStreamMessagesDeploymentsType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[DeploymentStreamMessagesTTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamMessagesToolMessageTypedDict(TypedDict):
    role: DeploymentStreamMessagesDeploymentsRequestRequestBody5Role
    r"""The role of the messages author, in this case tool."""
    content: DeploymentStreamMessagesDeploymentsRequestRequestBody5ContentTypedDict
    r"""The contents of the tool message."""
    tool_call_id: Nullable[str]
    r"""Tool call that this message is responding to."""
    cache_control: NotRequired[DeploymentStreamMessagesCacheControlTypedDict]


class DeploymentStreamMessagesToolMessage(BaseModel):
    role: DeploymentStreamMessagesDeploymentsRequestRequestBody5Role
    r"""The role of the messages author, in this case tool."""

    content: DeploymentStreamMessagesDeploymentsRequestRequestBody5Content
    r"""The contents of the tool message."""

    tool_call_id: Nullable[str]
    r"""Tool call that this message is responding to."""

    cache_control: Optional[DeploymentStreamMessagesCacheControl] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["cache_control"])
        nullable_fields = set(["tool_call_id"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


DeploymentStreamContentDeploymentsRequestRequestBodyMessages2TypedDict = TypeAliasType(
    "DeploymentStreamContentDeploymentsRequestRequestBodyMessages2TypedDict",
    Union[
        RefusalPartSchemaTypedDict,
        RedactedReasoningPartSchemaTypedDict,
        TextContentPartSchemaTypedDict,
        ReasoningPartSchemaTypedDict,
    ],
)


DeploymentStreamContentDeploymentsRequestRequestBodyMessages2 = Annotated[
    Union[
        Annotated[TextContentPartSchema, Tag("text")],
        Annotated[RefusalPartSchema, Tag("refusal")],
        Annotated[ReasoningPartSchema, Tag("reasoning")],
        Annotated[RedactedReasoningPartSchema, Tag("redacted_reasoning")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


DeploymentStreamMessagesDeploymentsRequestRequestBodyContentTypedDict = TypeAliasType(
    "DeploymentStreamMessagesDeploymentsRequestRequestBodyContentTypedDict",
    Union[
        str,
        List[DeploymentStreamContentDeploymentsRequestRequestBodyMessages2TypedDict],
    ],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


DeploymentStreamMessagesDeploymentsRequestRequestBodyContent = TypeAliasType(
    "DeploymentStreamMessagesDeploymentsRequestRequestBodyContent",
    Union[str, List[DeploymentStreamContentDeploymentsRequestRequestBodyMessages2]],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


DeploymentStreamMessagesDeploymentsRequestRequestBodyRole = Literal["assistant",]
r"""The role of the messages author, in this case `assistant`."""


class DeploymentStreamMessagesAudioTypedDict(TypedDict):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


class DeploymentStreamMessagesAudio(BaseModel):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


DeploymentStreamMessagesType = Literal["function",]
r"""The type of the tool. Currently, only `function` is supported."""


class DeploymentStreamMessagesFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class DeploymentStreamMessagesFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name", "arguments"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamMessagesToolCallsTypedDict(TypedDict):
    id: str
    r"""The ID of the tool call."""
    type: DeploymentStreamMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""
    function: DeploymentStreamMessagesFunctionTypedDict
    thought_signature: NotRequired[str]
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models when continuing a conversation after a tool call."""


class DeploymentStreamMessagesToolCalls(BaseModel):
    id: str
    r"""The ID of the tool call."""

    type: DeploymentStreamMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""

    function: DeploymentStreamMessagesFunction

    thought_signature: Optional[str] = None
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models when continuing a conversation after a tool call."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["thought_signature"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamMessagesAssistantMessageTypedDict(TypedDict):
    role: DeploymentStreamMessagesDeploymentsRequestRequestBodyRole
    r"""The role of the messages author, in this case `assistant`."""
    content: NotRequired[
        Nullable[DeploymentStreamMessagesDeploymentsRequestRequestBodyContentTypedDict]
    ]
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""
    refusal: NotRequired[Nullable[str]]
    r"""The refusal message by the assistant."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""
    audio: NotRequired[Nullable[DeploymentStreamMessagesAudioTypedDict]]
    r"""Data about a previous audio response from the model."""
    tool_calls: NotRequired[List[DeploymentStreamMessagesToolCallsTypedDict]]
    r"""The tool calls generated by the model, such as function calls."""


class DeploymentStreamMessagesAssistantMessage(BaseModel):
    role: DeploymentStreamMessagesDeploymentsRequestRequestBodyRole
    r"""The role of the messages author, in this case `assistant`."""

    content: OptionalNullable[
        DeploymentStreamMessagesDeploymentsRequestRequestBodyContent
    ] = UNSET
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""

    refusal: OptionalNullable[str] = UNSET
    r"""The refusal message by the assistant."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    audio: OptionalNullable[DeploymentStreamMessagesAudio] = UNSET
    r"""Data about a previous audio response from the model."""

    tool_calls: Optional[List[DeploymentStreamMessagesToolCalls]] = None
    r"""The tool calls generated by the model, such as function calls."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["content", "refusal", "name", "audio", "tool_calls"])
        nullable_fields = set(["content", "refusal", "audio"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


DeploymentStreamMessagesDeploymentsRequestRole = Literal["user",]
r"""The role of the messages author, in this case `user`."""


DeploymentStream2DeploymentsRequestType = Literal["file",]
r"""The type of the content part. Always `file`."""


DeploymentStream2DeploymentsRequestRequestBodyType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


DeploymentStream2DeploymentsTTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class DeploymentStream2DeploymentsCacheControlTypedDict(TypedDict):
    type: DeploymentStream2DeploymentsRequestRequestBodyType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[DeploymentStream2DeploymentsTTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class DeploymentStream2DeploymentsCacheControl(BaseModel):
    type: DeploymentStream2DeploymentsRequestRequestBodyType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[DeploymentStream2DeploymentsTTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStream2Deployments4TypedDict(TypedDict):
    type: DeploymentStream2DeploymentsRequestType
    r"""The type of the content part. Always `file`."""
    file: FileContentPartSchemaTypedDict
    r"""File data for the content part. Must contain either file_data or uri, but not both."""
    cache_control: NotRequired[DeploymentStream2DeploymentsCacheControlTypedDict]


class DeploymentStream2Deployments4(BaseModel):
    type: DeploymentStream2DeploymentsRequestType
    r"""The type of the content part. Always `file`."""

    file: FileContentPartSchema
    r"""File data for the content part. Must contain either file_data or uri, but not both."""

    cache_control: Optional[DeploymentStream2DeploymentsCacheControl] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["cache_control"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


DeploymentStreamContentDeploymentsRequestRequestBody2TypedDict = TypeAliasType(
    "DeploymentStreamContentDeploymentsRequestRequestBody2TypedDict",
    Union[
        AudioContentPartSchemaTypedDict,
        TextContentPartSchemaTypedDict,
        ImageContentPartSchemaTypedDict,
        DeploymentStream2Deployments4TypedDict,
    ],
)


DeploymentStreamContentDeploymentsRequestRequestBody2 = Annotated[
    Union[
        Annotated[TextContentPartSchema, Tag("text")],
        Annotated[ImageContentPartSchema, Tag("image_url")],
        Annotated[AudioContentPartSchema, Tag("input_audio")],
        Annotated[DeploymentStream2Deployments4, Tag("file")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


DeploymentStreamMessagesDeploymentsRequestContentTypedDict = TypeAliasType(
    "DeploymentStreamMessagesDeploymentsRequestContentTypedDict",
    Union[str, List[DeploymentStreamContentDeploymentsRequestRequestBody2TypedDict]],
)
r"""The contents of the user message."""


DeploymentStreamMessagesDeploymentsRequestContent = TypeAliasType(
    "DeploymentStreamMessagesDeploymentsRequestContent",
    Union[str, List[DeploymentStreamContentDeploymentsRequestRequestBody2]],
)
r"""The contents of the user message."""


class DeploymentStreamMessagesUserMessageTypedDict(TypedDict):
    role: DeploymentStreamMessagesDeploymentsRequestRole
    r"""The role of the messages author, in this case `user`."""
    content: DeploymentStreamMessagesDeploymentsRequestContentTypedDict
    r"""The contents of the user message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class DeploymentStreamMessagesUserMessage(BaseModel):
    role: DeploymentStreamMessagesDeploymentsRequestRole
    r"""The role of the messages author, in this case `user`."""

    content: DeploymentStreamMessagesDeploymentsRequestContent
    r"""The contents of the user message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


DeploymentStreamMessagesDeploymentsRole = Literal["developer",]
r"""The role of the messages author, in this case  `developer`."""


DeploymentStreamMessagesDeploymentsContentTypedDict = TypeAliasType(
    "DeploymentStreamMessagesDeploymentsContentTypedDict",
    Union[str, List[TextContentPartSchemaTypedDict]],
)
r"""The contents of the developer message."""


DeploymentStreamMessagesDeploymentsContent = TypeAliasType(
    "DeploymentStreamMessagesDeploymentsContent",
    Union[str, List[TextContentPartSchema]],
)
r"""The contents of the developer message."""


class DeploymentStreamMessagesDeveloperMessageTypedDict(TypedDict):
    role: DeploymentStreamMessagesDeploymentsRole
    r"""The role of the messages author, in this case  `developer`."""
    content: DeploymentStreamMessagesDeploymentsContentTypedDict
    r"""The contents of the developer message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class DeploymentStreamMessagesDeveloperMessage(BaseModel):
    role: DeploymentStreamMessagesDeploymentsRole
    r"""The role of the messages author, in this case  `developer`."""

    content: DeploymentStreamMessagesDeploymentsContent
    r"""The contents of the developer message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


DeploymentStreamMessagesRole = Literal["system",]
r"""The role of the messages author, in this case `system`."""


DeploymentStreamMessagesContentTypedDict = TypeAliasType(
    "DeploymentStreamMessagesContentTypedDict",
    Union[str, List[TextContentPartSchemaTypedDict]],
)
r"""The contents of the system message."""


DeploymentStreamMessagesContent = TypeAliasType(
    "DeploymentStreamMessagesContent", Union[str, List[TextContentPartSchema]]
)
r"""The contents of the system message."""


class DeploymentStreamMessagesSystemMessageTypedDict(TypedDict):
    r"""Developer-provided instructions that the model should follow, regardless of messages sent by the user."""

    role: DeploymentStreamMessagesRole
    r"""The role of the messages author, in this case `system`."""
    content: DeploymentStreamMessagesContentTypedDict
    r"""The contents of the system message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class DeploymentStreamMessagesSystemMessage(BaseModel):
    r"""Developer-provided instructions that the model should follow, regardless of messages sent by the user."""

    role: DeploymentStreamMessagesRole
    r"""The role of the messages author, in this case `system`."""

    content: DeploymentStreamMessagesContent
    r"""The contents of the system message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


DeploymentStreamMessagesTypedDict = TypeAliasType(
    "DeploymentStreamMessagesTypedDict",
    Union[
        DeploymentStreamMessagesSystemMessageTypedDict,
        DeploymentStreamMessagesDeveloperMessageTypedDict,
        DeploymentStreamMessagesUserMessageTypedDict,
        DeploymentStreamMessagesToolMessageTypedDict,
        DeploymentStreamMessagesAssistantMessageTypedDict,
    ],
)


DeploymentStreamMessages = Annotated[
    Union[
        Annotated[DeploymentStreamMessagesSystemMessage, Tag("system")],
        Annotated[DeploymentStreamMessagesDeveloperMessage, Tag("developer")],
        Annotated[DeploymentStreamMessagesUserMessage, Tag("user")],
        Annotated[DeploymentStreamMessagesAssistantMessage, Tag("assistant")],
        Annotated[DeploymentStreamMessagesToolMessage, Tag("tool")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


class DeploymentStreamMetadataTypedDict(TypedDict):
    r"""Metadata about the document"""

    file_name: NotRequired[str]
    r"""Name of the file the text is from."""
    file_type: NotRequired[str]
    r"""Content type of the file the text is from."""
    page_number: NotRequired[float]
    r"""The page number the text is from."""


class DeploymentStreamMetadata(BaseModel):
    r"""Metadata about the document"""

    file_name: Optional[str] = None
    r"""Name of the file the text is from."""

    file_type: Optional[str] = None
    r"""Content type of the file the text is from."""

    page_number: Optional[float] = None
    r"""The page number the text is from."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["file_name", "file_type", "page_number"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamDocumentsTypedDict(TypedDict):
    text: str
    r"""The text content of the document"""
    metadata: NotRequired[DeploymentStreamMetadataTypedDict]
    r"""Metadata about the document"""


class DeploymentStreamDocuments(BaseModel):
    text: str
    r"""The text content of the document"""

    metadata: Optional[DeploymentStreamMetadata] = None
    r"""Metadata about the document"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["metadata"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamInvokeOptionsTypedDict(TypedDict):
    include_retrievals: NotRequired[bool]
    r"""Whether to include the retrieved knowledge chunks in the response."""
    include_usage: NotRequired[bool]
    r"""Whether to include the usage metrics in the response."""
    mock_response: NotRequired[str]
    r"""A mock response to use instead of calling the LLM API. This is useful for testing purposes. When provided, the system will return a response object with this content as the completion, without making an actual API call to the LLM provider. This works for both streaming and non-streaming requests. Mock responses will not generate logs, traces or be counted for your plan usage."""


class DeploymentStreamInvokeOptions(BaseModel):
    include_retrievals: Optional[bool] = False
    r"""Whether to include the retrieved knowledge chunks in the response."""

    include_usage: Optional[bool] = False
    r"""Whether to include the usage metrics in the response."""

    mock_response: Optional[str] = None
    r"""A mock response to use instead of calling the LLM API. This is useful for testing purposes. When provided, the system will return a response object with this content as the completion, without making an actual API call to the LLM provider. This works for both streaming and non-streaming requests. Mock responses will not generate logs, traces or be counted for your plan usage."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_retrievals", "include_usage", "mock_response"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamThreadTypedDict(TypedDict):
    id: str
    r"""Unique thread identifier to group related invocations."""
    tags: NotRequired[List[str]]
    r"""Optional tags to differentiate or categorize threads"""


class DeploymentStreamThread(BaseModel):
    id: str
    r"""Unique thread identifier to group related invocations."""

    tags: Optional[List[str]] = None
    r"""Optional tags to differentiate or categorize threads"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["tags"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamOrExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class DeploymentStreamOrExists(BaseModel):
    r"""Exists"""

    exists: bool


DeploymentStreamOrDeploymentsNinTypedDict = TypeAliasType(
    "DeploymentStreamOrDeploymentsNinTypedDict", Union[str, float, bool]
)


DeploymentStreamOrDeploymentsNin = TypeAliasType(
    "DeploymentStreamOrDeploymentsNin", Union[str, float, bool]
)


class DeploymentStreamOrNinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[DeploymentStreamOrDeploymentsNinTypedDict]


class DeploymentStreamOrNin(BaseModel):
    r"""Not in"""

    nin: List[DeploymentStreamOrDeploymentsNin]


DeploymentStreamOrDeploymentsInTypedDict = TypeAliasType(
    "DeploymentStreamOrDeploymentsInTypedDict", Union[str, float, bool]
)


DeploymentStreamOrDeploymentsIn = TypeAliasType(
    "DeploymentStreamOrDeploymentsIn", Union[str, float, bool]
)


class DeploymentStreamOrInTypedDict(TypedDict):
    r"""In"""

    in_: List[DeploymentStreamOrDeploymentsInTypedDict]


class DeploymentStreamOrIn(BaseModel):
    r"""In"""

    in_: Annotated[List[DeploymentStreamOrDeploymentsIn], pydantic.Field(alias="in")]


class DeploymentStreamOrLteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class DeploymentStreamOrLte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class DeploymentStreamOrLtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class DeploymentStreamOrLt(BaseModel):
    r"""Less than"""

    lt: float


class DeploymentStreamOrGteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class DeploymentStreamOrGte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class DeploymentStreamOrGtTypedDict(TypedDict):
    r"""Greater than"""

    gt: float


class DeploymentStreamOrGt(BaseModel):
    r"""Greater than"""

    gt: float


DeploymentStreamOrDeploymentsNeTypedDict = TypeAliasType(
    "DeploymentStreamOrDeploymentsNeTypedDict", Union[str, float, bool]
)


DeploymentStreamOrDeploymentsNe = TypeAliasType(
    "DeploymentStreamOrDeploymentsNe", Union[str, float, bool]
)


class DeploymentStreamOrNeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: DeploymentStreamOrDeploymentsNeTypedDict


class DeploymentStreamOrNe(BaseModel):
    r"""Not equal to"""

    ne: DeploymentStreamOrDeploymentsNe


DeploymentStreamOrDeploymentsEqTypedDict = TypeAliasType(
    "DeploymentStreamOrDeploymentsEqTypedDict", Union[str, float, bool]
)


DeploymentStreamOrDeploymentsEq = TypeAliasType(
    "DeploymentStreamOrDeploymentsEq", Union[str, float, bool]
)


class DeploymentStreamOrEqTypedDict(TypedDict):
    r"""Equal to"""

    eq: DeploymentStreamOrDeploymentsEqTypedDict


class DeploymentStreamOrEq(BaseModel):
    r"""Equal to"""

    eq: DeploymentStreamOrDeploymentsEq


DeploymentStreamKnowledgeFilterDeploymentsOrTypedDict = TypeAliasType(
    "DeploymentStreamKnowledgeFilterDeploymentsOrTypedDict",
    Union[
        DeploymentStreamOrEqTypedDict,
        DeploymentStreamOrNeTypedDict,
        DeploymentStreamOrGtTypedDict,
        DeploymentStreamOrGteTypedDict,
        DeploymentStreamOrLtTypedDict,
        DeploymentStreamOrLteTypedDict,
        DeploymentStreamOrInTypedDict,
        DeploymentStreamOrNinTypedDict,
        DeploymentStreamOrExistsTypedDict,
    ],
)


DeploymentStreamKnowledgeFilterDeploymentsOr = TypeAliasType(
    "DeploymentStreamKnowledgeFilterDeploymentsOr",
    Union[
        DeploymentStreamOrEq,
        DeploymentStreamOrNe,
        DeploymentStreamOrGt,
        DeploymentStreamOrGte,
        DeploymentStreamOrLt,
        DeploymentStreamOrLte,
        DeploymentStreamOrIn,
        DeploymentStreamOrNin,
        DeploymentStreamOrExists,
    ],
)


class DeploymentStreamKnowledgeFilterOrTypedDict(TypedDict):
    r"""Or"""

    or_: List[Dict[str, DeploymentStreamKnowledgeFilterDeploymentsOrTypedDict]]


class DeploymentStreamKnowledgeFilterOr(BaseModel):
    r"""Or"""

    or_: Annotated[
        List[Dict[str, DeploymentStreamKnowledgeFilterDeploymentsOr]],
        pydantic.Field(alias="or"),
    ]


class DeploymentStreamAndExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class DeploymentStreamAndExists(BaseModel):
    r"""Exists"""

    exists: bool


DeploymentStreamAndDeploymentsNinTypedDict = TypeAliasType(
    "DeploymentStreamAndDeploymentsNinTypedDict", Union[str, float, bool]
)


DeploymentStreamAndDeploymentsNin = TypeAliasType(
    "DeploymentStreamAndDeploymentsNin", Union[str, float, bool]
)


class DeploymentStreamAndNinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[DeploymentStreamAndDeploymentsNinTypedDict]


class DeploymentStreamAndNin(BaseModel):
    r"""Not in"""

    nin: List[DeploymentStreamAndDeploymentsNin]


DeploymentStreamAndDeploymentsInTypedDict = TypeAliasType(
    "DeploymentStreamAndDeploymentsInTypedDict", Union[str, float, bool]
)


DeploymentStreamAndDeploymentsIn = TypeAliasType(
    "DeploymentStreamAndDeploymentsIn", Union[str, float, bool]
)


class DeploymentStreamAndInTypedDict(TypedDict):
    r"""In"""

    in_: List[DeploymentStreamAndDeploymentsInTypedDict]


class DeploymentStreamAndIn(BaseModel):
    r"""In"""

    in_: Annotated[List[DeploymentStreamAndDeploymentsIn], pydantic.Field(alias="in")]


class DeploymentStreamAndLteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class DeploymentStreamAndLte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class DeploymentStreamAndLtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class DeploymentStreamAndLt(BaseModel):
    r"""Less than"""

    lt: float


class DeploymentStreamAndGteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class DeploymentStreamAndGte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class DeploymentStreamAndGtTypedDict(TypedDict):
    r"""Greater than"""

    gt: float


class DeploymentStreamAndGt(BaseModel):
    r"""Greater than"""

    gt: float


DeploymentStreamAndDeploymentsNeTypedDict = TypeAliasType(
    "DeploymentStreamAndDeploymentsNeTypedDict", Union[str, float, bool]
)


DeploymentStreamAndDeploymentsNe = TypeAliasType(
    "DeploymentStreamAndDeploymentsNe", Union[str, float, bool]
)


class DeploymentStreamAndNeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: DeploymentStreamAndDeploymentsNeTypedDict


class DeploymentStreamAndNe(BaseModel):
    r"""Not equal to"""

    ne: DeploymentStreamAndDeploymentsNe


DeploymentStreamAndDeploymentsEqTypedDict = TypeAliasType(
    "DeploymentStreamAndDeploymentsEqTypedDict", Union[str, float, bool]
)


DeploymentStreamAndDeploymentsEq = TypeAliasType(
    "DeploymentStreamAndDeploymentsEq", Union[str, float, bool]
)


class DeploymentStreamAndEqTypedDict(TypedDict):
    r"""Equal to"""

    eq: DeploymentStreamAndDeploymentsEqTypedDict


class DeploymentStreamAndEq(BaseModel):
    r"""Equal to"""

    eq: DeploymentStreamAndDeploymentsEq


DeploymentStream1DeploymentsInTypedDict = TypeAliasType(
    "DeploymentStream1DeploymentsInTypedDict", Union[str, float, bool]
)


DeploymentStream1DeploymentsIn = TypeAliasType(
    "DeploymentStream1DeploymentsIn", Union[str, float, bool]
)


class DeploymentStream1InTypedDict(TypedDict):
    r"""In"""

    in_: List[DeploymentStream1DeploymentsInTypedDict]


class DeploymentStream1In(BaseModel):
    r"""In"""

    in_: Annotated[List[DeploymentStream1DeploymentsIn], pydantic.Field(alias="in")]


class DeploymentStream1LteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class DeploymentStream1Lte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class DeploymentStream1LtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class DeploymentStream1Lt(BaseModel):
    r"""Less than"""

    lt: float


class DeploymentStream1GteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class DeploymentStream1Gte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


DeploymentStreamKnowledgeFilterDeploymentsAndTypedDict = TypeAliasType(
    "DeploymentStreamKnowledgeFilterDeploymentsAndTypedDict",
    Union[
        DeploymentStreamAndEqTypedDict,
        DeploymentStreamAndNeTypedDict,
        DeploymentStreamAndGtTypedDict,
        DeploymentStreamAndGteTypedDict,
        DeploymentStreamAndLtTypedDict,
        DeploymentStreamAndLteTypedDict,
        DeploymentStreamAndInTypedDict,
        DeploymentStreamAndNinTypedDict,
        DeploymentStreamAndExistsTypedDict,
    ],
)


DeploymentStreamKnowledgeFilterDeploymentsAnd = TypeAliasType(
    "DeploymentStreamKnowledgeFilterDeploymentsAnd",
    Union[
        DeploymentStreamAndEq,
        DeploymentStreamAndNe,
        DeploymentStreamAndGt,
        DeploymentStreamAndGte,
        DeploymentStreamAndLt,
        DeploymentStreamAndLte,
        DeploymentStreamAndIn,
        DeploymentStreamAndNin,
        DeploymentStreamAndExists,
    ],
)


class DeploymentStreamKnowledgeFilterAndTypedDict(TypedDict):
    r"""And"""

    and_: List[Dict[str, DeploymentStreamKnowledgeFilterDeploymentsAndTypedDict]]


class DeploymentStreamKnowledgeFilterAnd(BaseModel):
    r"""And"""

    and_: Annotated[
        List[Dict[str, DeploymentStreamKnowledgeFilterDeploymentsAnd]],
        pydantic.Field(alias="and"),
    ]


class DeploymentStream1ExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class DeploymentStream1Exists(BaseModel):
    r"""Exists"""

    exists: bool


DeploymentStream1DeploymentsNinTypedDict = TypeAliasType(
    "DeploymentStream1DeploymentsNinTypedDict", Union[str, float, bool]
)


DeploymentStream1DeploymentsNin = TypeAliasType(
    "DeploymentStream1DeploymentsNin", Union[str, float, bool]
)


class DeploymentStream1NinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[DeploymentStream1DeploymentsNinTypedDict]


class DeploymentStream1Nin(BaseModel):
    r"""Not in"""

    nin: List[DeploymentStream1DeploymentsNin]


class DeploymentStream1GtTypedDict(TypedDict):
    r"""Greater than"""

    gt: float


class DeploymentStream1Gt(BaseModel):
    r"""Greater than"""

    gt: float


DeploymentStream1DeploymentsNeTypedDict = TypeAliasType(
    "DeploymentStream1DeploymentsNeTypedDict", Union[str, float, bool]
)


DeploymentStream1DeploymentsNe = TypeAliasType(
    "DeploymentStream1DeploymentsNe", Union[str, float, bool]
)


class DeploymentStream1NeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: DeploymentStream1DeploymentsNeTypedDict


class DeploymentStream1Ne(BaseModel):
    r"""Not equal to"""

    ne: DeploymentStream1DeploymentsNe


DeploymentStream1DeploymentsEqTypedDict = TypeAliasType(
    "DeploymentStream1DeploymentsEqTypedDict", Union[str, float, bool]
)


DeploymentStream1DeploymentsEq = TypeAliasType(
    "DeploymentStream1DeploymentsEq", Union[str, float, bool]
)


class DeploymentStream1EqTypedDict(TypedDict):
    r"""Equal to"""

    eq: DeploymentStream1DeploymentsEqTypedDict


class DeploymentStream1Eq(BaseModel):
    r"""Equal to"""

    eq: DeploymentStream1DeploymentsEq


DeploymentStreamKnowledgeFilter1TypedDict = TypeAliasType(
    "DeploymentStreamKnowledgeFilter1TypedDict",
    Union[
        DeploymentStream1EqTypedDict,
        DeploymentStream1NeTypedDict,
        DeploymentStream1GtTypedDict,
        DeploymentStream1GteTypedDict,
        DeploymentStream1LtTypedDict,
        DeploymentStream1LteTypedDict,
        DeploymentStream1InTypedDict,
        DeploymentStream1NinTypedDict,
        DeploymentStream1ExistsTypedDict,
    ],
)


DeploymentStreamKnowledgeFilter1 = TypeAliasType(
    "DeploymentStreamKnowledgeFilter1",
    Union[
        DeploymentStream1Eq,
        DeploymentStream1Ne,
        DeploymentStream1Gt,
        DeploymentStream1Gte,
        DeploymentStream1Lt,
        DeploymentStream1Lte,
        DeploymentStream1In,
        DeploymentStream1Nin,
        DeploymentStream1Exists,
    ],
)


DeploymentStreamKnowledgeFilterTypedDict = TypeAliasType(
    "DeploymentStreamKnowledgeFilterTypedDict",
    Union[
        DeploymentStreamKnowledgeFilterAndTypedDict,
        DeploymentStreamKnowledgeFilterOrTypedDict,
        Dict[str, DeploymentStreamKnowledgeFilter1TypedDict],
    ],
)
r"""A filter to apply to the knowledge base chunk metadata when using  knowledge bases in the deployment."""


DeploymentStreamKnowledgeFilter = TypeAliasType(
    "DeploymentStreamKnowledgeFilter",
    Union[
        DeploymentStreamKnowledgeFilterAnd,
        DeploymentStreamKnowledgeFilterOr,
        Dict[str, DeploymentStreamKnowledgeFilter1],
    ],
)
r"""A filter to apply to the knowledge base chunk metadata when using  knowledge bases in the deployment."""


class DeploymentStreamRequestBodyTypedDict(TypedDict):
    key: str
    r"""The deployment key to invoke"""
    inputs: NotRequired[Dict[str, Any]]
    r"""Key-value pairs variables to replace in your prompts. If a variable is not provided that is defined in the prompt, the default variables are used."""
    context: NotRequired[Dict[str, Any]]
    r"""Key-value pairs that match your data model and fields declared in your deployment routing configuration"""
    prefix_messages: NotRequired[List[DeploymentStreamPrefixMessagesTypedDict]]
    r"""A list of messages to include after the `System` message, but before the  `User` and `Assistant` pairs configured in your deployment."""
    messages: NotRequired[List[DeploymentStreamMessagesTypedDict]]
    r"""A list of messages to send to the deployment."""
    identity: NotRequired[PublicIdentityTypedDict]
    r"""Information about the identity making the request. If the identity does not exist, it will be created automatically."""
    file_ids: NotRequired[List[str]]
    r"""A list of file IDs that are associated with the deployment request."""
    metadata: NotRequired[Dict[str, Any]]
    r"""Key-value pairs that you want to attach to the log generated by this request."""
    extra_params: NotRequired[Dict[str, Any]]
    r"""Utilized for passing additional parameters to the model provider. Exercise caution when using this feature, as the included parameters will overwrite any parameters specified in the deployment prompt configuration."""
    documents: NotRequired[List[DeploymentStreamDocumentsTypedDict]]
    r"""A list of documents from your external knowledge base (e.g., chunks retrieved from your own vector database or RAG pipeline) that provide context for the model response. These documents can be used by evaluators and guardrails to assess the relevance and accuracy of the model output against the provided context."""
    invoke_options: NotRequired[DeploymentStreamInvokeOptionsTypedDict]
    thread: NotRequired[DeploymentStreamThreadTypedDict]
    knowledge_filter: NotRequired[DeploymentStreamKnowledgeFilterTypedDict]
    r"""A filter to apply to the knowledge base chunk metadata when using  knowledge bases in the deployment."""


class DeploymentStreamRequestBody(BaseModel):
    key: str
    r"""The deployment key to invoke"""

    inputs: Optional[Dict[str, Any]] = None
    r"""Key-value pairs variables to replace in your prompts. If a variable is not provided that is defined in the prompt, the default variables are used."""

    context: Optional[Dict[str, Any]] = None
    r"""Key-value pairs that match your data model and fields declared in your deployment routing configuration"""

    prefix_messages: Optional[List[DeploymentStreamPrefixMessages]] = None
    r"""A list of messages to include after the `System` message, but before the  `User` and `Assistant` pairs configured in your deployment."""

    messages: Optional[List[DeploymentStreamMessages]] = None
    r"""A list of messages to send to the deployment."""

    identity: Optional[PublicIdentity] = None
    r"""Information about the identity making the request. If the identity does not exist, it will be created automatically."""

    file_ids: Optional[List[str]] = None
    r"""A list of file IDs that are associated with the deployment request."""

    metadata: Optional[Dict[str, Any]] = None
    r"""Key-value pairs that you want to attach to the log generated by this request."""

    extra_params: Optional[Dict[str, Any]] = None
    r"""Utilized for passing additional parameters to the model provider. Exercise caution when using this feature, as the included parameters will overwrite any parameters specified in the deployment prompt configuration."""

    documents: Optional[List[DeploymentStreamDocuments]] = None
    r"""A list of documents from your external knowledge base (e.g., chunks retrieved from your own vector database or RAG pipeline) that provide context for the model response. These documents can be used by evaluators and guardrails to assess the relevance and accuracy of the model output against the provided context."""

    invoke_options: Optional[DeploymentStreamInvokeOptions] = None

    thread: Optional[DeploymentStreamThread] = None

    knowledge_filter: Optional[DeploymentStreamKnowledgeFilter] = None
    r"""A filter to apply to the knowledge base chunk metadata when using  knowledge bases in the deployment."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "inputs",
                "context",
                "prefix_messages",
                "messages",
                "identity",
                "file_ids",
                "metadata",
                "extra_params",
                "documents",
                "invoke_options",
                "thread",
                "knowledge_filter",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


DeploymentStreamObject = Literal[
    "chat",
    "completion",
    "image",
]
r"""Indicates the type of model used to generate the response"""


DeploymentStreamProvider = Literal[
    "openai",
    "groq",
    "cohere",
    "azure",
    "aws",
    "google",
    "google-ai",
    "huggingface",
    "togetherai",
    "perplexity",
    "anthropic",
    "leonardoai",
    "fal",
    "nvidia",
    "jina",
    "elevenlabs",
    "litellm",
    "cerebras",
    "openailike",
    "bytedance",
    "mistral",
    "deepseek",
    "contextualai",
    "moonshotai",
    "zai",
    "slack",
]
r"""The provider used to generate the response"""


class DeploymentStreamDeploymentsMetadataTypedDict(TypedDict):
    r"""Metadata of the retrieved chunk from the knowledge base"""

    file_name: str
    r"""Name of the file"""
    page_number: Nullable[float]
    r"""Page number of the chunk"""
    file_type: str
    r"""Type of the file"""
    search_score: float
    r"""Search scores are normalized to be in the range [0, 1]. Search score is calculated based on `[Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)` algorithm. Scores close to 1 indicate the document is closer to the query, and scores closer to 0 indicate the document is farther from the query."""
    rerank_score: NotRequired[float]
    r"""Rerank scores are normalized to be in the range [0, 1]. Scores close to 1 indicate a high relevance to the query, and scores closer to 0 indicate low relevance. It is not accurate to assume a score of 0.9 means the document is 2x more relevant than a document with a score of 0.45"""


class DeploymentStreamDeploymentsMetadata(BaseModel):
    r"""Metadata of the retrieved chunk from the knowledge base"""

    file_name: str
    r"""Name of the file"""

    page_number: Nullable[float]
    r"""Page number of the chunk"""

    file_type: str
    r"""Type of the file"""

    search_score: float
    r"""Search scores are normalized to be in the range [0, 1]. Search score is calculated based on `[Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)` algorithm. Scores close to 1 indicate the document is closer to the query, and scores closer to 0 indicate the document is farther from the query."""

    rerank_score: Optional[float] = None
    r"""Rerank scores are normalized to be in the range [0, 1]. Scores close to 1 indicate a high relevance to the query, and scores closer to 0 indicate low relevance. It is not accurate to assume a score of 0.9 means the document is 2x more relevant than a document with a score of 0.45"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["rerank_score"])
        nullable_fields = set(["page_number"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class DeploymentStreamRetrievalsTypedDict(TypedDict):
    document: str
    r"""Content of the retrieved chunk from the knowledge base"""
    metadata: DeploymentStreamDeploymentsMetadataTypedDict
    r"""Metadata of the retrieved chunk from the knowledge base"""


class DeploymentStreamRetrievals(BaseModel):
    document: str
    r"""Content of the retrieved chunk from the knowledge base"""

    metadata: DeploymentStreamDeploymentsMetadata
    r"""Metadata of the retrieved chunk from the knowledge base"""


class DeploymentStreamPromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[float]]


class DeploymentStreamPromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[float] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["cached_tokens"])
        nullable_fields = set(["cached_tokens"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class DeploymentStreamCompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]


class DeploymentStreamCompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["reasoning_tokens"])
        nullable_fields = set(["reasoning_tokens"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class DeploymentStreamUsageTypedDict(TypedDict):
    r"""Usage metrics for the response"""

    total_tokens: NotRequired[float]
    prompt_tokens: NotRequired[float]
    completion_tokens: NotRequired[float]
    prompt_tokens_details: NotRequired[DeploymentStreamPromptTokensDetailsTypedDict]
    completion_tokens_details: NotRequired[
        Nullable[DeploymentStreamCompletionTokensDetailsTypedDict]
    ]


class DeploymentStreamUsage(BaseModel):
    r"""Usage metrics for the response"""

    total_tokens: Optional[float] = None

    prompt_tokens: Optional[float] = None

    completion_tokens: Optional[float] = None

    prompt_tokens_details: Optional[DeploymentStreamPromptTokensDetails] = None

    completion_tokens_details: OptionalNullable[
        DeploymentStreamCompletionTokensDetails
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "total_tokens",
                "prompt_tokens",
                "completion_tokens",
                "prompt_tokens_details",
                "completion_tokens_details",
            ]
        )
        nullable_fields = set(["completion_tokens_details"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


DeploymentStreamMessageDeploymentsResponseType = Literal["image",]


DeploymentStreamMessageDeploymentsResponseRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""


class DeploymentStreamMessage3TypedDict(TypedDict):
    type: DeploymentStreamMessageDeploymentsResponseType
    role: DeploymentStreamMessageDeploymentsResponseRole
    r"""The role of the prompt message"""
    url: str


class DeploymentStreamMessage3(BaseModel):
    type: DeploymentStreamMessageDeploymentsResponseType

    role: DeploymentStreamMessageDeploymentsResponseRole
    r"""The role of the prompt message"""

    url: str


DeploymentStreamMessageDeploymentsType = Literal["content",]


DeploymentStreamMessageDeploymentsRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""


class DeploymentStreamMessage2TypedDict(TypedDict):
    type: DeploymentStreamMessageDeploymentsType
    role: DeploymentStreamMessageDeploymentsRole
    r"""The role of the prompt message"""
    content: Nullable[str]
    reasoning: NotRequired[str]
    r"""Internal thought process of the model"""
    reasoning_signature: NotRequired[str]
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""
    redacted_reasoning: NotRequired[str]
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""


class DeploymentStreamMessage2(BaseModel):
    type: DeploymentStreamMessageDeploymentsType

    role: DeploymentStreamMessageDeploymentsRole
    r"""The role of the prompt message"""

    content: Nullable[str]

    reasoning: Optional[str] = None
    r"""Internal thought process of the model"""

    reasoning_signature: Optional[str] = None
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""

    redacted_reasoning: Optional[str] = None
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["reasoning", "reasoning_signature", "redacted_reasoning"]
        )
        nullable_fields = set(["content"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


DeploymentStreamMessageType = Literal["tool_calls",]


DeploymentStreamMessageRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""


DeploymentStreamMessageDeploymentsResponse200Type = Literal["function",]


class DeploymentStreamMessageFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class DeploymentStreamMessageFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class DeploymentStreamMessageToolCallsTypedDict(TypedDict):
    type: DeploymentStreamMessageDeploymentsResponse200Type
    function: DeploymentStreamMessageFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class DeploymentStreamMessageToolCalls(BaseModel):
    type: DeploymentStreamMessageDeploymentsResponse200Type

    function: DeploymentStreamMessageFunction

    id: Optional[str] = None

    index: Optional[float] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["id", "index"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeploymentStreamMessage1TypedDict(TypedDict):
    type: DeploymentStreamMessageType
    role: DeploymentStreamMessageRole
    r"""The role of the prompt message"""
    tool_calls: List[DeploymentStreamMessageToolCallsTypedDict]
    content: NotRequired[Nullable[str]]
    reasoning: NotRequired[str]
    r"""Internal thought process of the model"""
    reasoning_signature: NotRequired[str]
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""
    redacted_reasoning: NotRequired[str]
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""


class DeploymentStreamMessage1(BaseModel):
    type: DeploymentStreamMessageType

    role: DeploymentStreamMessageRole
    r"""The role of the prompt message"""

    tool_calls: List[DeploymentStreamMessageToolCalls]

    content: OptionalNullable[str] = UNSET

    reasoning: Optional[str] = None
    r"""Internal thought process of the model"""

    reasoning_signature: Optional[str] = None
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""

    redacted_reasoning: Optional[str] = None
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["content", "reasoning", "reasoning_signature", "redacted_reasoning"]
        )
        nullable_fields = set(["content"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


DeploymentStreamMessageTypedDict = TypeAliasType(
    "DeploymentStreamMessageTypedDict",
    Union[
        DeploymentStreamMessage3TypedDict,
        DeploymentStreamMessage2TypedDict,
        DeploymentStreamMessage1TypedDict,
    ],
)


DeploymentStreamMessage = Annotated[
    Union[
        Annotated[DeploymentStreamMessage1, Tag("tool_calls")],
        Annotated[DeploymentStreamMessage2, Tag("content")],
        Annotated[DeploymentStreamMessage3, Tag("image")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class DeploymentStreamChoicesTypedDict(TypedDict):
    index: float
    message: DeploymentStreamMessageTypedDict
    finish_reason: NotRequired[Nullable[str]]


class DeploymentStreamChoices(BaseModel):
    index: float

    message: DeploymentStreamMessage

    finish_reason: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["finish_reason"])
        nullable_fields = set(["finish_reason"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class DeploymentStreamDataTypedDict(TypedDict):
    id: str
    r"""A unique identifier for the response. Can be used to add metrics to the transaction."""
    created: datetime
    r"""A timestamp indicating when the object was created. Usually in a standardized format like ISO 8601"""
    object: DeploymentStreamObject
    r"""Indicates the type of model used to generate the response"""
    model: str
    r"""The model used to generate the response"""
    provider: DeploymentStreamProvider
    r"""The provider used to generate the response"""
    is_final: bool
    r"""Indicates if the response is the final response"""
    choices: List[DeploymentStreamChoicesTypedDict]
    r"""A list of choices generated by the model"""
    integration_id: NotRequired[str]
    r"""Indicates integration id used to generate the response"""
    finalized: NotRequired[datetime]
    r"""A timestamp indicating when the object was finalized. Usually in a standardized format like ISO 8601"""
    system_fingerprint: NotRequired[Nullable[str]]
    r"""Provider backed system fingerprint."""
    retrievals: NotRequired[List[DeploymentStreamRetrievalsTypedDict]]
    r"""List of documents retrieved from the knowledge base. This property is only available when the `include_retrievals` flag is set to `true` in the invoke settings. When stream is set to true, the `retrievals` property will be returned in the last streamed chunk where the property `is_final` is set to `true`."""
    provider_response: NotRequired[Any]
    r"""Response returned by the model provider. This functionality is only supported when streaming is not used. If streaming is used, the `provider_response` property will be set to `null`."""
    usage: NotRequired[Nullable[DeploymentStreamUsageTypedDict]]
    r"""Usage metrics for the response"""


class DeploymentStreamData(BaseModel):
    id: str
    r"""A unique identifier for the response. Can be used to add metrics to the transaction."""

    created: datetime
    r"""A timestamp indicating when the object was created. Usually in a standardized format like ISO 8601"""

    object: DeploymentStreamObject
    r"""Indicates the type of model used to generate the response"""

    model: str
    r"""The model used to generate the response"""

    provider: DeploymentStreamProvider
    r"""The provider used to generate the response"""

    is_final: bool
    r"""Indicates if the response is the final response"""

    choices: List[DeploymentStreamChoices]
    r"""A list of choices generated by the model"""

    integration_id: Optional[str] = None
    r"""Indicates integration id used to generate the response"""

    finalized: Optional[datetime] = None
    r"""A timestamp indicating when the object was finalized. Usually in a standardized format like ISO 8601"""

    system_fingerprint: OptionalNullable[str] = UNSET
    r"""Provider backed system fingerprint."""

    retrievals: Optional[List[DeploymentStreamRetrievals]] = None
    r"""List of documents retrieved from the knowledge base. This property is only available when the `include_retrievals` flag is set to `true` in the invoke settings. When stream is set to true, the `retrievals` property will be returned in the last streamed chunk where the property `is_final` is set to `true`."""

    provider_response: Optional[Any] = None
    r"""Response returned by the model provider. This functionality is only supported when streaming is not used. If streaming is used, the `provider_response` property will be set to `null`."""

    usage: OptionalNullable[DeploymentStreamUsage] = UNSET
    r"""Usage metrics for the response"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "integration_id",
                "finalized",
                "system_fingerprint",
                "retrievals",
                "provider_response",
                "usage",
            ]
        )
        nullable_fields = set(["system_fingerprint", "usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class DeploymentStreamResponseBodyTypedDict(TypedDict):
    r"""Successful operation"""

    data: NotRequired[DeploymentStreamDataTypedDict]


class DeploymentStreamResponseBody(BaseModel):
    r"""Successful operation"""

    data: Optional[DeploymentStreamData] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["data"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
