"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from orq_ai_sdk.types import BaseModel, Nullable, UNSET_SENTINEL
from orq_ai_sdk.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


ParseChunkingRequestChunkingRequestReturnType = Literal[
    "chunks",
    "texts",
]
r"""Return format: chunks (with metadata) or texts (plain strings)"""


AgenticChunker = Literal["agentic",]


class AgenticChunkerStrategyTypedDict(TypedDict):
    r"""Agentic LLM-powered chunker that uses AI to determine optimal split points. Best for complex documents requiring intelligent segmentation."""

    text: str
    r"""The text content to be chunked"""
    strategy: AgenticChunker
    model: str
    r"""Model to use for chunking. (Available models)[https://docs.orq.ai/docs/proxy/supported-models#chat-models]"""
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[ParseChunkingRequestChunkingRequestReturnType]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    candidate_size: NotRequired[int]
    r"""Size of candidate splits for LLM evaluation"""
    min_characters_per_chunk: NotRequired[int]
    r"""Minimum characters allowed per chunk"""


class AgenticChunkerStrategy(BaseModel):
    r"""Agentic LLM-powered chunker that uses AI to determine optimal split points. Best for complex documents requiring intelligent segmentation."""

    text: str
    r"""The text content to be chunked"""

    strategy: AgenticChunker

    model: str
    r"""Model to use for chunking. (Available models)[https://docs.orq.ai/docs/proxy/supported-models#chat-models]"""

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[ParseChunkingRequestChunkingRequestReturnType] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 1024
    r"""Maximum tokens per chunk"""

    candidate_size: Optional[int] = 128
    r"""Size of candidate splits for LLM evaluation"""

    min_characters_per_chunk: Optional[int] = 24
    r"""Minimum characters allowed per chunk"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "metadata",
                "return_type",
                "chunk_size",
                "candidate_size",
                "min_characters_per_chunk",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ParseChunkingRequestChunkingReturnType = Literal[
    "chunks",
    "texts",
]
r"""Return format: chunks (with metadata) or texts (plain strings)"""


SemanticChunker = Literal["semantic",]


Threshold2 = Literal["auto",]


ThresholdTypedDict = TypeAliasType("ThresholdTypedDict", Union[float, Threshold2])
r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""


Threshold = TypeAliasType("Threshold", Union[float, Threshold2])
r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""


Mode = Literal[
    "window",
    "sentence",
]
r"""Chunking mode: window-based or sentence-based similarity"""


class SemanticChunkerStrategyTypedDict(TypedDict):
    r"""Groups semantically similar sentences using embeddings. Excellent for maintaining topic coherence and context within chunks."""

    text: str
    r"""The text content to be chunked"""
    strategy: SemanticChunker
    embedding_model: str
    r"""Embedding model to use for semantic similarity. (Available embedding models)[https://docs.orq.ai/docs/proxy/supported-models#embedding-models]"""
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[ParseChunkingRequestChunkingReturnType]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    threshold: NotRequired[ThresholdTypedDict]
    r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""
    dimensions: NotRequired[int]
    r"""Number of dimensions for the embedding output. Required for text-embedding-3 models. Supported range: 256-3072 for text-embedding-3-large, 256-1536 for text-embedding-3-small."""
    max_tokens: NotRequired[int]
    r"""Maximum number of tokens per embedding request. Default is 8191 for text-embedding-3 models."""
    mode: NotRequired[Mode]
    r"""Chunking mode: window-based or sentence-based similarity"""
    similarity_window: NotRequired[int]
    r"""Window size for similarity comparison"""


class SemanticChunkerStrategy(BaseModel):
    r"""Groups semantically similar sentences using embeddings. Excellent for maintaining topic coherence and context within chunks."""

    text: str
    r"""The text content to be chunked"""

    strategy: SemanticChunker

    embedding_model: str
    r"""Embedding model to use for semantic similarity. (Available embedding models)[https://docs.orq.ai/docs/proxy/supported-models#embedding-models]"""

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[ParseChunkingRequestChunkingReturnType] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 512
    r"""Maximum tokens per chunk"""

    threshold: Optional[Threshold] = None
    r"""Similarity threshold for grouping (0-1) or \"auto\" for automatic detection"""

    dimensions: Optional[int] = None
    r"""Number of dimensions for the embedding output. Required for text-embedding-3 models. Supported range: 256-3072 for text-embedding-3-large, 256-1536 for text-embedding-3-small."""

    max_tokens: Optional[int] = None
    r"""Maximum number of tokens per embedding request. Default is 8191 for text-embedding-3 models."""

    mode: Optional[Mode] = "window"
    r"""Chunking mode: window-based or sentence-based similarity"""

    similarity_window: Optional[int] = 1
    r"""Window size for similarity comparison"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "metadata",
                "return_type",
                "chunk_size",
                "threshold",
                "dimensions",
                "max_tokens",
                "mode",
                "similarity_window",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ParseChunkingRequestReturnType = Literal[
    "chunks",
    "texts",
]
r"""Return format: chunks (with metadata) or texts (plain strings)"""


RecursiveChunker = Literal["recursive",]


class RecursiveChunkerStrategyTypedDict(TypedDict):
    r"""Recursively splits text using a hierarchy of separators (paragraphs, sentences, words). Versatile general-purpose chunker that preserves document structure."""

    text: str
    r"""The text content to be chunked"""
    strategy: RecursiveChunker
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[ParseChunkingRequestReturnType]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    separators: NotRequired[List[str]]
    r"""Hierarchy of separators to use for splitting"""
    min_characters_per_chunk: NotRequired[int]
    r"""Minimum characters allowed per chunk"""


class RecursiveChunkerStrategy(BaseModel):
    r"""Recursively splits text using a hierarchy of separators (paragraphs, sentences, words). Versatile general-purpose chunker that preserves document structure."""

    text: str
    r"""The text content to be chunked"""

    strategy: RecursiveChunker

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[ParseChunkingRequestReturnType] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 512
    r"""Maximum tokens per chunk"""

    separators: Optional[List[str]] = None
    r"""Hierarchy of separators to use for splitting"""

    min_characters_per_chunk: Optional[int] = 24
    r"""Minimum characters allowed per chunk"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "metadata",
                "return_type",
                "chunk_size",
                "separators",
                "min_characters_per_chunk",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ChunkingRequestReturnType = Literal[
    "chunks",
    "texts",
]
r"""Return format: chunks (with metadata) or texts (plain strings)"""


SentenceChunker = Literal["sentence",]


class SentenceChunkerStrategyTypedDict(TypedDict):
    r"""Splits text at sentence boundaries while respecting token limits. Ideal for maintaining semantic coherence and readability."""

    text: str
    r"""The text content to be chunked"""
    strategy: SentenceChunker
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[ChunkingRequestReturnType]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    chunk_overlap: NotRequired[int]
    r"""Number of overlapping tokens between chunks"""
    min_sentences_per_chunk: NotRequired[int]
    r"""Minimum number of sentences per chunk"""


class SentenceChunkerStrategy(BaseModel):
    r"""Splits text at sentence boundaries while respecting token limits. Ideal for maintaining semantic coherence and readability."""

    text: str
    r"""The text content to be chunked"""

    strategy: SentenceChunker

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[ChunkingRequestReturnType] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 512
    r"""Maximum tokens per chunk"""

    chunk_overlap: Optional[int] = 0
    r"""Number of overlapping tokens between chunks"""

    min_sentences_per_chunk: Optional[int] = 1
    r"""Minimum number of sentences per chunk"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "metadata",
                "return_type",
                "chunk_size",
                "chunk_overlap",
                "min_sentences_per_chunk",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ReturnType = Literal[
    "chunks",
    "texts",
]
r"""Return format: chunks (with metadata) or texts (plain strings)"""


TokenChunker = Literal["token",]


class TokenChunkerStrategyTypedDict(TypedDict):
    r"""Splits text based on token count. Best for ensuring chunks fit within LLM context windows and maintaining consistent chunk sizes for embedding models."""

    text: str
    r"""The text content to be chunked"""
    strategy: TokenChunker
    metadata: NotRequired[bool]
    r"""Whether to include metadata for each chunk"""
    return_type: NotRequired[ReturnType]
    r"""Return format: chunks (with metadata) or texts (plain strings)"""
    chunk_size: NotRequired[int]
    r"""Maximum tokens per chunk"""
    chunk_overlap: NotRequired[int]
    r"""Number of tokens to overlap between chunks"""


class TokenChunkerStrategy(BaseModel):
    r"""Splits text based on token count. Best for ensuring chunks fit within LLM context windows and maintaining consistent chunk sizes for embedding models."""

    text: str
    r"""The text content to be chunked"""

    strategy: TokenChunker

    metadata: Optional[bool] = True
    r"""Whether to include metadata for each chunk"""

    return_type: Optional[ReturnType] = "chunks"
    r"""Return format: chunks (with metadata) or texts (plain strings)"""

    chunk_size: Optional[int] = 512
    r"""Maximum tokens per chunk"""

    chunk_overlap: Optional[int] = 0
    r"""Number of tokens to overlap between chunks"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["metadata", "return_type", "chunk_size", "chunk_overlap"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ParseChunkingRequestTypedDict = TypeAliasType(
    "ParseChunkingRequestTypedDict",
    Union[
        TokenChunkerStrategyTypedDict,
        SentenceChunkerStrategyTypedDict,
        RecursiveChunkerStrategyTypedDict,
        AgenticChunkerStrategyTypedDict,
        SemanticChunkerStrategyTypedDict,
    ],
)
r"""Request payload for text chunking with strategy-specific configuration"""


ParseChunkingRequest = Annotated[
    Union[
        Annotated[TokenChunkerStrategy, Tag("token")],
        Annotated[SentenceChunkerStrategy, Tag("sentence")],
        Annotated[RecursiveChunkerStrategy, Tag("recursive")],
        Annotated[SemanticChunkerStrategy, Tag("semantic")],
        Annotated[AgenticChunkerStrategy, Tag("agentic")],
    ],
    Discriminator(lambda m: get_discriminator(m, "strategy", "strategy")),
]
r"""Request payload for text chunking with strategy-specific configuration"""


class ParseMetadataTypedDict(TypedDict):
    start_index: Nullable[float]
    end_index: Nullable[float]
    token_count: Nullable[float]


class ParseMetadata(BaseModel):
    start_index: Nullable[float]

    end_index: Nullable[float]

    token_count: Nullable[float]

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class ChunksTypedDict(TypedDict):
    text: str
    r"""The text content of the chunk"""
    index: float
    r"""The position index of this chunk in the sequence"""
    metadata: NotRequired[ParseMetadataTypedDict]


class Chunks(BaseModel):
    text: str
    r"""The text content of the chunk"""

    index: float
    r"""The position index of this chunk in the sequence"""

    metadata: Optional[ParseMetadata] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["metadata"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class ParseResponseBodyTypedDict(TypedDict):
    r"""Text successfully chunked"""

    chunks: List[ChunksTypedDict]


class ParseResponseBody(BaseModel):
    r"""Text successfully chunked"""

    chunks: List[Chunks]
