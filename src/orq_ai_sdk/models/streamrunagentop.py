"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from dataclasses import dataclass, field
import httpx
from orq_ai_sdk.models import OrqError
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


StreamRunAgentModelVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


StreamRunAgentModelFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class StreamRunAgentModelAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: StreamRunAgentModelVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: StreamRunAgentModelFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class StreamRunAgentModelAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: StreamRunAgentModelVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[StreamRunAgentModelFormat, pydantic.Field(alias="format")]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


StreamRunAgentResponseFormatAgentsRequestType = Literal["json_schema",]


class StreamRunAgentResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class StreamRunAgentResponseFormatJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = None
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class StreamRunAgentResponseFormat3TypedDict(TypedDict):
    type: StreamRunAgentResponseFormatAgentsRequestType
    json_schema: StreamRunAgentResponseFormatJSONSchemaTypedDict


class StreamRunAgentResponseFormat3(BaseModel):
    type: StreamRunAgentResponseFormatAgentsRequestType

    json_schema: StreamRunAgentResponseFormatJSONSchema


StreamRunAgentResponseFormatAgentsType = Literal["json_object",]


class StreamRunAgentResponseFormat2TypedDict(TypedDict):
    type: StreamRunAgentResponseFormatAgentsType


class StreamRunAgentResponseFormat2(BaseModel):
    type: StreamRunAgentResponseFormatAgentsType


StreamRunAgentResponseFormatType = Literal["text",]


class StreamRunAgentResponseFormat1TypedDict(TypedDict):
    type: StreamRunAgentResponseFormatType


class StreamRunAgentResponseFormat1(BaseModel):
    type: StreamRunAgentResponseFormatType


StreamRunAgentModelResponseFormatTypedDict = TypeAliasType(
    "StreamRunAgentModelResponseFormatTypedDict",
    Union[
        StreamRunAgentResponseFormat1TypedDict,
        StreamRunAgentResponseFormat2TypedDict,
        StreamRunAgentResponseFormat3TypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


StreamRunAgentModelResponseFormat = TypeAliasType(
    "StreamRunAgentModelResponseFormat",
    Union[
        StreamRunAgentResponseFormat1,
        StreamRunAgentResponseFormat2,
        StreamRunAgentResponseFormat3,
    ],
)
r"""An object specifying the format that the model must output"""


StreamRunAgentModelStopTypedDict = TypeAliasType(
    "StreamRunAgentModelStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


StreamRunAgentModelStop = TypeAliasType(
    "StreamRunAgentModelStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class StreamRunAgentModelStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class StreamRunAgentModelStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


StreamRunAgentModelType = Literal[
    "enabled",
    "disabled",
]
r"""Enables or disables the thinking mode capability"""


class StreamRunAgentModelThinkingTypedDict(TypedDict):
    type: StreamRunAgentModelType
    r"""Enables or disables the thinking mode capability"""
    budget_tokens: float
    r"""Determines how many tokens the model can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than `max_tokens`."""


class StreamRunAgentModelThinking(BaseModel):
    type: StreamRunAgentModelType
    r"""Enables or disables the thinking mode capability"""

    budget_tokens: float
    r"""Determines how many tokens the model can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than `max_tokens`."""


StreamRunAgentToolChoiceType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class StreamRunAgentToolChoiceFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""


class StreamRunAgentToolChoiceFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""


class StreamRunAgentToolChoice2TypedDict(TypedDict):
    function: StreamRunAgentToolChoiceFunctionTypedDict
    type: NotRequired[StreamRunAgentToolChoiceType]
    r"""The type of the tool. Currently, only function is supported."""


class StreamRunAgentToolChoice2(BaseModel):
    function: StreamRunAgentToolChoiceFunction

    type: Optional[StreamRunAgentToolChoiceType] = None
    r"""The type of the tool. Currently, only function is supported."""


StreamRunAgentToolChoice1 = Literal[
    "none",
    "auto",
    "required",
]


StreamRunAgentModelToolChoiceTypedDict = TypeAliasType(
    "StreamRunAgentModelToolChoiceTypedDict",
    Union[StreamRunAgentToolChoice2TypedDict, StreamRunAgentToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


StreamRunAgentModelToolChoice = TypeAliasType(
    "StreamRunAgentModelToolChoice",
    Union[StreamRunAgentToolChoice2, StreamRunAgentToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


StreamRunAgentModelModalities = Literal[
    "text",
    "audio",
]


class StreamRunAgentModelWebSearchOptionsTypedDict(TypedDict):
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""

    enabled: NotRequired[bool]
    r"""Whether to enable web search for this request."""


class StreamRunAgentModelWebSearchOptions(BaseModel):
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""

    enabled: Optional[bool] = None
    r"""Whether to enable web search for this request."""


class StreamRunAgentModelParametersTypedDict(TypedDict):
    r"""Model parameters to customize behavior (snake_case). Common: temperature (0-1, controls randomness), max_tokens (response length). Advanced: top_p (nucleus sampling), frequency_penalty, presence_penalty, response_format, reasoning_effort. Not all parameters work with all models."""

    audio: NotRequired[Nullable[StreamRunAgentModelAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[StreamRunAgentModelResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[str]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[StreamRunAgentModelStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[Nullable[StreamRunAgentModelStreamOptionsTypedDict]]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[StreamRunAgentModelThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[StreamRunAgentModelToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[Nullable[List[StreamRunAgentModelModalities]]]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    web_search_options: NotRequired[StreamRunAgentModelWebSearchOptionsTypedDict]
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""


class StreamRunAgentModelParameters(BaseModel):
    r"""Model parameters to customize behavior (snake_case). Common: temperature (0-1, controls randomness), max_tokens (response length). Advanced: top_p (nucleus sampling), frequency_penalty, presence_penalty, response_format, reasoning_effort. Not all parameters work with all models."""

    audio: OptionalNullable[StreamRunAgentModelAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[StreamRunAgentModelResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[str] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[StreamRunAgentModelStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[StreamRunAgentModelStreamOptions] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[StreamRunAgentModelThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[StreamRunAgentModelToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[StreamRunAgentModelModalities]] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    web_search_options: Optional[StreamRunAgentModelWebSearchOptions] = None
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "audio",
            "frequency_penalty",
            "max_tokens",
            "max_completion_tokens",
            "logprobs",
            "top_logprobs",
            "n",
            "presence_penalty",
            "response_format",
            "reasoning_effort",
            "verbosity",
            "seed",
            "stop",
            "stream_options",
            "thinking",
            "temperature",
            "top_p",
            "top_k",
            "tool_choice",
            "parallel_tool_calls",
            "modalities",
            "web_search_options",
        ]
        nullable_fields = [
            "audio",
            "frequency_penalty",
            "max_tokens",
            "max_completion_tokens",
            "logprobs",
            "top_logprobs",
            "n",
            "presence_penalty",
            "seed",
            "stop",
            "stream_options",
            "temperature",
            "top_p",
            "top_k",
            "modalities",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class StreamRunAgentModel2TypedDict(TypedDict):
    r"""Model configuration with parameters"""

    id: str
    r"""Model ID or provider/model string"""
    integration_id: NotRequired[Nullable[str]]
    r"""Optional integration ID for custom configurations"""
    parameters: NotRequired[StreamRunAgentModelParametersTypedDict]
    r"""Model parameters to customize behavior (snake_case). Common: temperature (0-1, controls randomness), max_tokens (response length). Advanced: top_p (nucleus sampling), frequency_penalty, presence_penalty, response_format, reasoning_effort. Not all parameters work with all models."""


class StreamRunAgentModel2(BaseModel):
    r"""Model configuration with parameters"""

    id: str
    r"""Model ID or provider/model string"""

    integration_id: OptionalNullable[str] = UNSET
    r"""Optional integration ID for custom configurations"""

    parameters: Optional[StreamRunAgentModelParameters] = None
    r"""Model parameters to customize behavior (snake_case). Common: temperature (0-1, controls randomness), max_tokens (response length). Advanced: top_p (nucleus sampling), frequency_penalty, presence_penalty, response_format, reasoning_effort. Not all parameters work with all models."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["integration_id", "parameters"]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


StreamRunAgentModelTypedDict = TypeAliasType(
    "StreamRunAgentModelTypedDict", Union[StreamRunAgentModel2TypedDict, str]
)
r"""The language model that powers the agent. Can be a simple string (e.g., \"openai/gpt-4o\") or an object with model ID and parameters. The model must support tool calling capabilities."""


StreamRunAgentModel = TypeAliasType(
    "StreamRunAgentModel", Union[StreamRunAgentModel2, str]
)
r"""The language model that powers the agent. Can be a simple string (e.g., \"openai/gpt-4o\") or an object with model ID and parameters. The model must support tool calling capabilities."""


StreamRunAgentFallbackModelsVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


StreamRunAgentFallbackModelsFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class StreamRunAgentFallbackModelsAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: StreamRunAgentFallbackModelsVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: StreamRunAgentFallbackModelsFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class StreamRunAgentFallbackModelsAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: StreamRunAgentFallbackModelsVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[
        StreamRunAgentFallbackModelsFormat, pydantic.Field(alias="format")
    ]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModels2Type = Literal[
    "json_schema",
]


class StreamRunAgentResponseFormatAgentsJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class StreamRunAgentResponseFormatAgentsJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = None
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class StreamRunAgentResponseFormatAgents3TypedDict(TypedDict):
    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModels2Type
    json_schema: StreamRunAgentResponseFormatAgentsJSONSchemaTypedDict


class StreamRunAgentResponseFormatAgents3(BaseModel):
    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModels2Type

    json_schema: StreamRunAgentResponseFormatAgentsJSONSchema


StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsType = Literal[
    "json_object",
]


class StreamRunAgentResponseFormatAgents2TypedDict(TypedDict):
    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsType


class StreamRunAgentResponseFormatAgents2(BaseModel):
    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyFallbackModelsType


StreamRunAgentResponseFormatAgentsRequestRequestBodyType = Literal["text",]


class StreamRunAgentResponseFormatAgents1TypedDict(TypedDict):
    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyType


class StreamRunAgentResponseFormatAgents1(BaseModel):
    type: StreamRunAgentResponseFormatAgentsRequestRequestBodyType


StreamRunAgentFallbackModelsResponseFormatTypedDict = TypeAliasType(
    "StreamRunAgentFallbackModelsResponseFormatTypedDict",
    Union[
        StreamRunAgentResponseFormatAgents1TypedDict,
        StreamRunAgentResponseFormatAgents2TypedDict,
        StreamRunAgentResponseFormatAgents3TypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


StreamRunAgentFallbackModelsResponseFormat = TypeAliasType(
    "StreamRunAgentFallbackModelsResponseFormat",
    Union[
        StreamRunAgentResponseFormatAgents1,
        StreamRunAgentResponseFormatAgents2,
        StreamRunAgentResponseFormatAgents3,
    ],
)
r"""An object specifying the format that the model must output"""


StreamRunAgentFallbackModelsStopTypedDict = TypeAliasType(
    "StreamRunAgentFallbackModelsStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


StreamRunAgentFallbackModelsStop = TypeAliasType(
    "StreamRunAgentFallbackModelsStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class StreamRunAgentFallbackModelsStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class StreamRunAgentFallbackModelsStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


StreamRunAgentFallbackModelsType = Literal[
    "enabled",
    "disabled",
]
r"""Enables or disables the thinking mode capability"""


class StreamRunAgentFallbackModelsThinkingTypedDict(TypedDict):
    type: StreamRunAgentFallbackModelsType
    r"""Enables or disables the thinking mode capability"""
    budget_tokens: float
    r"""Determines how many tokens the model can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than `max_tokens`."""


class StreamRunAgentFallbackModelsThinking(BaseModel):
    type: StreamRunAgentFallbackModelsType
    r"""Enables or disables the thinking mode capability"""

    budget_tokens: float
    r"""Determines how many tokens the model can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than `max_tokens`."""


StreamRunAgentToolChoiceAgentsType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class StreamRunAgentToolChoiceAgentsFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""


class StreamRunAgentToolChoiceAgentsFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""


class StreamRunAgentToolChoiceAgents2TypedDict(TypedDict):
    function: StreamRunAgentToolChoiceAgentsFunctionTypedDict
    type: NotRequired[StreamRunAgentToolChoiceAgentsType]
    r"""The type of the tool. Currently, only function is supported."""


class StreamRunAgentToolChoiceAgents2(BaseModel):
    function: StreamRunAgentToolChoiceAgentsFunction

    type: Optional[StreamRunAgentToolChoiceAgentsType] = None
    r"""The type of the tool. Currently, only function is supported."""


StreamRunAgentToolChoiceAgents1 = Literal[
    "none",
    "auto",
    "required",
]


StreamRunAgentFallbackModelsToolChoiceTypedDict = TypeAliasType(
    "StreamRunAgentFallbackModelsToolChoiceTypedDict",
    Union[StreamRunAgentToolChoiceAgents2TypedDict, StreamRunAgentToolChoiceAgents1],
)
r"""Controls which (if any) tool is called by the model."""


StreamRunAgentFallbackModelsToolChoice = TypeAliasType(
    "StreamRunAgentFallbackModelsToolChoice",
    Union[StreamRunAgentToolChoiceAgents2, StreamRunAgentToolChoiceAgents1],
)
r"""Controls which (if any) tool is called by the model."""


StreamRunAgentFallbackModelsModalities = Literal[
    "text",
    "audio",
]


class StreamRunAgentFallbackModelsWebSearchOptionsTypedDict(TypedDict):
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""

    enabled: NotRequired[bool]
    r"""Whether to enable web search for this request."""


class StreamRunAgentFallbackModelsWebSearchOptions(BaseModel):
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""

    enabled: Optional[bool] = None
    r"""Whether to enable web search for this request."""


class StreamRunAgentFallbackModelsParametersTypedDict(TypedDict):
    audio: NotRequired[Nullable[StreamRunAgentFallbackModelsAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[StreamRunAgentFallbackModelsResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[str]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[StreamRunAgentFallbackModelsStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[
        Nullable[StreamRunAgentFallbackModelsStreamOptionsTypedDict]
    ]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[StreamRunAgentFallbackModelsThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tool_choice: NotRequired[StreamRunAgentFallbackModelsToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[Nullable[List[StreamRunAgentFallbackModelsModalities]]]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    web_search_options: NotRequired[
        StreamRunAgentFallbackModelsWebSearchOptionsTypedDict
    ]
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""


class StreamRunAgentFallbackModelsParameters(BaseModel):
    audio: OptionalNullable[StreamRunAgentFallbackModelsAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[StreamRunAgentFallbackModelsResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[str] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[StreamRunAgentFallbackModelsStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[StreamRunAgentFallbackModelsStreamOptions] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[StreamRunAgentFallbackModelsThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tool_choice: Optional[StreamRunAgentFallbackModelsToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[StreamRunAgentFallbackModelsModalities]] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    web_search_options: Optional[StreamRunAgentFallbackModelsWebSearchOptions] = None
    r"""This tool searches the web for relevant results to use in a response. Learn more about the web search tool."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "audio",
            "frequency_penalty",
            "max_tokens",
            "max_completion_tokens",
            "logprobs",
            "top_logprobs",
            "n",
            "presence_penalty",
            "response_format",
            "reasoning_effort",
            "verbosity",
            "seed",
            "stop",
            "stream_options",
            "thinking",
            "temperature",
            "top_p",
            "top_k",
            "tool_choice",
            "parallel_tool_calls",
            "modalities",
            "web_search_options",
        ]
        nullable_fields = [
            "audio",
            "frequency_penalty",
            "max_tokens",
            "max_completion_tokens",
            "logprobs",
            "top_logprobs",
            "n",
            "presence_penalty",
            "seed",
            "stop",
            "stream_options",
            "temperature",
            "top_p",
            "top_k",
            "modalities",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class StreamRunAgentFallbackModels2TypedDict(TypedDict):
    id: str
    r"""Fallback model ID"""
    integration_id: NotRequired[Nullable[str]]
    parameters: NotRequired[StreamRunAgentFallbackModelsParametersTypedDict]


class StreamRunAgentFallbackModels2(BaseModel):
    id: str
    r"""Fallback model ID"""

    integration_id: OptionalNullable[str] = UNSET

    parameters: Optional[StreamRunAgentFallbackModelsParameters] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["integration_id", "parameters"]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


StreamRunAgentFallbackModelsTypedDict = TypeAliasType(
    "StreamRunAgentFallbackModelsTypedDict",
    Union[StreamRunAgentFallbackModels2TypedDict, str],
)


StreamRunAgentFallbackModels = TypeAliasType(
    "StreamRunAgentFallbackModels", Union[StreamRunAgentFallbackModels2, str]
)


StreamRunAgentRoleToolMessage = Literal["tool",]
r"""Tool message"""


StreamRunAgentRoleUserMessage = Literal["user",]
r"""User message"""


StreamRunAgentRoleTypedDict = TypeAliasType(
    "StreamRunAgentRoleTypedDict",
    Union[StreamRunAgentRoleUserMessage, StreamRunAgentRoleToolMessage],
)
r"""Message role (user or tool for continuing executions)"""


StreamRunAgentRole = TypeAliasType(
    "StreamRunAgentRole",
    Union[StreamRunAgentRoleUserMessage, StreamRunAgentRoleToolMessage],
)
r"""Message role (user or tool for continuing executions)"""


StreamRunAgentPublicMessagePartAgentsRequestKind = Literal["tool_result",]


class StreamRunAgentPublicMessagePartToolResultPartTypedDict(TypedDict):
    r"""Tool execution result part. Use this ONLY when providing results for a pending tool call from the agent. The tool_call_id must match the ID from the agent's tool call request."""

    kind: StreamRunAgentPublicMessagePartAgentsRequestKind
    tool_call_id: str
    result: NotRequired[Any]
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPublicMessagePartToolResultPart(BaseModel):
    r"""Tool execution result part. Use this ONLY when providing results for a pending tool call from the agent. The tool_call_id must match the ID from the agent's tool call request."""

    kind: StreamRunAgentPublicMessagePartAgentsRequestKind

    tool_call_id: str

    result: Optional[Any] = None

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPublicMessagePartAgentsKind = Literal["file",]


class StreamRunAgentFileFileInURIFormatTypedDict(TypedDict):
    r"""File in URI format. Check in the model's documentation for the supported mime types for the URI format"""

    uri: str
    r"""URL for the File content"""
    mime_type: NotRequired[str]
    r"""Optional mimeType for the file"""
    name: NotRequired[str]
    r"""Optional name for the file"""


class StreamRunAgentFileFileInURIFormat(BaseModel):
    r"""File in URI format. Check in the model's documentation for the supported mime types for the URI format"""

    uri: str
    r"""URL for the File content"""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""Optional mimeType for the file"""

    name: Optional[str] = None
    r"""Optional name for the file"""


class StreamRunAgentFileBinaryFormatTypedDict(TypedDict):
    r"""Binary in base64 format. Check in the model's documentation for the supported mime types for the binary format."""

    bytes_: str
    r"""base64 encoded content of the file"""
    mime_type: NotRequired[str]
    r"""Optional mimeType for the file"""
    name: NotRequired[str]
    r"""Optional name for the file"""


class StreamRunAgentFileBinaryFormat(BaseModel):
    r"""Binary in base64 format. Check in the model's documentation for the supported mime types for the binary format."""

    bytes_: Annotated[str, pydantic.Field(alias="bytes")]
    r"""base64 encoded content of the file"""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""Optional mimeType for the file"""

    name: Optional[str] = None
    r"""Optional name for the file"""


StreamRunAgentPublicMessagePartFileTypedDict = TypeAliasType(
    "StreamRunAgentPublicMessagePartFileTypedDict",
    Union[
        StreamRunAgentFileBinaryFormatTypedDict,
        StreamRunAgentFileFileInURIFormatTypedDict,
    ],
)


StreamRunAgentPublicMessagePartFile = TypeAliasType(
    "StreamRunAgentPublicMessagePartFile",
    Union[StreamRunAgentFileBinaryFormat, StreamRunAgentFileFileInURIFormat],
)


class StreamRunAgentPublicMessagePartFilePartTypedDict(TypedDict):
    r"""File attachment part. Use this to send files (images, documents, etc.) to the agent for processing."""

    kind: StreamRunAgentPublicMessagePartAgentsKind
    file: StreamRunAgentPublicMessagePartFileTypedDict
    metadata: NotRequired[Dict[str, Any]]


class StreamRunAgentPublicMessagePartFilePart(BaseModel):
    r"""File attachment part. Use this to send files (images, documents, etc.) to the agent for processing."""

    kind: StreamRunAgentPublicMessagePartAgentsKind

    file: StreamRunAgentPublicMessagePartFile

    metadata: Optional[Dict[str, Any]] = None


StreamRunAgentPublicMessagePartKind = Literal["text",]


class StreamRunAgentPublicMessagePartTextPartTypedDict(TypedDict):
    r"""Text content part. Use this to send text messages to the agent."""

    kind: StreamRunAgentPublicMessagePartKind
    text: str


class StreamRunAgentPublicMessagePartTextPart(BaseModel):
    r"""Text content part. Use this to send text messages to the agent."""

    kind: StreamRunAgentPublicMessagePartKind

    text: str


StreamRunAgentPublicMessagePartTypedDict = TypeAliasType(
    "StreamRunAgentPublicMessagePartTypedDict",
    Union[
        StreamRunAgentPublicMessagePartTextPartTypedDict,
        StreamRunAgentPublicMessagePartFilePartTypedDict,
        StreamRunAgentPublicMessagePartToolResultPartTypedDict,
    ],
)
r"""Message part that can be provided by users. Use \"text\" for regular messages, \"file\" for attachments, or \"tool_result\" when responding to tool call requests."""


StreamRunAgentPublicMessagePart = TypeAliasType(
    "StreamRunAgentPublicMessagePart",
    Union[
        StreamRunAgentPublicMessagePartTextPart,
        StreamRunAgentPublicMessagePartFilePart,
        StreamRunAgentPublicMessagePartToolResultPart,
    ],
)
r"""Message part that can be provided by users. Use \"text\" for regular messages, \"file\" for attachments, or \"tool_result\" when responding to tool call requests."""


class StreamRunAgentMessageTypedDict(TypedDict):
    r"""The A2A format message containing the task for the agent to perform."""

    role: StreamRunAgentRoleTypedDict
    r"""Message role (user or tool for continuing executions)"""
    parts: List[StreamRunAgentPublicMessagePartTypedDict]
    r"""A2A message parts (text, file, or tool_result only)"""
    message_id: NotRequired[str]
    r"""Optional A2A message ID in ULID format"""


class StreamRunAgentMessage(BaseModel):
    r"""The A2A format message containing the task for the agent to perform."""

    role: StreamRunAgentRole
    r"""Message role (user or tool for continuing executions)"""

    parts: List[StreamRunAgentPublicMessagePart]
    r"""A2A message parts (text, file, or tool_result only)"""

    message_id: Annotated[Optional[str], pydantic.Field(alias="messageId")] = None
    r"""Optional A2A message ID in ULID format"""


class StreamRunAgentContactTypedDict(TypedDict):
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""

    id: str
    r"""Unique identifier for the contact"""
    display_name: NotRequired[str]
    r"""Display name of the contact"""
    email: NotRequired[str]
    r"""Email address of the contact"""
    metadata: NotRequired[List[Dict[str, Any]]]
    r"""A hash of key/value pairs containing any other data about the contact"""
    logo_url: NotRequired[str]
    r"""URL to the contact's avatar or logo"""
    tags: NotRequired[List[str]]
    r"""A list of tags associated with the contact"""


class StreamRunAgentContact(BaseModel):
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""

    id: str
    r"""Unique identifier for the contact"""

    display_name: Optional[str] = None
    r"""Display name of the contact"""

    email: Optional[str] = None
    r"""Email address of the contact"""

    metadata: Optional[List[Dict[str, Any]]] = None
    r"""A hash of key/value pairs containing any other data about the contact"""

    logo_url: Optional[str] = None
    r"""URL to the contact's avatar or logo"""

    tags: Optional[List[str]] = None
    r"""A list of tags associated with the contact"""


class StreamRunAgentThreadTypedDict(TypedDict):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""
    tags: NotRequired[List[str]]
    r"""Optional tags to differentiate or categorize threads"""


class StreamRunAgentThread(BaseModel):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""

    tags: Optional[List[str]] = None
    r"""Optional tags to differentiate or categorize threads"""


class StreamRunAgentMemoryTypedDict(TypedDict):
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""

    entity_id: str
    r"""An entity ID used to link memory stores to a specific user, session, or conversation. This ID is used to isolate and retrieve memories specific to the entity across agent executions."""


class StreamRunAgentMemory(BaseModel):
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""

    entity_id: str
    r"""An entity ID used to link memory stores to a specific user, session, or conversation. This ID is used to isolate and retrieve memories specific to the entity across agent executions."""


class StreamRunAgentKnowledgeBasesTypedDict(TypedDict):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


class StreamRunAgentKnowledgeBases(BaseModel):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""


class StreamRunAgentTeamOfAgentsTypedDict(TypedDict):
    key: str
    r"""The unique key of the agent within the workspace"""
    role: NotRequired[str]
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""


class StreamRunAgentTeamOfAgents(BaseModel):
    key: str
    r"""The unique key of the agent within the workspace"""

    role: Optional[str] = None
    r"""The role of the agent in this context. This is used to give extra information to the leader to help it decide which agent to hand off to."""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14Type = Literal[
    "function",
]


class StreamRunAgentAgentToolInputRunFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the function does, used by the model to choose when and how to call the function."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Currently only compatible with `OpenAI` models."""
    parameters: NotRequired[Dict[str, Any]]
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""


class StreamRunAgentAgentToolInputRunFunction(BaseModel):
    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the function does, used by the model to choose when and how to call the function."""

    strict: Optional[bool] = None
    r"""Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Currently only compatible with `OpenAI` models."""

    parameters: Optional[Dict[str, Any]] = None
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""


class AgentToolInputRunFunctionToolRunTypedDict(TypedDict):
    r"""Function tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    function: StreamRunAgentAgentToolInputRunFunctionTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    description: NotRequired[str]
    requires_approval: NotRequired[bool]


class AgentToolInputRunFunctionToolRun(BaseModel):
    r"""Function tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools14Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    function: StreamRunAgentAgentToolInputRunFunction

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    description: Optional[str] = None

    requires_approval: Optional[bool] = False


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13Type = Literal[
    "code",
]


AgentToolInputRunLanguage = Literal["python",]


class AgentToolInputRunCodeToolTypedDict(TypedDict):
    language: AgentToolInputRunLanguage
    code: str
    r"""The code to execute."""
    parameters: NotRequired[Dict[str, Any]]
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""


class AgentToolInputRunCodeTool(BaseModel):
    language: AgentToolInputRunLanguage

    code: str
    r"""The code to execute."""

    parameters: Optional[Dict[str, Any]] = None
    r"""The parameters the functions accepts, described as a JSON Schema object. See the `OpenAI` [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format."""


class AgentToolInputRunCodeToolRunTypedDict(TypedDict):
    r"""Code execution tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""
    code_tool: AgentToolInputRunCodeToolTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    requires_approval: NotRequired[bool]


class AgentToolInputRunCodeToolRun(BaseModel):
    r"""Code execution tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools13Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""

    code_tool: AgentToolInputRunCodeTool

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    requires_approval: Optional[bool] = False


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12Type = Literal[
    "http",
]


AgentToolInputRunMethod = Literal[
    "GET",
    "POST",
    "PUT",
    "DELETE",
]
r"""The HTTP method to use."""


class AgentToolInputRunBlueprintTypedDict(TypedDict):
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""

    url: str
    r"""The URL to send the request to."""
    method: AgentToolInputRunMethod
    r"""The HTTP method to use."""
    headers: NotRequired[Dict[str, str]]
    r"""The headers to send with the request."""
    body: NotRequired[Dict[str, Any]]
    r"""The body to send with the request."""


class AgentToolInputRunBlueprint(BaseModel):
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""

    url: str
    r"""The URL to send the request to."""

    method: AgentToolInputRunMethod
    r"""The HTTP method to use."""

    headers: Optional[Dict[str, str]] = None
    r"""The headers to send with the request."""

    body: Optional[Dict[str, Any]] = None
    r"""The body to send with the request."""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12HTTPType = (
    Literal[
        "string",
        "number",
        "boolean",
    ]
)
r"""The type of the argument."""


AgentToolInputRunDefaultValueTypedDict = TypeAliasType(
    "AgentToolInputRunDefaultValueTypedDict", Union[str, float, bool]
)
r"""The default value of the argument."""


AgentToolInputRunDefaultValue = TypeAliasType(
    "AgentToolInputRunDefaultValue", Union[str, float, bool]
)
r"""The default value of the argument."""


class AgentToolInputRunArgumentsTypedDict(TypedDict):
    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12HTTPType
    r"""The type of the argument."""
    description: str
    r"""A description of the argument."""
    send_to_model: NotRequired[bool]
    r"""Whether to send the argument to the model. If set to false, the argument will not be sent to the model and needs to be provided by the user or it will be left blank."""
    default_value: NotRequired[AgentToolInputRunDefaultValueTypedDict]
    r"""The default value of the argument."""


class AgentToolInputRunArguments(BaseModel):
    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12HTTPType
    r"""The type of the argument."""

    description: str
    r"""A description of the argument."""

    send_to_model: Optional[bool] = True
    r"""Whether to send the argument to the model. If set to false, the argument will not be sent to the model and needs to be provided by the user or it will be left blank."""

    default_value: Optional[AgentToolInputRunDefaultValue] = None
    r"""The default value of the argument."""


class AgentToolInputRunHTTPTypedDict(TypedDict):
    blueprint: AgentToolInputRunBlueprintTypedDict
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""
    arguments: NotRequired[Dict[str, AgentToolInputRunArgumentsTypedDict]]
    r"""The arguments to send with the request. The keys will be used to replace the placeholders in the `blueprint` field."""


class AgentToolInputRunHTTP(BaseModel):
    blueprint: AgentToolInputRunBlueprint
    r"""The blueprint for the HTTP request. The `arguments` field will be used to replace the placeholders in the `url`, `headers`, `body`, and `arguments` fields."""

    arguments: Optional[Dict[str, AgentToolInputRunArguments]] = None
    r"""The arguments to send with the request. The keys will be used to replace the placeholders in the `blueprint` field."""


class AgentToolInputRunHTTPToolRunTypedDict(TypedDict):
    r"""HTTP tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12Type
    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""
    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""
    http: AgentToolInputRunHTTPTypedDict
    id: NotRequired[str]
    display_name: NotRequired[str]
    requires_approval: NotRequired[bool]


class AgentToolInputRunHTTPToolRun(BaseModel):
    r"""HTTP tool with inline definition for on-the-fly creation in run endpoint"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools12Type

    key: str
    r"""Unique key of the tool as it will be displayed in the UI"""

    description: str
    r"""A description of the tool, used by the model to choose when and how to call the tool. We do recommend using the `description` field as accurate as possible to give enough context to the model to make the right decision."""

    http: AgentToolInputRunHTTP

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None

    display_name: Optional[str] = None

    requires_approval: Optional[bool] = False


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools11Type = Literal[
    "current_date",
]


class StreamRunAgentAgentToolInputRunCurrentDateToolTypedDict(TypedDict):
    r"""Returns the current date and time"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools11Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunCurrentDateTool(BaseModel):
    r"""Returns the current date and time"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools11Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools10Type = Literal[
    "query_knowledge_base",
]


class StreamRunAgentAgentToolInputRunQueryKnowledgeBaseToolTypedDict(TypedDict):
    r"""Queries knowledge bases for information"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools10Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunQueryKnowledgeBaseTool(BaseModel):
    r"""Queries knowledge bases for information"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools10Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools9Type = Literal[
    "retrieve_knowledge_bases",
]


class StreamRunAgentAgentToolInputRunRetrieveKnowledgeBasesToolTypedDict(TypedDict):
    r"""Lists available knowledge bases"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools9Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunRetrieveKnowledgeBasesTool(BaseModel):
    r"""Lists available knowledge bases"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools9Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools8Type = Literal[
    "delete_memory_document",
]


class StreamRunAgentAgentToolInputRunDeleteMemoryDocumentToolTypedDict(TypedDict):
    r"""Deletes documents from memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools8Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunDeleteMemoryDocumentTool(BaseModel):
    r"""Deletes documents from memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools8Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools7Type = Literal[
    "retrieve_memory_stores",
]


class StreamRunAgentAgentToolInputRunRetrieveMemoryStoresToolTypedDict(TypedDict):
    r"""Lists available memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools7Type
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunRetrieveMemoryStoresTool(BaseModel):
    r"""Lists available memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsTools7Type

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsToolsType = Literal[
    "write_memory_store",
]


class StreamRunAgentAgentToolInputRunWriteMemoryStoreToolTypedDict(TypedDict):
    r"""Writes information to agent memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsToolsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunWriteMemoryStoreTool(BaseModel):
    r"""Writes information to agent memory stores"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsToolsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsType = Literal[
    "query_memory_store",
]


class StreamRunAgentAgentToolInputRunQueryMemoryStoreToolTypedDict(TypedDict):
    r"""Queries agent memory stores for context"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunQueryMemoryStoreTool(BaseModel):
    r"""Queries agent memory stores for context"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodySettingsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestRequestBodyType = Literal[
    "retrieve_agents",
]


class StreamRunAgentAgentToolInputRunRetrieveAgentsToolTypedDict(TypedDict):
    r"""Retrieves available agents in the system"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodyType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunRetrieveAgentsTool(BaseModel):
    r"""Retrieves available agents in the system"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestRequestBodyType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsRequestType = Literal["call_sub_agent",]


class StreamRunAgentAgentToolInputRunCallSubAgentToolTypedDict(TypedDict):
    r"""Delegates tasks to specialized sub-agents"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunCallSubAgentTool(BaseModel):
    r"""Delegates tasks to specialized sub-agents"""

    type: StreamRunAgentAgentToolInputRunAgentsRequestType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunAgentsType = Literal["web_scraper",]


class StreamRunAgentAgentToolInputRunWebScraperToolTypedDict(TypedDict):
    r"""Scrapes and extracts content from web pages"""

    type: StreamRunAgentAgentToolInputRunAgentsType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunWebScraperTool(BaseModel):
    r"""Scrapes and extracts content from web pages"""

    type: StreamRunAgentAgentToolInputRunAgentsType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunType = Literal["google_search",]


class StreamRunAgentAgentToolInputRunGoogleSearchToolTypedDict(TypedDict):
    r"""Performs Google searches to retrieve web content"""

    type: StreamRunAgentAgentToolInputRunType
    requires_approval: NotRequired[bool]
    r"""Whether this tool requires approval before execution"""


class StreamRunAgentAgentToolInputRunGoogleSearchTool(BaseModel):
    r"""Performs Google searches to retrieve web content"""

    type: StreamRunAgentAgentToolInputRunType

    requires_approval: Optional[bool] = None
    r"""Whether this tool requires approval before execution"""


StreamRunAgentAgentToolInputRunTypedDict = TypeAliasType(
    "StreamRunAgentAgentToolInputRunTypedDict",
    Union[
        StreamRunAgentAgentToolInputRunGoogleSearchToolTypedDict,
        StreamRunAgentAgentToolInputRunWebScraperToolTypedDict,
        StreamRunAgentAgentToolInputRunCallSubAgentToolTypedDict,
        StreamRunAgentAgentToolInputRunRetrieveAgentsToolTypedDict,
        StreamRunAgentAgentToolInputRunQueryMemoryStoreToolTypedDict,
        StreamRunAgentAgentToolInputRunWriteMemoryStoreToolTypedDict,
        StreamRunAgentAgentToolInputRunRetrieveMemoryStoresToolTypedDict,
        StreamRunAgentAgentToolInputRunDeleteMemoryDocumentToolTypedDict,
        StreamRunAgentAgentToolInputRunRetrieveKnowledgeBasesToolTypedDict,
        StreamRunAgentAgentToolInputRunQueryKnowledgeBaseToolTypedDict,
        StreamRunAgentAgentToolInputRunCurrentDateToolTypedDict,
        AgentToolInputRunHTTPToolRunTypedDict,
        AgentToolInputRunCodeToolRunTypedDict,
        AgentToolInputRunFunctionToolRunTypedDict,
    ],
)
r"""Tool configuration for agent run operations. Built-in tools only require a type and requires_approval, while custom tools (HTTP, Code, Function) support full inline definitions for on-the-fly creation."""


StreamRunAgentAgentToolInputRun = TypeAliasType(
    "StreamRunAgentAgentToolInputRun",
    Union[
        StreamRunAgentAgentToolInputRunGoogleSearchTool,
        StreamRunAgentAgentToolInputRunWebScraperTool,
        StreamRunAgentAgentToolInputRunCallSubAgentTool,
        StreamRunAgentAgentToolInputRunRetrieveAgentsTool,
        StreamRunAgentAgentToolInputRunQueryMemoryStoreTool,
        StreamRunAgentAgentToolInputRunWriteMemoryStoreTool,
        StreamRunAgentAgentToolInputRunRetrieveMemoryStoresTool,
        StreamRunAgentAgentToolInputRunDeleteMemoryDocumentTool,
        StreamRunAgentAgentToolInputRunRetrieveKnowledgeBasesTool,
        StreamRunAgentAgentToolInputRunQueryKnowledgeBaseTool,
        StreamRunAgentAgentToolInputRunCurrentDateTool,
        AgentToolInputRunHTTPToolRun,
        AgentToolInputRunCodeToolRun,
        AgentToolInputRunFunctionToolRun,
    ],
)
r"""Tool configuration for agent run operations. Built-in tools only require a type and requires_approval, while custom tools (HTTP, Code, Function) support full inline definitions for on-the-fly creation."""


StreamRunAgentToolApprovalRequired = Literal[
    "all",
    "respect_tool",
    "none",
]
r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""


class StreamRunAgentSettingsTypedDict(TypedDict):
    tools: NotRequired[List[StreamRunAgentAgentToolInputRunTypedDict]]
    r"""Tools available to the agent"""
    tool_approval_required: NotRequired[StreamRunAgentToolApprovalRequired]
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""
    max_iterations: NotRequired[int]
    r"""Maximum iterations(llm calls) before the agent will stop executing."""
    max_execution_time: NotRequired[int]
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""


class StreamRunAgentSettings(BaseModel):
    tools: Optional[List[StreamRunAgentAgentToolInputRun]] = None
    r"""Tools available to the agent"""

    tool_approval_required: Optional[StreamRunAgentToolApprovalRequired] = "none"
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""

    max_iterations: Optional[int] = 15
    r"""Maximum iterations(llm calls) before the agent will stop executing."""

    max_execution_time: Optional[int] = 300
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""


class StreamRunAgentRequestBodyTypedDict(TypedDict):
    key: str
    r"""A unique identifier for the agent. This key must be unique within the same workspace and cannot be reused. When executing the agent, this key determines if the agent already exists. If the agent version differs, a new version is created at the end of the execution, except for the task. All agent parameters are evaluated to decide if a new version is needed."""
    model: StreamRunAgentModelTypedDict
    r"""The language model that powers the agent. Can be a simple string (e.g., \"openai/gpt-4o\") or an object with model ID and parameters. The model must support tool calling capabilities."""
    role: str
    r"""Specifies the agent's function and area of expertise."""
    instructions: str
    r"""Provides context and purpose for the agent. Combined with the system prompt template to generate the agent's instructions."""
    message: StreamRunAgentMessageTypedDict
    r"""The A2A format message containing the task for the agent to perform."""
    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """
    settings: StreamRunAgentSettingsTypedDict
    task_id: NotRequired[str]
    r"""Optional task ID to continue an existing agent execution. When provided, the agent will continue the conversation from the existing task state. The task must be in an inactive state to continue."""
    fallback_models: NotRequired[List[StreamRunAgentFallbackModelsTypedDict]]
    r"""Optional array of fallback models (string IDs or config objects) to use when the primary model fails. Models are tried in order. All models must support tool calling capabilities."""
    variables: NotRequired[Dict[str, Any]]
    r"""Optional variables for template replacement in system prompt, instructions, and messages"""
    contact: NotRequired[StreamRunAgentContactTypedDict]
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""
    thread: NotRequired[StreamRunAgentThreadTypedDict]
    r"""Thread information to group related requests"""
    memory: NotRequired[StreamRunAgentMemoryTypedDict]
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""
    description: NotRequired[str]
    r"""A brief summary of the agent's purpose."""
    system_prompt: NotRequired[str]
    r"""A custom system prompt template for the agent. If omitted, the default template is used."""
    memory_stores: NotRequired[List[str]]
    r"""The list of keys of the memory stores that are accessible to the agent."""
    knowledge_bases: NotRequired[List[StreamRunAgentKnowledgeBasesTypedDict]]
    r"""Knowledge base configurations for the agent to access"""
    team_of_agents: NotRequired[List[StreamRunAgentTeamOfAgentsTypedDict]]
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""
    metadata: NotRequired[Dict[str, Any]]
    r"""Optional metadata for the agent run as key-value pairs that will be included in traces"""
    stream_timeout_seconds: NotRequired[float]
    r"""Stream timeout in seconds (1-3600). Default: 1800 (30 minutes)"""


class StreamRunAgentRequestBody(BaseModel):
    key: str
    r"""A unique identifier for the agent. This key must be unique within the same workspace and cannot be reused. When executing the agent, this key determines if the agent already exists. If the agent version differs, a new version is created at the end of the execution, except for the task. All agent parameters are evaluated to decide if a new version is needed."""

    model: StreamRunAgentModel
    r"""The language model that powers the agent. Can be a simple string (e.g., \"openai/gpt-4o\") or an object with model ID and parameters. The model must support tool calling capabilities."""

    role: str
    r"""Specifies the agent's function and area of expertise."""

    instructions: str
    r"""Provides context and purpose for the agent. Combined with the system prompt template to generate the agent's instructions."""

    message: StreamRunAgentMessage
    r"""The A2A format message containing the task for the agent to perform."""

    path: str
    r"""Entity storage path in the format: `project/folder/subfolder/...`

    The first element identifies the project, followed by nested folders (auto-created as needed).

    With project-based API keys, the first element is treated as a folder name, as the project is predetermined by the API key.
    """

    settings: StreamRunAgentSettings

    task_id: Optional[str] = None
    r"""Optional task ID to continue an existing agent execution. When provided, the agent will continue the conversation from the existing task state. The task must be in an inactive state to continue."""

    fallback_models: Optional[List[StreamRunAgentFallbackModels]] = None
    r"""Optional array of fallback models (string IDs or config objects) to use when the primary model fails. Models are tried in order. All models must support tool calling capabilities."""

    variables: Optional[Dict[str, Any]] = None
    r"""Optional variables for template replacement in system prompt, instructions, and messages"""

    contact: Optional[StreamRunAgentContact] = None
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""

    thread: Optional[StreamRunAgentThread] = None
    r"""Thread information to group related requests"""

    memory: Optional[StreamRunAgentMemory] = None
    r"""Memory configuration for the agent execution. Used to associate memory stores with specific entities like users or sessions."""

    description: Optional[str] = None
    r"""A brief summary of the agent's purpose."""

    system_prompt: Optional[str] = None
    r"""A custom system prompt template for the agent. If omitted, the default template is used."""

    memory_stores: Optional[List[str]] = None
    r"""The list of keys of the memory stores that are accessible to the agent."""

    knowledge_bases: Optional[List[StreamRunAgentKnowledgeBases]] = None
    r"""Knowledge base configurations for the agent to access"""

    team_of_agents: Optional[List[StreamRunAgentTeamOfAgents]] = None
    r"""The agents that are accessible to this orchestrator. The main agent can hand off to these agents to perform tasks."""

    metadata: Optional[Dict[str, Any]] = None
    r"""Optional metadata for the agent run as key-value pairs that will be included in traces"""

    stream_timeout_seconds: Optional[float] = None
    r"""Stream timeout in seconds (1-3600). Default: 1800 (30 minutes)"""


class StreamRunAgentAgentsResponseBodyData(BaseModel):
    message: str


@dataclass(unsafe_hash=True)
class StreamRunAgentAgentsResponseBody(OrqError):
    r"""Model not found"""

    data: StreamRunAgentAgentsResponseBodyData = field(hash=False)

    def __init__(
        self,
        data: StreamRunAgentAgentsResponseBodyData,
        raw_response: httpx.Response,
        body: Optional[str] = None,
    ):
        fallback = body or raw_response.text
        message = str(data.message) or fallback
        super().__init__(message, raw_response, body)
        object.__setattr__(self, "data", data)


class StreamRunAgentResponseBodyTypedDict(TypedDict):
    r"""SSE stream of agent events"""

    data: str
    r"""JSON-encoded event data"""


class StreamRunAgentResponseBody(BaseModel):
    r"""SSE stream of agent events"""

    data: str
    r"""JSON-encoded event data"""
