"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import FieldMetadata, QueryParamMetadata, get_discriminator
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class GetAllPromptsRequestTypedDict(TypedDict):
    limit: NotRequired[float]
    r"""A limit on the number of objects to be returned. Limit can range between 1 and 50, and the default is 10"""
    starting_after: NotRequired[str]
    r"""A cursor for use in pagination. `starting_after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 20 objects, ending with `01JJ1HDHN79XAS7A01WB3HYSDB`, your subsequent call can include `after=01JJ1HDHN79XAS7A01WB3HYSDB` in order to fetch the next page of the list."""
    ending_before: NotRequired[str]
    r"""A cursor for use in pagination. `ending_before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 20 objects, starting with `01JJ1HDHN79XAS7A01WB3HYSDB`, your subsequent call can include `before=01JJ1HDHN79XAS7A01WB3HYSDB` in order to fetch the previous page of the list."""


class GetAllPromptsRequest(BaseModel):
    limit: Annotated[
        Optional[float],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 10
    r"""A limit on the number of objects to be returned. Limit can range between 1 and 50, and the default is 10"""

    starting_after: Annotated[
        Optional[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = None
    r"""A cursor for use in pagination. `starting_after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 20 objects, ending with `01JJ1HDHN79XAS7A01WB3HYSDB`, your subsequent call can include `after=01JJ1HDHN79XAS7A01WB3HYSDB` in order to fetch the next page of the list."""

    ending_before: Annotated[
        Optional[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = None
    r"""A cursor for use in pagination. `ending_before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 20 objects, starting with `01JJ1HDHN79XAS7A01WB3HYSDB`, your subsequent call can include `before=01JJ1HDHN79XAS7A01WB3HYSDB` in order to fetch the previous page of the list."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["limit", "starting_after", "ending_before"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


GetAllPromptsObject = Literal["list",]


GetAllPromptsType = Literal["prompt",]


GetAllPromptsModelType = Literal[
    "chat",
    "completion",
    "embedding",
    "image",
    "tts",
    "stt",
    "rerank",
    "moderation",
    "vision",
]
r"""The modality of the model"""


GetAllPromptsFormat = Literal[
    "url",
    "b64_json",
    "text",
    "json_object",
]
r"""Only supported on `image` models."""


GetAllPromptsResponseFormat6 = Literal[
    "json",
    "text",
    "srt",
    "verbose_json",
    "vtt",
]


GetAllPromptsResponseFormat5 = Literal[
    "url",
    "base64_json",
]


GetAllPromptsResponseFormat4 = Literal[
    "mp3",
    "opus",
    "aac",
    "flac",
    "wav",
    "pcm",
]


GetAllPromptsResponseFormatPromptsResponseType = Literal["text",]


class GetAllPromptsResponseFormat3TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsResponseType


class GetAllPromptsResponseFormat3(BaseModel):
    type: GetAllPromptsResponseFormatPromptsResponseType


GetAllPromptsResponseFormatPromptsType = Literal["json_object",]


class GetAllPromptsResponseFormat2TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatPromptsType


class GetAllPromptsResponseFormat2(BaseModel):
    type: GetAllPromptsResponseFormatPromptsType


GetAllPromptsResponseFormatType = Literal["json_schema",]


class GetAllPromptsResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    schema_: Dict[str, Any]
    description: NotRequired[str]
    strict: NotRequired[bool]


class GetAllPromptsResponseFormatJSONSchema(BaseModel):
    name: str

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]

    description: Optional[str] = None

    strict: Optional[bool] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class GetAllPromptsResponseFormat1TypedDict(TypedDict):
    type: GetAllPromptsResponseFormatType
    json_schema: GetAllPromptsResponseFormatJSONSchemaTypedDict
    display_name: NotRequired[str]


class GetAllPromptsResponseFormat1(BaseModel):
    type: GetAllPromptsResponseFormatType

    json_schema: GetAllPromptsResponseFormatJSONSchema

    display_name: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["display_name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


GetAllPromptsResponseFormatTypedDict = TypeAliasType(
    "GetAllPromptsResponseFormatTypedDict",
    Union[
        GetAllPromptsResponseFormat2TypedDict,
        GetAllPromptsResponseFormat3TypedDict,
        GetAllPromptsResponseFormat1TypedDict,
        GetAllPromptsResponseFormat4,
        GetAllPromptsResponseFormat5,
        GetAllPromptsResponseFormat6,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsResponseFormat = TypeAliasType(
    "GetAllPromptsResponseFormat",
    Union[
        GetAllPromptsResponseFormat2,
        GetAllPromptsResponseFormat3,
        GetAllPromptsResponseFormat1,
        GetAllPromptsResponseFormat4,
        GetAllPromptsResponseFormat5,
        GetAllPromptsResponseFormat6,
    ],
)
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptsPhotoRealVersion = Literal[
    "v1",
    "v2",
]
r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""


GetAllPromptsEncodingFormat = Literal[
    "float",
    "base64",
]
r"""The format to return the embeddings"""


GetAllPromptsReasoningEffort = Literal[
    "none",
    "disable",
    "minimal",
    "low",
    "medium",
    "high",
]
r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""


GetAllPromptsVerbosity = Literal[
    "low",
    "medium",
    "high",
]
r"""Controls the verbosity of the model output."""


GetAllPromptsThinkingLevel = Literal[
    "low",
    "high",
]
r"""The level of thinking to use for the model. Only supported by `Google AI`"""


class GetAllPromptsModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format_: NotRequired[GetAllPromptsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[str]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[Nullable[GetAllPromptsResponseFormatTypedDict]]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[GetAllPromptsPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[GetAllPromptsEncodingFormat]
    r"""The format to return the embeddings"""
    reasoning_effort: NotRequired[GetAllPromptsReasoningEffort]
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""
    budget_tokens: NotRequired[float]
    r"""Gives the model enhanced reasoning capabilities for complex tasks. A value of 0 disables thinking. The minimum budget tokens for thinking are 1024. The Budget Tokens should never exceed the Max Tokens parameter. Only supported by `Anthropic`"""
    verbosity: NotRequired[GetAllPromptsVerbosity]
    r"""Controls the verbosity of the model output."""
    thinking_level: NotRequired[GetAllPromptsThinkingLevel]
    r"""The level of thinking to use for the model. Only supported by `Google AI`"""


class GetAllPromptsModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format_: Annotated[
        Optional[GetAllPromptsFormat], pydantic.Field(alias="format")
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[str] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[GetAllPromptsResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptsPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[GetAllPromptsEncodingFormat] = None
    r"""The format to return the embeddings"""

    reasoning_effort: Annotated[
        Optional[GetAllPromptsReasoningEffort], pydantic.Field(alias="reasoningEffort")
    ] = None
    r"""Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."""

    budget_tokens: Annotated[Optional[float], pydantic.Field(alias="budgetTokens")] = (
        None
    )
    r"""Gives the model enhanced reasoning capabilities for complex tasks. A value of 0 disables thinking. The minimum budget tokens for thinking are 1024. The Budget Tokens should never exceed the Max Tokens parameter. Only supported by `Anthropic`"""

    verbosity: Optional[GetAllPromptsVerbosity] = None
    r"""Controls the verbosity of the model output."""

    thinking_level: Annotated[
        Optional[GetAllPromptsThinkingLevel], pydantic.Field(alias="thinkingLevel")
    ] = None
    r"""The level of thinking to use for the model. Only supported by `Google AI`"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "temperature",
                "maxTokens",
                "topK",
                "topP",
                "frequencyPenalty",
                "presencePenalty",
                "numImages",
                "seed",
                "format",
                "dimensions",
                "quality",
                "style",
                "responseFormat",
                "photoRealVersion",
                "encoding_format",
                "reasoningEffort",
                "budgetTokens",
                "verbosity",
                "thinkingLevel",
            ]
        )
        nullable_fields = set(["responseFormat"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


GetAllPromptsProvider = Literal[
    "openai",
    "groq",
    "cohere",
    "azure",
    "aws",
    "google",
    "google-ai",
    "huggingface",
    "togetherai",
    "perplexity",
    "anthropic",
    "leonardoai",
    "fal",
    "nvidia",
    "jina",
    "elevenlabs",
    "litellm",
    "cerebras",
    "openailike",
    "bytedance",
    "mistral",
    "deepseek",
    "contextualai",
    "moonshotai",
    "zai",
    "slack",
]


GetAllPromptsRole = Literal[
    "system",
    "assistant",
    "user",
    "exception",
    "tool",
    "prompt",
    "correction",
    "expected_output",
]
r"""The role of the prompt message"""


GetAllPrompts2PromptsResponseType = Literal["file",]
r"""The type of the content part. Always `file`."""


class GetAllPrompts2FileTypedDict(TypedDict):
    file_data: NotRequired[str]
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""
    uri: NotRequired[str]
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""
    mime_type: NotRequired[str]
    r"""MIME type of the file (e.g., application/pdf, image/png)"""
    filename: NotRequired[str]
    r"""The name of the file, used when passing the file to the model as a string."""


class GetAllPrompts2File(BaseModel):
    file_data: Optional[str] = None
    r"""The file data as a data URI string in the format 'data:<mime-type>;base64,<base64-encoded-data>'. Example: 'data:image/png;base64,iVBORw0KGgoAAAANS...'"""

    uri: Optional[str] = None
    r"""URL to the file. Only supported by Anthropic Claude models for PDF files."""

    mime_type: Annotated[Optional[str], pydantic.Field(alias="mimeType")] = None
    r"""MIME type of the file (e.g., application/pdf, image/png)"""

    filename: Optional[str] = None
    r"""The name of the file, used when passing the file to the model as a string."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["file_data", "uri", "mimeType", "filename"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class GetAllPrompts23TypedDict(TypedDict):
    type: GetAllPrompts2PromptsResponseType
    r"""The type of the content part. Always `file`."""
    file: GetAllPrompts2FileTypedDict


class GetAllPrompts23(BaseModel):
    type: GetAllPrompts2PromptsResponseType
    r"""The type of the content part. Always `file`."""

    file: GetAllPrompts2File


GetAllPrompts2PromptsType = Literal["image_url",]


class GetAllPrompts2ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPrompts2ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["id", "detail"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class GetAllPrompts22TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsType
    image_url: GetAllPrompts2ImageURLTypedDict


class GetAllPrompts22(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPrompts2PromptsType

    image_url: GetAllPrompts2ImageURL


GetAllPrompts2Type = Literal["text",]


class GetAllPrompts21TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2Type
    text: str


class GetAllPrompts21(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPrompts2Type

    text: str


GetAllPromptsContent2TypedDict = TypeAliasType(
    "GetAllPromptsContent2TypedDict",
    Union[GetAllPrompts21TypedDict, GetAllPrompts22TypedDict, GetAllPrompts23TypedDict],
)


GetAllPromptsContent2 = Annotated[
    Union[
        Annotated[GetAllPrompts21, Tag("text")],
        Annotated[GetAllPrompts22, Tag("image_url")],
        Annotated[GetAllPrompts23, Tag("file")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


GetAllPromptsContentTypedDict = TypeAliasType(
    "GetAllPromptsContentTypedDict", Union[str, List[GetAllPromptsContent2TypedDict]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""


GetAllPromptsContent = TypeAliasType(
    "GetAllPromptsContent", Union[str, List[GetAllPromptsContent2]]
)
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""


GetAllPromptsPromptsType = Literal["function",]


class GetAllPromptsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptsToolCallsTypedDict(TypedDict):
    type: GetAllPromptsPromptsType
    function: GetAllPromptsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptsToolCalls(BaseModel):
    type: GetAllPromptsPromptsType

    function: GetAllPromptsFunction

    id: Optional[str] = None

    index: Optional[float] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["id", "index"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class GetAllPromptsMessagesTypedDict(TypedDict):
    role: GetAllPromptsRole
    r"""The role of the prompt message"""
    content: Nullable[GetAllPromptsContentTypedDict]
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""
    tool_calls: NotRequired[List[GetAllPromptsToolCallsTypedDict]]
    tool_call_id: NotRequired[Nullable[str]]


class GetAllPromptsMessages(BaseModel):
    role: GetAllPromptsRole
    r"""The role of the prompt message"""

    content: Nullable[GetAllPromptsContent]
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Can be null for tool messages in certain scenarios."""

    tool_calls: Optional[List[GetAllPromptsToolCalls]] = None

    tool_call_id: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["tool_calls", "tool_call_id"])
        nullable_fields = set(["content", "tool_call_id"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class GetAllPromptsPromptConfigTypedDict(TypedDict):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[GetAllPromptsMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    model_type: NotRequired[Nullable[GetAllPromptsModelType]]
    r"""The modality of the model"""
    model_parameters: NotRequired[GetAllPromptsModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptsProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The ID of the integration to use"""
    version: NotRequired[str]


class GetAllPromptsPromptConfig(BaseModel):
    r"""A list of messages compatible with the openAI schema"""

    messages: List[GetAllPromptsMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    model_type: OptionalNullable[GetAllPromptsModelType] = UNSET
    r"""The modality of the model"""

    model_parameters: Optional[GetAllPromptsModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptsProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The ID of the integration to use"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "stream",
                "model",
                "model_db_id",
                "model_type",
                "model_parameters",
                "provider",
                "integration_id",
                "version",
            ]
        )
        nullable_fields = set(["model_db_id", "model_type", "integration_id"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


GetAllPromptsUseCases = Literal[
    "Agents simulations",
    "Agents",
    "API interaction",
    "Autonomous Agents",
    "Chatbots",
    "Classification",
    "Code understanding",
    "Code writing",
    "Conversation",
    "Documents QA",
    "Evaluation",
    "Extraction",
    "Multi-modal",
    "Self-checking",
    "Sentiment analysis",
    "SQL",
    "Summarization",
    "Tagging",
    "Translation (document)",
    "Translation (sentences)",
]


GetAllPromptsLanguage = Literal[
    "Chinese",
    "Dutch",
    "English",
    "French",
    "German",
    "Russian",
    "Spanish",
]
r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class GetAllPromptsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[GetAllPromptsUseCases]]
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""
    language: NotRequired[Nullable[GetAllPromptsLanguage]]
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""


class GetAllPromptsMetadata(BaseModel):
    use_cases: Optional[List[GetAllPromptsUseCases]] = None
    r"""A list of use cases that the prompt is meant to be used for. Use this field to categorize the prompt for your own purpose"""

    language: OptionalNullable[GetAllPromptsLanguage] = UNSET
    r"""The language that the prompt is written in. Use this field to categorize the prompt for your own purpose"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["use_cases", "language"])
        nullable_fields = set(["language"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class GetAllPromptsPromptTypedDict(TypedDict):
    r"""A prompt entity with configuration, metadata, and versioning."""

    id: str
    type: GetAllPromptsType
    owner: str
    domain_id: str
    created: str
    updated: str
    display_name: str
    r"""The prompt’s name, meant to be displayable in the UI."""
    prompt_config: GetAllPromptsPromptConfigTypedDict
    r"""A list of messages compatible with the openAI schema"""
    created_by_id: NotRequired[Nullable[str]]
    updated_by_id: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]
    r"""The prompt’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""
    metadata: NotRequired[GetAllPromptsMetadataTypedDict]


class GetAllPromptsPrompt(BaseModel):
    r"""A prompt entity with configuration, metadata, and versioning."""

    id: Annotated[str, pydantic.Field(alias="_id")]

    type: GetAllPromptsType

    owner: str

    domain_id: str

    created: str

    updated: str

    display_name: str
    r"""The prompt’s name, meant to be displayable in the UI."""

    prompt_config: GetAllPromptsPromptConfig
    r"""A list of messages compatible with the openAI schema"""

    created_by_id: OptionalNullable[str] = UNSET

    updated_by_id: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET
    r"""The prompt’s description, meant to be displayable in the UI. Use this field to optionally store a long form explanation of the prompt for your own purpose"""

    metadata: Optional[GetAllPromptsMetadata] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["created_by_id", "updated_by_id", "description", "metadata"]
        )
        nullable_fields = set(["created_by_id", "updated_by_id", "description"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class GetAllPromptsResponseBodyTypedDict(TypedDict):
    r"""Prompts retrieved."""

    object: GetAllPromptsObject
    data: List[GetAllPromptsPromptTypedDict]
    has_more: bool


class GetAllPromptsResponseBody(BaseModel):
    r"""Prompts retrieved."""

    object: GetAllPromptsObject

    data: List[GetAllPromptsPrompt]

    has_more: bool
