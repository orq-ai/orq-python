"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .datapart import DataPart, DataPartTypedDict
from .filepart import FilePart, FilePartTypedDict
from .textpart import TextPart, TextPartTypedDict
from .toolcallpart import ToolCallPart, ToolCallPartTypedDict
from .toolresultpart import ToolResultPart, ToolResultPartTypedDict
from orq_ai_sdk.types import BaseModel, UNSET_SENTINEL
from orq_ai_sdk.utils import get_discriminator
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


AgentStartedStreamingEventType = Literal["event.agents.started",]


AgentStartedStreamingEventRole = Literal[
    "user",
    "agent",
    "tool",
    "system",
]
r"""Extended A2A message role"""


AgentStartedStreamingEventPartsTypedDict = TypeAliasType(
    "AgentStartedStreamingEventPartsTypedDict",
    Union[
        TextPartTypedDict,
        DataPartTypedDict,
        FilePartTypedDict,
        ToolResultPartTypedDict,
        ToolCallPartTypedDict,
    ],
)


AgentStartedStreamingEventParts = Annotated[
    Union[
        Annotated[TextPart, Tag("text")],
        Annotated[DataPart, Tag("data")],
        Annotated[FilePart, Tag("file")],
        Annotated[ToolCallPart, Tag("tool_call")],
        Annotated[ToolResultPart, Tag("tool_result")],
    ],
    Discriminator(lambda m: get_discriminator(m, "kind", "kind")),
]


class InputMessageTypedDict(TypedDict):
    role: AgentStartedStreamingEventRole
    r"""Extended A2A message role"""
    parts: List[AgentStartedStreamingEventPartsTypedDict]
    message_id: NotRequired[str]
    metadata: NotRequired[Dict[str, Any]]


class InputMessage(BaseModel):
    role: AgentStartedStreamingEventRole
    r"""Extended A2A message role"""

    parts: List[AgentStartedStreamingEventParts]

    message_id: Annotated[Optional[str], pydantic.Field(alias="messageId")] = None

    metadata: Optional[Dict[str, Any]] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["messageId", "metadata"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ToolApprovalRequired = Literal[
    "all",
    "respect_tool",
    "none",
]
r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""


class AgentStartedStreamingEventConditionsTypedDict(TypedDict):
    condition: str
    r"""The argument of the tool call to evaluate"""
    operator: str
    r"""The operator to use"""
    value: str
    r"""The value to compare against"""


class AgentStartedStreamingEventConditions(BaseModel):
    condition: str
    r"""The argument of the tool call to evaluate"""

    operator: str
    r"""The operator to use"""

    value: str
    r"""The value to compare against"""


class ToolsModelTypedDict(TypedDict):
    id: str
    r"""The id of the resource"""
    action_type: str
    key: NotRequired[str]
    r"""Optional tool key for custom tools"""
    display_name: NotRequired[str]
    description: NotRequired[str]
    r"""Optional tool description"""
    requires_approval: NotRequired[bool]
    tool_id: NotRequired[str]
    r"""Nested tool ID for MCP tools (identifies specific tool within MCP server)"""
    conditions: NotRequired[List[AgentStartedStreamingEventConditionsTypedDict]]
    timeout: NotRequired[float]
    r"""Tool execution timeout in seconds (default: 2 minutes, max: 10 minutes)"""


class ToolsModel(BaseModel):
    id: str
    r"""The id of the resource"""

    action_type: str

    key: Optional[str] = None
    r"""Optional tool key for custom tools"""

    display_name: Optional[str] = None

    description: Optional[str] = None
    r"""Optional tool description"""

    requires_approval: Optional[bool] = False

    tool_id: Optional[str] = None
    r"""Nested tool ID for MCP tools (identifies specific tool within MCP server)"""

    conditions: Optional[List[AgentStartedStreamingEventConditions]] = None

    timeout: Optional[float] = 120
    r"""Tool execution timeout in seconds (default: 2 minutes, max: 10 minutes)"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "key",
                "display_name",
                "description",
                "requires_approval",
                "tool_id",
                "conditions",
                "timeout",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


ExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class EvaluatorsModelTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: ExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class EvaluatorsModel(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: ExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["sample_rate"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


AgentStartedStreamingEventExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""


class GuardrailsTypedDict(TypedDict):
    id: str
    r"""Unique key or identifier of the evaluator"""
    execute_on: AgentStartedStreamingEventExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""
    sample_rate: NotRequired[float]
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""


class Guardrails(BaseModel):
    id: str
    r"""Unique key or identifier of the evaluator"""

    execute_on: AgentStartedStreamingEventExecuteOn
    r"""Determines whether the evaluator runs on the agent input (user message) or output (agent response)."""

    sample_rate: Optional[float] = 50
    r"""The percentage of executions to evaluate with this evaluator (1-100). For example, a value of 50 means the evaluator will run on approximately half of the executions."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["sample_rate"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class SettingsTypedDict(TypedDict):
    max_iterations: NotRequired[int]
    r"""Maximum iterations(llm calls) before the agent will stop executing."""
    max_execution_time: NotRequired[int]
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""
    tool_approval_required: NotRequired[ToolApprovalRequired]
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""
    tools: NotRequired[List[ToolsModelTypedDict]]
    evaluators: NotRequired[List[EvaluatorsModelTypedDict]]
    r"""Configuration for an evaluator applied to the agent"""
    guardrails: NotRequired[List[GuardrailsTypedDict]]
    r"""Configuration for a guardrail applied to the agent"""


class Settings(BaseModel):
    max_iterations: Optional[int] = 100
    r"""Maximum iterations(llm calls) before the agent will stop executing."""

    max_execution_time: Optional[int] = 600
    r"""Maximum time (in seconds) for the agent thinking process. This does not include the time for tool calls and sub agent calls. It will be loosely enforced, the in progress LLM calls will not be terminated and the last assistant message will be returned."""

    tool_approval_required: Optional[ToolApprovalRequired] = "respect_tool"
    r"""If all, the agent will require approval for all tools. If respect_tool, the agent will require approval for tools that have the requires_approval flag set to true. If none, the agent will not require approval for any tools."""

    tools: Optional[List[ToolsModel]] = None

    evaluators: Optional[List[EvaluatorsModel]] = None
    r"""Configuration for an evaluator applied to the agent"""

    guardrails: Optional[List[Guardrails]] = None
    r"""Configuration for a guardrail applied to the agent"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "max_iterations",
                "max_execution_time",
                "tool_approval_required",
                "tools",
                "evaluators",
                "guardrails",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class AgentStartedStreamingEventDataTypedDict(TypedDict):
    workflow_run_id: str
    input_message: InputMessageTypedDict
    model_id: str
    instructions: str
    system_prompt: str
    agent_manifest_id: str
    agent_key: str
    integration_id: NotRequired[str]
    settings: NotRequired[SettingsTypedDict]
    variables: NotRequired[Dict[str, Any]]
    tool_execution_id: NotRequired[str]
    is_continuation: NotRequired[bool]
    stream: NotRequired[bool]
    response_id: NotRequired[str]


class AgentStartedStreamingEventData(BaseModel):
    workflow_run_id: Annotated[str, pydantic.Field(alias="workflowRunId")]

    input_message: Annotated[InputMessage, pydantic.Field(alias="inputMessage")]

    model_id: Annotated[str, pydantic.Field(alias="modelId")]

    instructions: str

    system_prompt: str

    agent_manifest_id: str

    agent_key: str

    integration_id: Optional[str] = None

    settings: Optional[Settings] = None

    variables: Optional[Dict[str, Any]] = None

    tool_execution_id: Optional[str] = None

    is_continuation: Optional[bool] = None

    stream: Optional[bool] = None

    response_id: Annotated[Optional[str], pydantic.Field(alias="responseId")] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "integration_id",
                "settings",
                "variables",
                "tool_execution_id",
                "is_continuation",
                "stream",
                "responseId",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class AgentStartedStreamingEventTypedDict(TypedDict):
    r"""Emitted when the agent begins processing. Contains configuration details including the model, instructions, system prompt, and input message."""

    type: AgentStartedStreamingEventType
    timestamp: str
    r"""ISO timestamp of the event"""
    data: AgentStartedStreamingEventDataTypedDict


class AgentStartedStreamingEvent(BaseModel):
    r"""Emitted when the agent begins processing. Contains configuration details including the model, instructions, system prompt, and input message."""

    type: AgentStartedStreamingEventType

    timestamp: str
    r"""ISO timestamp of the event"""

    data: AgentStartedStreamingEventData
