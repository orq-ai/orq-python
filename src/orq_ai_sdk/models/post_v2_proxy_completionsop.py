"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import eventstreaming
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


PostV2ProxyCompletionsStopTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""


PostV2ProxyCompletionsStop = TypeAliasType(
    "PostV2ProxyCompletionsStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""


class PostV2ProxyCompletionsRequestBodyTypedDict(TypedDict):
    model: str
    r"""ID of the model to use"""
    prompt: str
    r"""The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays."""
    echo: NotRequired[Nullable[bool]]
    r"""Echo back the prompt in addition to the completion"""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[float]]
    r"""The maximum number of tokens that can be generated in the completion."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[PostV2ProxyCompletionsStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."""
    user: NotRequired[str]
    r"""A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse."""
    stream: NotRequired[bool]


class PostV2ProxyCompletionsRequestBody(BaseModel):
    model: str
    r"""ID of the model to use"""

    prompt: str
    r"""The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays."""

    echo: OptionalNullable[bool] = False
    r"""Echo back the prompt in addition to the completion"""

    frequency_penalty: OptionalNullable[float] = 0
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[float] = 16
    r"""The maximum number of tokens that can be generated in the completion."""

    presence_penalty: OptionalNullable[float] = 0
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[PostV2ProxyCompletionsStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""

    temperature: OptionalNullable[float] = 1
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = 1
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."""

    user: Optional[str] = None
    r"""A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse."""

    stream: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "echo",
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "seed",
            "stop",
            "temperature",
            "top_p",
            "user",
            "stream",
        ]
        nullable_fields = [
            "echo",
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "seed",
            "stop",
            "temperature",
            "top_p",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


PostV2ProxyCompletionsProxyFinishReason = Literal[
    "stop", "length", "content_filter", "tool_calls"
]
r"""The reason the model stopped generating tokens."""


class PostV2ProxyCompletionsProxyChoicesTypedDict(TypedDict):
    finish_reason: PostV2ProxyCompletionsProxyFinishReason
    r"""The reason the model stopped generating tokens."""
    index: float
    r"""The index of the choice in the list of choices."""
    text: str


class PostV2ProxyCompletionsProxyChoices(BaseModel):
    finish_reason: PostV2ProxyCompletionsProxyFinishReason
    r"""The reason the model stopped generating tokens."""

    index: float
    r"""The index of the choice in the list of choices."""

    text: str


PostV2ProxyCompletionsCreatedTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsCreatedTypedDict", Union[str, float]
)
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


PostV2ProxyCompletionsCreated = TypeAliasType(
    "PostV2ProxyCompletionsCreated", Union[str, float]
)
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


class PostV2ProxyCompletionsProxyPromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[int]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio input tokens consumed by the request."""


class PostV2ProxyCompletionsProxyPromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[int] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio input tokens consumed by the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["cached_tokens", "audio_tokens"]
        nullable_fields = ["cached_tokens", "audio_tokens"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsProxyCompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]
    accepted_prediction_tokens: NotRequired[Nullable[float]]
    rejected_prediction_tokens: NotRequired[Nullable[float]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio output tokens produced by the response."""


class PostV2ProxyCompletionsProxyCompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    accepted_prediction_tokens: OptionalNullable[float] = UNSET

    rejected_prediction_tokens: OptionalNullable[float] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio output tokens produced by the response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        nullable_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsProxyUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""
    prompt_tokens_details: NotRequired[
        Nullable[PostV2ProxyCompletionsProxyPromptTokensDetailsTypedDict]
    ]
    completion_tokens_details: NotRequired[
        Nullable[PostV2ProxyCompletionsProxyCompletionTokensDetailsTypedDict]
    ]


class PostV2ProxyCompletionsProxyUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""

    prompt_tokens_details: OptionalNullable[
        PostV2ProxyCompletionsProxyPromptTokensDetails
    ] = UNSET

    completion_tokens_details: OptionalNullable[
        PostV2ProxyCompletionsProxyCompletionTokensDetails
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "completion_tokens",
            "prompt_tokens",
            "total_tokens",
            "prompt_tokens_details",
            "completion_tokens_details",
        ]
        nullable_fields = ["prompt_tokens_details", "completion_tokens_details"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsDataTypedDict(TypedDict):
    id: str
    r"""A unique identifier for the completion."""
    choices: List[PostV2ProxyCompletionsProxyChoicesTypedDict]
    r"""The list of completion choices the model generated for the input prompt."""
    model: str
    r"""The model used for the chat completion."""
    object: str
    r"""The object type"""
    created: NotRequired[PostV2ProxyCompletionsCreatedTypedDict]
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    system_fingerprint: NotRequired[str]
    r"""This fingerprint represents the backend configuration that the model runs with."""
    usage: NotRequired[PostV2ProxyCompletionsProxyUsageTypedDict]
    r"""Usage statistics for the completion request."""


class PostV2ProxyCompletionsData(BaseModel):
    id: str
    r"""A unique identifier for the completion."""

    choices: List[PostV2ProxyCompletionsProxyChoices]
    r"""The list of completion choices the model generated for the input prompt."""

    model: str
    r"""The model used for the chat completion."""

    object: str
    r"""The object type"""

    created: Optional[PostV2ProxyCompletionsCreated] = None
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    system_fingerprint: Optional[str] = None
    r"""This fingerprint represents the backend configuration that the model runs with."""

    usage: Optional[PostV2ProxyCompletionsProxyUsage] = None
    r"""Usage statistics for the completion request."""


class PostV2ProxyCompletionsProxyResponseBodyTypedDict(TypedDict):
    r"""Represents a completion response from the API."""

    data: NotRequired[PostV2ProxyCompletionsDataTypedDict]


class PostV2ProxyCompletionsProxyResponseBody(BaseModel):
    r"""Represents a completion response from the API."""

    data: Optional[PostV2ProxyCompletionsData] = None


PostV2ProxyCompletionsFinishReason = Literal[
    "stop", "length", "content_filter", "tool_calls"
]
r"""The reason the model stopped generating tokens."""


class PostV2ProxyCompletionsChoicesTypedDict(TypedDict):
    finish_reason: PostV2ProxyCompletionsFinishReason
    r"""The reason the model stopped generating tokens."""
    index: float
    r"""The index of the choice in the list of choices."""
    text: str


class PostV2ProxyCompletionsChoices(BaseModel):
    finish_reason: PostV2ProxyCompletionsFinishReason
    r"""The reason the model stopped generating tokens."""

    index: float
    r"""The index of the choice in the list of choices."""

    text: str


CreatedTypedDict = TypeAliasType("CreatedTypedDict", Union[str, float])
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


Created = TypeAliasType("Created", Union[str, float])
r"""The Unix timestamp (in seconds) of when the chat completion was created."""


class PostV2ProxyCompletionsPromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[int]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio input tokens consumed by the request."""


class PostV2ProxyCompletionsPromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[int] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio input tokens consumed by the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["cached_tokens", "audio_tokens"]
        nullable_fields = ["cached_tokens", "audio_tokens"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsCompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]
    accepted_prediction_tokens: NotRequired[Nullable[float]]
    rejected_prediction_tokens: NotRequired[Nullable[float]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio output tokens produced by the response."""


class PostV2ProxyCompletionsCompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    accepted_prediction_tokens: OptionalNullable[float] = UNSET

    rejected_prediction_tokens: OptionalNullable[float] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio output tokens produced by the response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        nullable_fields = [
            "reasoning_tokens",
            "accepted_prediction_tokens",
            "rejected_prediction_tokens",
            "audio_tokens",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""
    prompt_tokens_details: NotRequired[
        Nullable[PostV2ProxyCompletionsPromptTokensDetailsTypedDict]
    ]
    completion_tokens_details: NotRequired[
        Nullable[PostV2ProxyCompletionsCompletionTokensDetailsTypedDict]
    ]


class PostV2ProxyCompletionsUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""

    prompt_tokens_details: OptionalNullable[
        PostV2ProxyCompletionsPromptTokensDetails
    ] = UNSET

    completion_tokens_details: OptionalNullable[
        PostV2ProxyCompletionsCompletionTokensDetails
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "completion_tokens",
            "prompt_tokens",
            "total_tokens",
            "prompt_tokens_details",
            "completion_tokens_details",
        ]
        nullable_fields = ["prompt_tokens_details", "completion_tokens_details"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class PostV2ProxyCompletionsResponseBodyTypedDict(TypedDict):
    r"""Represents a completion response from the API."""

    id: str
    r"""A unique identifier for the completion."""
    choices: List[PostV2ProxyCompletionsChoicesTypedDict]
    r"""The list of completion choices the model generated for the input prompt."""
    model: str
    r"""The model used for the chat completion."""
    object: str
    r"""The object type"""
    created: NotRequired[CreatedTypedDict]
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    system_fingerprint: NotRequired[str]
    r"""This fingerprint represents the backend configuration that the model runs with."""
    usage: NotRequired[PostV2ProxyCompletionsUsageTypedDict]
    r"""Usage statistics for the completion request."""


class PostV2ProxyCompletionsResponseBody(BaseModel):
    r"""Represents a completion response from the API."""

    id: str
    r"""A unique identifier for the completion."""

    choices: List[PostV2ProxyCompletionsChoices]
    r"""The list of completion choices the model generated for the input prompt."""

    model: str
    r"""The model used for the chat completion."""

    object: str
    r"""The object type"""

    created: Optional[Created] = None
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    system_fingerprint: Optional[str] = None
    r"""This fingerprint represents the backend configuration that the model runs with."""

    usage: Optional[PostV2ProxyCompletionsUsage] = None
    r"""Usage statistics for the completion request."""


PostV2ProxyCompletionsResponseTypedDict = TypeAliasType(
    "PostV2ProxyCompletionsResponseTypedDict",
    Union[
        PostV2ProxyCompletionsResponseBodyTypedDict,
        Union[
            eventstreaming.EventStream[
                PostV2ProxyCompletionsProxyResponseBodyTypedDict
            ],
            eventstreaming.EventStreamAsync[
                PostV2ProxyCompletionsProxyResponseBodyTypedDict
            ],
        ],
    ],
)


PostV2ProxyCompletionsResponse = TypeAliasType(
    "PostV2ProxyCompletionsResponse",
    Union[
        PostV2ProxyCompletionsResponseBody,
        Union[
            eventstreaming.EventStream[PostV2ProxyCompletionsProxyResponseBody],
            eventstreaming.EventStreamAsync[PostV2ProxyCompletionsProxyResponseBody],
        ],
    ],
)
