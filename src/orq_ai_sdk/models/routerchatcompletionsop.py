"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from enum import Enum
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import Any, AsyncGenerator, Dict, Generator, List, Optional, Union
from typing_extensions import NotRequired, TypedDict


class RouterChatCompletionsMessagesRouterChatCompletionsRequestRole(str, Enum):
    r"""The role of the messages author, in this case **tool**."""

    TOOL = "tool"


class RouterChatCompletions2RouterChatCompletionsRequestRequestBodyMessages4Type(
    str, Enum
):
    IMAGE_URL = "image_url"


class RouterChatCompletions2Detail(str, Enum):
    r"""Specifies the detail level of the image."""

    LOW = "low"
    HIGH = "high"
    AUTO = "auto"


class RouterChatCompletions2RouterChatCompletionsRequestImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded image data."""
    detail: NotRequired[RouterChatCompletions2Detail]
    r"""Specifies the detail level of the image."""


class RouterChatCompletions2RouterChatCompletionsRequestImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded image data."""

    detail: Optional[RouterChatCompletions2Detail] = None
    r"""Specifies the detail level of the image."""


class RouterChatCompletions2RouterChatCompletionsRequest2TypedDict(TypedDict):
    type: RouterChatCompletions2RouterChatCompletionsRequestRequestBodyMessages4Type
    image_url: RouterChatCompletions2RouterChatCompletionsRequestImageURLTypedDict


class RouterChatCompletions2RouterChatCompletionsRequest2(BaseModel):
    type: RouterChatCompletions2RouterChatCompletionsRequestRequestBodyMessages4Type

    image_url: RouterChatCompletions2RouterChatCompletionsRequestImageURL


class RouterChatCompletions2RouterChatCompletionsRequestRequestBodyMessagesType(
    str, Enum
):
    TEXT = "text"


class RouterChatCompletions2RouterChatCompletionsRequest1TypedDict(TypedDict):
    type: RouterChatCompletions2RouterChatCompletionsRequestRequestBodyMessagesType
    text: str


class RouterChatCompletions2RouterChatCompletionsRequest1(BaseModel):
    type: RouterChatCompletions2RouterChatCompletionsRequestRequestBodyMessagesType

    text: str


RouterChatCompletionsContentRouterChatCompletionsRequest2TypedDict = Union[
    RouterChatCompletions2RouterChatCompletionsRequest1TypedDict,
    RouterChatCompletions2RouterChatCompletionsRequest2TypedDict,
]


RouterChatCompletionsContentRouterChatCompletionsRequest2 = Union[
    RouterChatCompletions2RouterChatCompletionsRequest1,
    RouterChatCompletions2RouterChatCompletionsRequest2,
]


RouterChatCompletionsMessagesRouterChatCompletionsRequestContentTypedDict = Union[
    str, List[RouterChatCompletionsContentRouterChatCompletionsRequest2TypedDict]
]
r"""The contents of a particular role's message."""


RouterChatCompletionsMessagesRouterChatCompletionsRequestContent = Union[
    str, List[RouterChatCompletionsContentRouterChatCompletionsRequest2]
]
r"""The contents of a particular role's message."""


class FourTypedDict(TypedDict):
    role: RouterChatCompletionsMessagesRouterChatCompletionsRequestRole
    r"""The role of the messages author, in this case **tool**."""
    tool_call_id: str
    content: RouterChatCompletionsMessagesRouterChatCompletionsRequestContentTypedDict
    r"""The contents of a particular role's message."""


class Four(BaseModel):
    role: RouterChatCompletionsMessagesRouterChatCompletionsRequestRole
    r"""The role of the messages author, in this case **tool**."""

    tool_call_id: str

    content: RouterChatCompletionsMessagesRouterChatCompletionsRequestContent
    r"""The contents of a particular role's message."""


class RouterChatCompletionsMessagesRouterChatCompletionsRole(str, Enum):
    r"""The role of the messages author, in this case **assistant**."""

    ASSISTANT = "assistant"


RouterChatCompletionsMessagesRouterChatCompletionsContentTypedDict = Union[
    str, List[Any]
]
r"""The contents of the assistant message. Required unless tool_calls is specified."""


RouterChatCompletionsMessagesRouterChatCompletionsContent = Union[str, List[Any]]
r"""The contents of the assistant message. Required unless tool_calls is specified."""


class MessagesType(str, Enum):
    r"""The type of the tool. Currently, only **function** is supported."""

    FUNCTION = "function"


class MessagesFunctionTypedDict(TypedDict):
    r"""The function that the model called."""

    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    arguments: str
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class MessagesFunction(BaseModel):
    r"""The function that the model called."""

    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    arguments: str
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class MessagesToolCallsTypedDict(TypedDict):
    id: str
    r"""The ID of the tool call."""
    type: MessagesType
    r"""The type of the tool. Currently, only **function** is supported."""
    function: MessagesFunctionTypedDict
    r"""The function that the model called."""


class MessagesToolCalls(BaseModel):
    id: str
    r"""The ID of the tool call."""

    type: MessagesType
    r"""The type of the tool. Currently, only **function** is supported."""

    function: MessagesFunction
    r"""The function that the model called."""


class Messages3TypedDict(TypedDict):
    role: RouterChatCompletionsMessagesRouterChatCompletionsRole
    r"""The role of the messages author, in this case **assistant**."""
    tool_calls: List[MessagesToolCallsTypedDict]
    r"""The tool calls generated by the model, such as function calls."""
    content: NotRequired[
        RouterChatCompletionsMessagesRouterChatCompletionsContentTypedDict
    ]
    r"""The contents of the assistant message. Required unless tool_calls is specified."""
    refusal: NotRequired[Nullable[str]]
    r"""The refusal message by the assistant."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class Messages3(BaseModel):
    role: RouterChatCompletionsMessagesRouterChatCompletionsRole
    r"""The role of the messages author, in this case **assistant**."""

    tool_calls: List[MessagesToolCalls]
    r"""The tool calls generated by the model, such as function calls."""

    content: Optional[RouterChatCompletionsMessagesRouterChatCompletionsContent] = None
    r"""The contents of the assistant message. Required unless tool_calls is specified."""

    refusal: OptionalNullable[str] = UNSET
    r"""The refusal message by the assistant."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["content", "refusal", "name"]
        nullable_fields = ["refusal"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class RouterChatCompletionsMessagesRole(str, Enum):
    r"""The role of the messages author, in this case **user**."""

    USER = "user"


class RouterChatCompletions2RouterChatCompletionsRequestRequestBodyType(str, Enum):
    IMAGE_URL = "image_url"


class TwoDetail(str, Enum):
    r"""Specifies the detail level of the image."""

    LOW = "low"
    HIGH = "high"
    AUTO = "auto"


class RouterChatCompletions2RouterChatCompletionsImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded image data."""
    detail: NotRequired[TwoDetail]
    r"""Specifies the detail level of the image."""


class RouterChatCompletions2RouterChatCompletionsImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded image data."""

    detail: Optional[TwoDetail] = None
    r"""Specifies the detail level of the image."""


class RouterChatCompletions2RouterChatCompletions2TypedDict(TypedDict):
    type: RouterChatCompletions2RouterChatCompletionsRequestRequestBodyType
    image_url: RouterChatCompletions2RouterChatCompletionsImageURLTypedDict


class RouterChatCompletions2RouterChatCompletions2(BaseModel):
    type: RouterChatCompletions2RouterChatCompletionsRequestRequestBodyType

    image_url: RouterChatCompletions2RouterChatCompletionsImageURL


class RouterChatCompletions2RouterChatCompletionsRequestType(str, Enum):
    TEXT = "text"


class RouterChatCompletions2RouterChatCompletions1TypedDict(TypedDict):
    type: RouterChatCompletions2RouterChatCompletionsRequestType
    text: str


class RouterChatCompletions2RouterChatCompletions1(BaseModel):
    type: RouterChatCompletions2RouterChatCompletionsRequestType

    text: str


RouterChatCompletionsContentRouterChatCompletions2TypedDict = Union[
    RouterChatCompletions2RouterChatCompletions1TypedDict,
    RouterChatCompletions2RouterChatCompletions2TypedDict,
]


RouterChatCompletionsContentRouterChatCompletions2 = Union[
    RouterChatCompletions2RouterChatCompletions1,
    RouterChatCompletions2RouterChatCompletions2,
]


RouterChatCompletionsMessagesContentTypedDict = Union[
    str, List[RouterChatCompletionsContentRouterChatCompletions2TypedDict]
]
r"""The contents of a particular role's message."""


RouterChatCompletionsMessagesContent = Union[
    str, List[RouterChatCompletionsContentRouterChatCompletions2]
]
r"""The contents of a particular role's message."""


class Messages2TypedDict(TypedDict):
    role: RouterChatCompletionsMessagesRole
    r"""The role of the messages author, in this case **user**."""
    content: RouterChatCompletionsMessagesContentTypedDict
    r"""The contents of a particular role's message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class Messages2(BaseModel):
    role: RouterChatCompletionsMessagesRole
    r"""The role of the messages author, in this case **user**."""

    content: RouterChatCompletionsMessagesContent
    r"""The contents of a particular role's message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class MessagesRole(str, Enum):
    r"""The role of the messages author, in this case **system**."""

    SYSTEM = "system"


class RouterChatCompletions2RouterChatCompletionsType(str, Enum):
    IMAGE_URL = "image_url"


class Detail(str, Enum):
    r"""Specifies the detail level of the image."""

    LOW = "low"
    HIGH = "high"
    AUTO = "auto"


class RouterChatCompletions2ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded image data."""
    detail: NotRequired[Detail]
    r"""Specifies the detail level of the image."""


class RouterChatCompletions2ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded image data."""

    detail: Optional[Detail] = None
    r"""Specifies the detail level of the image."""


class RouterChatCompletions22TypedDict(TypedDict):
    type: RouterChatCompletions2RouterChatCompletionsType
    image_url: RouterChatCompletions2ImageURLTypedDict


class RouterChatCompletions22(BaseModel):
    type: RouterChatCompletions2RouterChatCompletionsType

    image_url: RouterChatCompletions2ImageURL


class RouterChatCompletions2Type(str, Enum):
    TEXT = "text"


class RouterChatCompletions21TypedDict(TypedDict):
    type: RouterChatCompletions2Type
    text: str


class RouterChatCompletions21(BaseModel):
    type: RouterChatCompletions2Type

    text: str


RouterChatCompletionsContent2TypedDict = Union[
    RouterChatCompletions21TypedDict, RouterChatCompletions22TypedDict
]


RouterChatCompletionsContent2 = Union[RouterChatCompletions21, RouterChatCompletions22]


MessagesContentTypedDict = Union[str, List[RouterChatCompletionsContent2TypedDict]]
r"""The contents of a particular role's message."""


MessagesContent = Union[str, List[RouterChatCompletionsContent2]]
r"""The contents of a particular role's message."""


class Messages1TypedDict(TypedDict):
    role: MessagesRole
    r"""The role of the messages author, in this case **system**."""
    content: MessagesContentTypedDict
    r"""The contents of a particular role's message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class Messages1(BaseModel):
    role: MessagesRole
    r"""The role of the messages author, in this case **system**."""

    content: MessagesContent
    r"""The contents of a particular role's message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


RouterChatCompletionsMessagesTypedDict = Union[
    Messages1TypedDict, Messages2TypedDict, FourTypedDict, Messages3TypedDict
]


RouterChatCompletionsMessages = Union[Messages1, Messages2, Four, Messages3]


class RouterChatCompletionsType(str, Enum):
    r"""The type of the tool. Currently, only function is supported."""

    FUNCTION = "function"


class RouterChatCompletionsRouterChatCompletionsType(str, Enum):
    OBJECT = "object"


class ParametersTypedDict(TypedDict):
    r"""The parameters the functions accepts, described as a JSON Schema object"""

    type: RouterChatCompletionsRouterChatCompletionsType
    properties: Dict[str, Any]
    required: NotRequired[List[str]]


class Parameters(BaseModel):
    r"""The parameters the functions accepts, described as a JSON Schema object"""

    type: RouterChatCompletionsRouterChatCompletionsType

    properties: Dict[str, Any]

    required: Optional[List[str]] = None


class RouterChatCompletionsFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the function does, used by the model to choose when and how to call the function."""
    parameters: NotRequired[ParametersTypedDict]
    r"""The parameters the functions accepts, described as a JSON Schema object"""
    strict: NotRequired[Nullable[bool]]
    r"""Whether to enable strict schema adherence when generating the function call."""


class RouterChatCompletionsFunction(BaseModel):
    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the function does, used by the model to choose when and how to call the function."""

    parameters: Optional[Parameters] = None
    r"""The parameters the functions accepts, described as a JSON Schema object"""

    strict: OptionalNullable[bool] = False
    r"""Whether to enable strict schema adherence when generating the function call."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "parameters", "strict"]
        nullable_fields = ["strict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ToolsTypedDict(TypedDict):
    type: RouterChatCompletionsType
    r"""The type of the tool. Currently, only function is supported."""
    function: RouterChatCompletionsFunctionTypedDict


class Tools(BaseModel):
    type: RouterChatCompletionsType
    r"""The type of the tool. Currently, only function is supported."""

    function: RouterChatCompletionsFunction


class RouterChatCompletionsRequestBodyTypedDict(TypedDict):
    r"""Creates a model response for the given chat conversation."""

    model: str
    r"""ID of the model to use"""
    messages: List[RouterChatCompletionsMessagesTypedDict]
    r"""A list of messages comprising the conversation so far."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[float]]
    r"""The maximum number of tokens that can be generated in the chat completion."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stream: NotRequired[Nullable[bool]]
    r"""If set, partial message deltas will be sent, like in ChatGPT."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    tools: NotRequired[List[ToolsTypedDict]]
    r"""A list of tools the model may call."""


class RouterChatCompletionsRequestBody(BaseModel):
    r"""Creates a model response for the given chat conversation."""

    model: str
    r"""ID of the model to use"""

    messages: List[RouterChatCompletionsMessages]
    r"""A list of messages comprising the conversation so far."""

    frequency_penalty: OptionalNullable[float] = 0
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[float] = UNSET
    r"""The maximum number of tokens that can be generated in the chat completion."""

    presence_penalty: OptionalNullable[float] = 0
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stream: OptionalNullable[bool] = False
    r"""If set, partial message deltas will be sent, like in ChatGPT."""

    temperature: OptionalNullable[float] = 1
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = 1
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    tools: Optional[List[Tools]] = None
    r"""A list of tools the model may call."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "seed",
            "stream",
            "temperature",
            "top_p",
            "tools",
        ]
        nullable_fields = [
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "seed",
            "stream",
            "temperature",
            "top_p",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class RouterChatCompletionsRouterChatCompletionsResponseBodyTypedDict(TypedDict):
    r"""A response body that follows the official OpenAI schema"""

    data: str
    event: NotRequired[str]
    id: NotRequired[str]
    retry: NotRequired[int]


class RouterChatCompletionsRouterChatCompletionsResponseBody(BaseModel):
    r"""A response body that follows the official OpenAI schema"""

    data: str

    event: Optional[str] = None

    id: Optional[str] = None

    retry: Optional[int] = None


class ResponseBodyFinishReason(str, Enum):
    r"""The reason the model stopped generating tokens."""

    STOP = "stop"
    LENGTH = "length"
    CONTENT_FILTER = "content_filter"
    TOOL_CALLS = "tool_calls"


class RouterChatCompletionsResponseBodyTopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class RouterChatCompletionsResponseBodyTopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class RouterChatCompletionsResponseBodyContentTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[RouterChatCompletionsResponseBodyTopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class RouterChatCompletionsResponseBodyContent(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[RouterChatCompletionsResponseBodyTopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class RouterChatCompletionsResponseBodyRouterChatCompletionsTopLogprobsTypedDict(
    TypedDict
):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class RouterChatCompletionsResponseBodyRouterChatCompletionsTopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ResponseBodyRefusalTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[
        RouterChatCompletionsResponseBodyRouterChatCompletionsTopLogprobsTypedDict
    ]
    r"""List of the most likely tokens and their log probability, at this token position."""


class ResponseBodyRefusal(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[
        RouterChatCompletionsResponseBodyRouterChatCompletionsTopLogprobs
    ]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ResponseBodyLogprobsTypedDict(TypedDict):
    r"""Log probability information for the choice."""

    content: Nullable[List[RouterChatCompletionsResponseBodyContentTypedDict]]
    r"""A list of message content tokens with log probability information."""
    refusal: Nullable[List[ResponseBodyRefusalTypedDict]]
    r"""A list of message refusal tokens with log probability information."""


class ResponseBodyLogprobs(BaseModel):
    r"""Log probability information for the choice."""

    content: Nullable[List[RouterChatCompletionsResponseBodyContent]]
    r"""A list of message content tokens with log probability information."""

    refusal: Nullable[List[ResponseBodyRefusal]]
    r"""A list of message refusal tokens with log probability information."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["content", "refusal"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class RouterChatCompletionsResponseBodyType(str, Enum):
    FUNCTION = "function"


class RouterChatCompletionsResponseBodyFunctionTypedDict(TypedDict):
    r"""The function that the model called."""

    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    arguments: str
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class RouterChatCompletionsResponseBodyFunction(BaseModel):
    r"""The function that the model called."""

    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    arguments: str
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class RouterChatCompletionsResponseBodyToolCallsTypedDict(TypedDict):
    id: str
    type: RouterChatCompletionsResponseBodyType
    function: RouterChatCompletionsResponseBodyFunctionTypedDict
    r"""The function that the model called."""


class RouterChatCompletionsResponseBodyToolCalls(BaseModel):
    id: str

    type: RouterChatCompletionsResponseBodyType

    function: RouterChatCompletionsResponseBodyFunction
    r"""The function that the model called."""


class RouterChatCompletionsResponseBodyRole(str, Enum):
    SYSTEM = "system"
    ASSISTANT = "assistant"
    USER = "user"
    EXCEPTION = "exception"
    TOOL = "tool"
    PROMPT = "prompt"
    CORRECTION = "correction"
    EXPECTED_OUTPUT = "expected_output"


class DeltaTypedDict(TypedDict):
    r"""A chat completion delta generated by streamed model responses."""

    content: Nullable[str]
    refusal: Nullable[str]
    tool_calls: List[RouterChatCompletionsResponseBodyToolCallsTypedDict]
    role: RouterChatCompletionsResponseBodyRole


class Delta(BaseModel):
    r"""A chat completion delta generated by streamed model responses."""

    content: Nullable[str]

    refusal: Nullable[str]

    tool_calls: List[RouterChatCompletionsResponseBodyToolCalls]

    role: RouterChatCompletionsResponseBodyRole

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["content", "refusal"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class RouterChatCompletionsResponseBodyChoicesTypedDict(TypedDict):
    finish_reason: ResponseBodyFinishReason
    r"""The reason the model stopped generating tokens."""
    index: float
    r"""The index of the choice in the list of choices."""
    logprobs: Nullable[ResponseBodyLogprobsTypedDict]
    r"""Log probability information for the choice."""
    delta: DeltaTypedDict
    r"""A chat completion delta generated by streamed model responses."""


class RouterChatCompletionsResponseBodyChoices(BaseModel):
    finish_reason: ResponseBodyFinishReason
    r"""The reason the model stopped generating tokens."""

    index: float
    r"""The index of the choice in the list of choices."""

    logprobs: Nullable[ResponseBodyLogprobs]
    r"""Log probability information for the choice."""

    delta: Delta
    r"""A chat completion delta generated by streamed model responses."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["logprobs"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class RouterChatCompletionsResponseBodyUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""


class RouterChatCompletionsResponseBodyUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""


class ResponseBody2TypedDict(TypedDict):
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    id: str
    r"""A unique identifier for the chat completion."""
    choices: List[RouterChatCompletionsResponseBodyChoicesTypedDict]
    r"""A list of chat completion choices. Can contain more than one elements if n is greater than 1. Can also be empty for the last chunk if you set stream_options: {\"include_usage\": true}."""
    created: float
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    model: str
    r"""The model used for the chat completion."""
    system_fingerprint: str
    r"""This fingerprint represents the backend configuration that the model runs with."""
    object: str
    r"""The object type"""
    usage: RouterChatCompletionsResponseBodyUsageTypedDict
    r"""Usage statistics for the completion request."""


class ResponseBody2(BaseModel):
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    id: str
    r"""A unique identifier for the chat completion."""

    choices: List[RouterChatCompletionsResponseBodyChoices]
    r"""A list of chat completion choices. Can contain more than one elements if n is greater than 1. Can also be empty for the last chunk if you set stream_options: {\"include_usage\": true}."""

    created: float
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    model: str
    r"""The model used for the chat completion."""

    system_fingerprint: str
    r"""This fingerprint represents the backend configuration that the model runs with."""

    object: str
    r"""The object type"""

    usage: RouterChatCompletionsResponseBodyUsage
    r"""Usage statistics for the completion request."""


class RouterChatCompletionsResponseBodyFinishReason(str, Enum):
    r"""The reason the model stopped generating tokens."""

    STOP = "stop"
    LENGTH = "length"
    CONTENT_FILTER = "content_filter"
    TOOL_CALLS = "tool_calls"


class ResponseBodyType(str, Enum):
    FUNCTION = "function"


class ResponseBodyFunctionTypedDict(TypedDict):
    r"""The function that the model called."""

    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    arguments: str
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class ResponseBodyFunction(BaseModel):
    r"""The function that the model called."""

    name: str
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    arguments: str
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class ResponseBodyToolCallsTypedDict(TypedDict):
    id: str
    type: ResponseBodyType
    function: ResponseBodyFunctionTypedDict
    r"""The function that the model called."""


class ResponseBodyToolCalls(BaseModel):
    id: str

    type: ResponseBodyType

    function: ResponseBodyFunction
    r"""The function that the model called."""


class ResponseBodyRole(str, Enum):
    SYSTEM = "system"
    ASSISTANT = "assistant"
    USER = "user"
    EXCEPTION = "exception"
    TOOL = "tool"
    PROMPT = "prompt"
    CORRECTION = "correction"
    EXPECTED_OUTPUT = "expected_output"


class ResponseBodyMessageTypedDict(TypedDict):
    r"""A chat completion message generated by the model."""

    content: Nullable[str]
    refusal: Nullable[str]
    tool_calls: List[ResponseBodyToolCallsTypedDict]
    role: ResponseBodyRole


class ResponseBodyMessage(BaseModel):
    r"""A chat completion message generated by the model."""

    content: Nullable[str]

    refusal: Nullable[str]

    tool_calls: List[ResponseBodyToolCalls]

    role: ResponseBodyRole

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["content", "refusal"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class TopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class TopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ResponseBodyContentTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[TopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class ResponseBodyContent(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[TopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ResponseBodyTopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class ResponseBodyTopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class RefusalTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[ResponseBodyTopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class Refusal(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[ResponseBodyTopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class LogprobsTypedDict(TypedDict):
    r"""Log probability information for the choice."""

    content: Nullable[List[ResponseBodyContentTypedDict]]
    r"""A list of message content tokens with log probability information."""
    refusal: Nullable[List[RefusalTypedDict]]
    r"""A list of message refusal tokens with log probability information."""


class Logprobs(BaseModel):
    r"""Log probability information for the choice."""

    content: Nullable[List[ResponseBodyContent]]
    r"""A list of message content tokens with log probability information."""

    refusal: Nullable[List[Refusal]]
    r"""A list of message refusal tokens with log probability information."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["content", "refusal"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ResponseBodyChoicesTypedDict(TypedDict):
    finish_reason: RouterChatCompletionsResponseBodyFinishReason
    r"""The reason the model stopped generating tokens."""
    index: float
    r"""The index of the choice in the list of choices."""
    message: ResponseBodyMessageTypedDict
    r"""A chat completion message generated by the model."""
    logprobs: Nullable[LogprobsTypedDict]
    r"""Log probability information for the choice."""


class ResponseBodyChoices(BaseModel):
    finish_reason: RouterChatCompletionsResponseBodyFinishReason
    r"""The reason the model stopped generating tokens."""

    index: float
    r"""The index of the choice in the list of choices."""

    message: ResponseBodyMessage
    r"""A chat completion message generated by the model."""

    logprobs: Nullable[Logprobs]
    r"""Log probability information for the choice."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["logprobs"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ResponseBodyUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""


class ResponseBodyUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""


class ResponseBody1TypedDict(TypedDict):
    r"""Represents a chat completion response returned by model, based on the provided input."""

    id: str
    r"""A unique identifier for the chat completion."""
    choices: List[ResponseBodyChoicesTypedDict]
    r"""A list of chat completion choices. Can be more than one if n is greater than 1."""
    created: float
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    model: str
    r"""The model used for the chat completion."""
    system_fingerprint: str
    r"""This fingerprint represents the backend configuration that the model runs with."""
    object: str
    r"""The object type"""
    usage: ResponseBodyUsageTypedDict
    r"""Usage statistics for the completion request."""


class ResponseBody1(BaseModel):
    r"""Represents a chat completion response returned by model, based on the provided input."""

    id: str
    r"""A unique identifier for the chat completion."""

    choices: List[ResponseBodyChoices]
    r"""A list of chat completion choices. Can be more than one if n is greater than 1."""

    created: float
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    model: str
    r"""The model used for the chat completion."""

    system_fingerprint: str
    r"""This fingerprint represents the backend configuration that the model runs with."""

    object: str
    r"""The object type"""

    usage: ResponseBodyUsage
    r"""Usage statistics for the completion request."""


RouterChatCompletionsResponseBodyTypedDict = Union[
    ResponseBody1TypedDict, ResponseBody2TypedDict
]
r"""A response body that follows the official OpenAI schema"""


RouterChatCompletionsResponseBody = Union[ResponseBody1, ResponseBody2]
r"""A response body that follows the official OpenAI schema"""


RouterChatCompletionsResponseTypedDict = Union[
    RouterChatCompletionsResponseBodyTypedDict,
    Union[
        Generator[
            RouterChatCompletionsRouterChatCompletionsResponseBodyTypedDict, None, None
        ],
        AsyncGenerator[
            RouterChatCompletionsRouterChatCompletionsResponseBodyTypedDict, None
        ],
    ],
]


RouterChatCompletionsResponse = Union[
    RouterChatCompletionsResponseBody,
    Union[
        Generator[RouterChatCompletionsRouterChatCompletionsResponseBody, None, None],
        AsyncGenerator[RouterChatCompletionsRouterChatCompletionsResponseBody, None],
    ],
]
