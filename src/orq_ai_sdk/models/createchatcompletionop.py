"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .audiocontentpartschema import (
    AudioContentPartSchema,
    AudioContentPartSchemaTypedDict,
)
from .filecontentpartschema import FileContentPartSchema, FileContentPartSchemaTypedDict
from .imagecontentpartschema import (
    ImageContentPartSchema,
    ImageContentPartSchemaTypedDict,
)
from .publiccontact import PublicContact, PublicContactTypedDict
from .reasoningpartschema import ReasoningPartSchema, ReasoningPartSchemaTypedDict
from .redactedreasoningpartschema import (
    RedactedReasoningPartSchema,
    RedactedReasoningPartSchemaTypedDict,
)
from .refusalpartschema import RefusalPartSchema, RefusalPartSchemaTypedDict
from .textcontentpartschema import TextContentPartSchema, TextContentPartSchemaTypedDict
from .thinkingconfigdisabledschema import (
    ThinkingConfigDisabledSchema,
    ThinkingConfigDisabledSchemaTypedDict,
)
from .thinkingconfigenabledschema import (
    ThinkingConfigEnabledSchema,
    ThinkingConfigEnabledSchemaTypedDict,
)
from orq_ai_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_ai_sdk.utils import eventstreaming, get_discriminator
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


CreateChatCompletionMessagesRouterRequestRequestBody5Role = Literal["tool",]
r"""The role of the messages author, in this case tool."""


CreateChatCompletionContentRouterRequest2TypedDict = TextContentPartSchemaTypedDict


CreateChatCompletionContentRouterRequest2 = TextContentPartSchema


CreateChatCompletionMessagesRouterRequestRequestBody5ContentTypedDict = TypeAliasType(
    "CreateChatCompletionMessagesRouterRequestRequestBody5ContentTypedDict",
    Union[str, List[CreateChatCompletionContentRouterRequest2TypedDict]],
)
r"""The contents of the tool message."""


CreateChatCompletionMessagesRouterRequestRequestBody5Content = TypeAliasType(
    "CreateChatCompletionMessagesRouterRequestRequestBody5Content",
    Union[str, List[CreateChatCompletionContentRouterRequest2]],
)
r"""The contents of the tool message."""


CreateChatCompletionMessagesRouterType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


CreateChatCompletionMessagesTTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class CreateChatCompletionMessagesCacheControlTypedDict(TypedDict):
    type: CreateChatCompletionMessagesRouterType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[CreateChatCompletionMessagesTTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class CreateChatCompletionMessagesCacheControl(BaseModel):
    type: CreateChatCompletionMessagesRouterType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[CreateChatCompletionMessagesTTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionMessagesToolMessageTypedDict(TypedDict):
    role: CreateChatCompletionMessagesRouterRequestRequestBody5Role
    r"""The role of the messages author, in this case tool."""
    content: CreateChatCompletionMessagesRouterRequestRequestBody5ContentTypedDict
    r"""The contents of the tool message."""
    tool_call_id: Nullable[str]
    r"""Tool call that this message is responding to."""
    cache_control: NotRequired[CreateChatCompletionMessagesCacheControlTypedDict]


class CreateChatCompletionMessagesToolMessage(BaseModel):
    role: CreateChatCompletionMessagesRouterRequestRequestBody5Role
    r"""The role of the messages author, in this case tool."""

    content: CreateChatCompletionMessagesRouterRequestRequestBody5Content
    r"""The contents of the tool message."""

    tool_call_id: Nullable[str]
    r"""Tool call that this message is responding to."""

    cache_control: Optional[CreateChatCompletionMessagesCacheControl] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["cache_control"])
        nullable_fields = set(["tool_call_id"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateChatCompletionContentRouter2TypedDict = TypeAliasType(
    "CreateChatCompletionContentRouter2TypedDict",
    Union[
        RefusalPartSchemaTypedDict,
        RedactedReasoningPartSchemaTypedDict,
        TextContentPartSchemaTypedDict,
        ReasoningPartSchemaTypedDict,
    ],
)


CreateChatCompletionContentRouter2 = Annotated[
    Union[
        Annotated[TextContentPartSchema, Tag("text")],
        Annotated[RefusalPartSchema, Tag("refusal")],
        Annotated[ReasoningPartSchema, Tag("reasoning")],
        Annotated[RedactedReasoningPartSchema, Tag("redacted_reasoning")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


CreateChatCompletionMessagesRouterRequestRequestBodyContentTypedDict = TypeAliasType(
    "CreateChatCompletionMessagesRouterRequestRequestBodyContentTypedDict",
    Union[str, List[CreateChatCompletionContentRouter2TypedDict]],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


CreateChatCompletionMessagesRouterRequestRequestBodyContent = TypeAliasType(
    "CreateChatCompletionMessagesRouterRequestRequestBodyContent",
    Union[str, List[CreateChatCompletionContentRouter2]],
)
r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""


CreateChatCompletionMessagesRouterRequestRequestBodyRole = Literal["assistant",]
r"""The role of the messages author, in this case `assistant`."""


class CreateChatCompletionMessagesAudioTypedDict(TypedDict):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


class CreateChatCompletionMessagesAudio(BaseModel):
    r"""Data about a previous audio response from the model."""

    id: str
    r"""Unique identifier for a previous audio response from the model."""


CreateChatCompletionMessagesType = Literal["function",]
r"""The type of the tool. Currently, only `function` is supported."""


class CreateChatCompletionMessagesFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to call."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreateChatCompletionMessagesFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to call."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name", "arguments"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionMessagesToolCallsTypedDict(TypedDict):
    id: str
    r"""The ID of the tool call."""
    type: CreateChatCompletionMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""
    function: CreateChatCompletionMessagesFunctionTypedDict
    thought_signature: NotRequired[str]
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models when continuing a conversation after a tool call."""


class CreateChatCompletionMessagesToolCalls(BaseModel):
    id: str
    r"""The ID of the tool call."""

    type: CreateChatCompletionMessagesType
    r"""The type of the tool. Currently, only `function` is supported."""

    function: CreateChatCompletionMessagesFunction

    thought_signature: Optional[str] = None
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models when continuing a conversation after a tool call."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["thought_signature"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionMessagesAssistantMessageTypedDict(TypedDict):
    role: CreateChatCompletionMessagesRouterRequestRequestBodyRole
    r"""The role of the messages author, in this case `assistant`."""
    content: NotRequired[
        Nullable[CreateChatCompletionMessagesRouterRequestRequestBodyContentTypedDict]
    ]
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""
    refusal: NotRequired[Nullable[str]]
    r"""The refusal message by the assistant."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""
    audio: NotRequired[Nullable[CreateChatCompletionMessagesAudioTypedDict]]
    r"""Data about a previous audio response from the model."""
    tool_calls: NotRequired[List[CreateChatCompletionMessagesToolCallsTypedDict]]
    r"""The tool calls generated by the model, such as function calls."""


class CreateChatCompletionMessagesAssistantMessage(BaseModel):
    role: CreateChatCompletionMessagesRouterRequestRequestBodyRole
    r"""The role of the messages author, in this case `assistant`."""

    content: OptionalNullable[
        CreateChatCompletionMessagesRouterRequestRequestBodyContent
    ] = UNSET
    r"""The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."""

    refusal: OptionalNullable[str] = UNSET
    r"""The refusal message by the assistant."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    audio: OptionalNullable[CreateChatCompletionMessagesAudio] = UNSET
    r"""Data about a previous audio response from the model."""

    tool_calls: Optional[List[CreateChatCompletionMessagesToolCalls]] = None
    r"""The tool calls generated by the model, such as function calls."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["content", "refusal", "name", "audio", "tool_calls"])
        nullable_fields = set(["content", "refusal", "audio"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateChatCompletionMessagesRouterRequestRole = Literal["user",]
r"""The role of the messages author, in this case `user`."""


CreateChatCompletion2Type = Literal["file",]
r"""The type of the content part. Always `file`."""


CreateChatCompletion2RouterType = Literal["ephemeral",]
r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""


CreateChatCompletion2TTL = Literal[
    "5m",
    "1h",
]
r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

- `5m`: 5 minutes
- `1h`: 1 hour

Defaults to `5m`. Only supported by `Anthropic` Claude models.
"""


class CreateChatCompletion2CacheControlTypedDict(TypedDict):
    type: CreateChatCompletion2RouterType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""
    ttl: NotRequired[CreateChatCompletion2TTL]
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """


class CreateChatCompletion2CacheControl(BaseModel):
    type: CreateChatCompletion2RouterType
    r"""Create a cache control breakpoint at this content block. Accepts only the value \"ephemeral\"."""

    ttl: Optional[CreateChatCompletion2TTL] = "5m"
    r"""The time-to-live for the cache control breakpoint. This may be one of the following values:

    - `5m`: 5 minutes
    - `1h`: 1 hour

    Defaults to `5m`. Only supported by `Anthropic` Claude models.
    """

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletion24TypedDict(TypedDict):
    type: CreateChatCompletion2Type
    r"""The type of the content part. Always `file`."""
    file: FileContentPartSchemaTypedDict
    r"""File data for the content part. Must contain either file_data or uri, but not both."""
    cache_control: NotRequired[CreateChatCompletion2CacheControlTypedDict]


class CreateChatCompletion24(BaseModel):
    type: CreateChatCompletion2Type
    r"""The type of the content part. Always `file`."""

    file: FileContentPartSchema
    r"""File data for the content part. Must contain either file_data or uri, but not both."""

    cache_control: Optional[CreateChatCompletion2CacheControl] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["cache_control"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionContent2TypedDict = TypeAliasType(
    "CreateChatCompletionContent2TypedDict",
    Union[
        AudioContentPartSchemaTypedDict,
        TextContentPartSchemaTypedDict,
        ImageContentPartSchemaTypedDict,
        CreateChatCompletion24TypedDict,
    ],
)


CreateChatCompletionContent2 = Annotated[
    Union[
        Annotated[TextContentPartSchema, Tag("text")],
        Annotated[ImageContentPartSchema, Tag("image_url")],
        Annotated[AudioContentPartSchema, Tag("input_audio")],
        Annotated[CreateChatCompletion24, Tag("file")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


CreateChatCompletionMessagesRouterRequestContentTypedDict = TypeAliasType(
    "CreateChatCompletionMessagesRouterRequestContentTypedDict",
    Union[str, List[CreateChatCompletionContent2TypedDict]],
)
r"""The contents of the user message."""


CreateChatCompletionMessagesRouterRequestContent = TypeAliasType(
    "CreateChatCompletionMessagesRouterRequestContent",
    Union[str, List[CreateChatCompletionContent2]],
)
r"""The contents of the user message."""


class CreateChatCompletionMessagesUserMessageTypedDict(TypedDict):
    role: CreateChatCompletionMessagesRouterRequestRole
    r"""The role of the messages author, in this case `user`."""
    content: CreateChatCompletionMessagesRouterRequestContentTypedDict
    r"""The contents of the user message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class CreateChatCompletionMessagesUserMessage(BaseModel):
    role: CreateChatCompletionMessagesRouterRequestRole
    r"""The role of the messages author, in this case `user`."""

    content: CreateChatCompletionMessagesRouterRequestContent
    r"""The contents of the user message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionMessagesRouterRole = Literal["developer",]
r"""The role of the messages author, in this case  `developer`."""


CreateChatCompletionMessagesRouterContentTypedDict = TypeAliasType(
    "CreateChatCompletionMessagesRouterContentTypedDict",
    Union[str, List[TextContentPartSchemaTypedDict]],
)
r"""The contents of the developer message."""


CreateChatCompletionMessagesRouterContent = TypeAliasType(
    "CreateChatCompletionMessagesRouterContent", Union[str, List[TextContentPartSchema]]
)
r"""The contents of the developer message."""


class CreateChatCompletionMessagesDeveloperMessageTypedDict(TypedDict):
    role: CreateChatCompletionMessagesRouterRole
    r"""The role of the messages author, in this case  `developer`."""
    content: CreateChatCompletionMessagesRouterContentTypedDict
    r"""The contents of the developer message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class CreateChatCompletionMessagesDeveloperMessage(BaseModel):
    role: CreateChatCompletionMessagesRouterRole
    r"""The role of the messages author, in this case  `developer`."""

    content: CreateChatCompletionMessagesRouterContent
    r"""The contents of the developer message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionMessagesRole = Literal["system",]
r"""The role of the messages author, in this case `system`."""


CreateChatCompletionMessagesContentTypedDict = TypeAliasType(
    "CreateChatCompletionMessagesContentTypedDict",
    Union[str, List[TextContentPartSchemaTypedDict]],
)
r"""The contents of the system message."""


CreateChatCompletionMessagesContent = TypeAliasType(
    "CreateChatCompletionMessagesContent", Union[str, List[TextContentPartSchema]]
)
r"""The contents of the system message."""


class CreateChatCompletionMessagesSystemMessageTypedDict(TypedDict):
    r"""Developer-provided instructions that the model should follow, regardless of messages sent by the user."""

    role: CreateChatCompletionMessagesRole
    r"""The role of the messages author, in this case `system`."""
    content: CreateChatCompletionMessagesContentTypedDict
    r"""The contents of the system message."""
    name: NotRequired[str]
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""


class CreateChatCompletionMessagesSystemMessage(BaseModel):
    r"""Developer-provided instructions that the model should follow, regardless of messages sent by the user."""

    role: CreateChatCompletionMessagesRole
    r"""The role of the messages author, in this case `system`."""

    content: CreateChatCompletionMessagesContent
    r"""The contents of the system message."""

    name: Optional[str] = None
    r"""An optional name for the participant. Provides the model information to differentiate between participants of the same role."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionMessagesTypedDict = TypeAliasType(
    "CreateChatCompletionMessagesTypedDict",
    Union[
        CreateChatCompletionMessagesSystemMessageTypedDict,
        CreateChatCompletionMessagesDeveloperMessageTypedDict,
        CreateChatCompletionMessagesUserMessageTypedDict,
        CreateChatCompletionMessagesToolMessageTypedDict,
        CreateChatCompletionMessagesAssistantMessageTypedDict,
    ],
)


CreateChatCompletionMessages = Annotated[
    Union[
        Annotated[CreateChatCompletionMessagesSystemMessage, Tag("system")],
        Annotated[CreateChatCompletionMessagesDeveloperMessage, Tag("developer")],
        Annotated[CreateChatCompletionMessagesUserMessage, Tag("user")],
        Annotated[CreateChatCompletionMessagesAssistantMessage, Tag("assistant")],
        Annotated[CreateChatCompletionMessagesToolMessage, Tag("tool")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


CreateChatCompletionVoice = Literal[
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
]
r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""


CreateChatCompletionFormat = Literal[
    "wav",
    "mp3",
    "flac",
    "opus",
    "pcm16",
]
r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class CreateChatCompletionAudioTypedDict(TypedDict):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: CreateChatCompletionVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""
    format_: CreateChatCompletionFormat
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


class CreateChatCompletionAudio(BaseModel):
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    voice: CreateChatCompletionVoice
    r"""The voice the model uses to respond. Supported voices are alloy, echo, fable, onyx, nova, and shimmer."""

    format_: Annotated[CreateChatCompletionFormat, pydantic.Field(alias="format")]
    r"""Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."""


CreateChatCompletionResponseFormatRouterRequestType = Literal["json_schema",]


class CreateChatCompletionResponseFormatRouterJSONSchemaTypedDict(TypedDict):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    description: NotRequired[str]
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""
    schema_: NotRequired[Any]
    r"""The schema for the response format, described as a JSON Schema object."""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""


class CreateChatCompletionResponseFormatRouterJSONSchema(BaseModel):
    name: str
    r"""The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    description: Optional[str] = None
    r"""A description of what the response format is for, used by the model to determine how to respond in the format."""

    schema_: Annotated[Optional[Any], pydantic.Field(alias="schema")] = None
    r"""The schema for the response format, described as a JSON Schema object."""

    strict: Optional[bool] = False
    r"""Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "schema", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionResponseFormatJSONSchemaTypedDict(TypedDict):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreateChatCompletionResponseFormatRouterRequestType
    json_schema: CreateChatCompletionResponseFormatRouterJSONSchemaTypedDict


class CreateChatCompletionResponseFormatJSONSchema(BaseModel):
    r"""

    JSON Schema response format. Used to generate structured JSON responses
    """

    type: CreateChatCompletionResponseFormatRouterRequestType

    json_schema: CreateChatCompletionResponseFormatRouterJSONSchema


CreateChatCompletionResponseFormatRouterType = Literal["json_object",]


class CreateChatCompletionResponseFormatJSONObjectTypedDict(TypedDict):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreateChatCompletionResponseFormatRouterType


class CreateChatCompletionResponseFormatJSONObject(BaseModel):
    r"""

    JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
    """

    type: CreateChatCompletionResponseFormatRouterType


CreateChatCompletionResponseFormatType = Literal["text",]


class CreateChatCompletionResponseFormatTextTypedDict(TypedDict):
    r"""

    Default response format. Used to generate text responses
    """

    type: CreateChatCompletionResponseFormatType


class CreateChatCompletionResponseFormatText(BaseModel):
    r"""

    Default response format. Used to generate text responses
    """

    type: CreateChatCompletionResponseFormatType


CreateChatCompletionResponseFormatTypedDict = TypeAliasType(
    "CreateChatCompletionResponseFormatTypedDict",
    Union[
        CreateChatCompletionResponseFormatTextTypedDict,
        CreateChatCompletionResponseFormatJSONObjectTypedDict,
        CreateChatCompletionResponseFormatJSONSchemaTypedDict,
    ],
)
r"""An object specifying the format that the model must output"""


CreateChatCompletionResponseFormat = Annotated[
    Union[
        Annotated[CreateChatCompletionResponseFormatText, Tag("text")],
        Annotated[CreateChatCompletionResponseFormatJSONObject, Tag("json_object")],
        Annotated[CreateChatCompletionResponseFormatJSONSchema, Tag("json_schema")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""An object specifying the format that the model must output"""


CreateChatCompletionReasoningEffort = Literal[
    "none",
    "minimal",
    "low",
    "medium",
    "high",
    "xhigh",
]
r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

- `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
- All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
- The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
- `xhigh` is currently only supported for `gpt-5.1-codex-max`.

Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
"""


CreateChatCompletionStopTypedDict = TypeAliasType(
    "CreateChatCompletionStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


CreateChatCompletionStop = TypeAliasType(
    "CreateChatCompletionStop", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens."""


class CreateChatCompletionStreamOptionsTypedDict(TypedDict):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: NotRequired[bool]
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""


class CreateChatCompletionStreamOptions(BaseModel):
    r"""Options for streaming response. Only set this when you set stream: true."""

    include_usage: Optional[bool] = None
    r"""If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionThinkingTypedDict = TypeAliasType(
    "CreateChatCompletionThinkingTypedDict",
    Union[ThinkingConfigDisabledSchemaTypedDict, ThinkingConfigEnabledSchemaTypedDict],
)


CreateChatCompletionThinking = Annotated[
    Union[
        Annotated[ThinkingConfigDisabledSchema, Tag("disabled")],
        Annotated[ThinkingConfigEnabledSchema, Tag("enabled")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


CreateChatCompletionType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


CreateChatCompletionRouterType = Literal["object",]


class CreateChatCompletionParametersTypedDict(TypedDict):
    r"""The parameters the functions accepts, described as a JSON Schema object"""

    type: CreateChatCompletionRouterType
    properties: Dict[str, Any]
    required: NotRequired[List[str]]
    additional_properties: NotRequired[bool]


class CreateChatCompletionParameters(BaseModel):
    r"""The parameters the functions accepts, described as a JSON Schema object"""

    type: CreateChatCompletionRouterType

    properties: Dict[str, Any]

    required: Optional[List[str]] = None

    additional_properties: Annotated[
        Optional[bool], pydantic.Field(alias="additionalProperties")
    ] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["required", "additionalProperties"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to call."""
    description: NotRequired[str]
    r"""A description of what the function does, used by the model to choose when and how to call the function."""
    parameters: NotRequired[CreateChatCompletionParametersTypedDict]
    r"""The parameters the functions accepts, described as a JSON Schema object"""
    strict: NotRequired[bool]
    r"""Whether to enable strict schema adherence when generating the function call."""


class CreateChatCompletionFunction(BaseModel):
    name: str
    r"""The name of the function to call."""

    description: Optional[str] = None
    r"""A description of what the function does, used by the model to choose when and how to call the function."""

    parameters: Optional[CreateChatCompletionParameters] = None
    r"""The parameters the functions accepts, described as a JSON Schema object"""

    strict: Optional[bool] = None
    r"""Whether to enable strict schema adherence when generating the function call."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description", "parameters", "strict"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionToolsTypedDict(TypedDict):
    function: CreateChatCompletionFunctionTypedDict
    type: NotRequired[CreateChatCompletionType]
    r"""The type of the tool. Currently, only function is supported."""


class CreateChatCompletionTools(BaseModel):
    function: CreateChatCompletionFunction

    type: Optional[CreateChatCompletionType] = None
    r"""The type of the tool. Currently, only function is supported."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionToolChoiceType = Literal["function",]
r"""The type of the tool. Currently, only function is supported."""


class CreateChatCompletionToolChoiceFunctionTypedDict(TypedDict):
    name: str
    r"""The name of the function to call."""


class CreateChatCompletionToolChoiceFunction(BaseModel):
    name: str
    r"""The name of the function to call."""


class CreateChatCompletionToolChoice2TypedDict(TypedDict):
    function: CreateChatCompletionToolChoiceFunctionTypedDict
    type: NotRequired[CreateChatCompletionToolChoiceType]
    r"""The type of the tool. Currently, only function is supported."""


class CreateChatCompletionToolChoice2(BaseModel):
    function: CreateChatCompletionToolChoiceFunction

    type: Optional[CreateChatCompletionToolChoiceType] = None
    r"""The type of the tool. Currently, only function is supported."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["type"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionToolChoice1 = Literal[
    "none",
    "auto",
    "required",
]


CreateChatCompletionToolChoiceTypedDict = TypeAliasType(
    "CreateChatCompletionToolChoiceTypedDict",
    Union[CreateChatCompletionToolChoice2TypedDict, CreateChatCompletionToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


CreateChatCompletionToolChoice = TypeAliasType(
    "CreateChatCompletionToolChoice",
    Union[CreateChatCompletionToolChoice2, CreateChatCompletionToolChoice1],
)
r"""Controls which (if any) tool is called by the model."""


CreateChatCompletionModalities = Literal[
    "text",
    "audio",
]


CreateChatCompletionID1 = Literal[
    "orq_pii_detection",
    "orq_sexual_moderation",
    "orq_harmful_moderation",
]
r"""The key of the guardrail."""


CreateChatCompletionIDTypedDict = TypeAliasType(
    "CreateChatCompletionIDTypedDict", Union[CreateChatCompletionID1, str]
)


CreateChatCompletionID = TypeAliasType(
    "CreateChatCompletionID", Union[CreateChatCompletionID1, str]
)


CreateChatCompletionExecuteOn = Literal[
    "input",
    "output",
]
r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateChatCompletionGuardrailsTypedDict(TypedDict):
    id: CreateChatCompletionIDTypedDict
    execute_on: CreateChatCompletionExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateChatCompletionGuardrails(BaseModel):
    id: CreateChatCompletionID

    execute_on: CreateChatCompletionExecuteOn
    r"""Determines whether the guardrail runs on the input (user message) or output (model response)."""


class CreateChatCompletionRetryTypedDict(TypedDict):
    r"""Retry configuration for the request"""

    count: NotRequired[float]
    r"""Number of retry attempts (1-5)"""
    on_codes: NotRequired[List[float]]
    r"""HTTP status codes that trigger retry logic"""


class CreateChatCompletionRetry(BaseModel):
    r"""Retry configuration for the request"""

    count: Optional[float] = 3
    r"""Number of retry attempts (1-5)"""

    on_codes: Optional[List[float]] = None
    r"""HTTP status codes that trigger retry logic"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["count", "on_codes"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class FallbacksTypedDict(TypedDict):
    model: str
    r"""Fallback model identifier"""


class Fallbacks(BaseModel):
    model: str
    r"""Fallback model identifier"""


Version = Literal["latest",]
r"""Version of the prompt to use (currently only \"latest\" supported)"""


class PromptTypedDict(TypedDict):
    r"""Prompt configuration for the request"""

    id: str
    r"""Unique identifier of the prompt to use"""
    version: Version
    r"""Version of the prompt to use (currently only \"latest\" supported)"""


class Prompt(BaseModel):
    r"""Prompt configuration for the request"""

    id: str
    r"""Unique identifier of the prompt to use"""

    version: Version
    r"""Version of the prompt to use (currently only \"latest\" supported)"""


class CreateChatCompletionThreadTypedDict(TypedDict):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""
    tags: NotRequired[List[str]]
    r"""Optional tags to differentiate or categorize threads"""


class CreateChatCompletionThread(BaseModel):
    r"""Thread information to group related requests"""

    id: str
    r"""Unique thread identifier to group related invocations."""

    tags: Optional[List[str]] = None
    r"""Optional tags to differentiate or categorize threads"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["tags"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class Inputs2TypedDict(TypedDict):
    key: str
    value: NotRequired[Any]
    is_pii: NotRequired[bool]


class Inputs2(BaseModel):
    key: str

    value: Optional[Any] = None

    is_pii: Optional[bool] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["value", "is_pii"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


InputsTypedDict = TypeAliasType(
    "InputsTypedDict", Union[Dict[str, Any], List[Inputs2TypedDict]]
)
r"""Values to replace in the prompt messages using {{variableName}} syntax"""


Inputs = TypeAliasType("Inputs", Union[Dict[str, Any], List[Inputs2]])
r"""Values to replace in the prompt messages using {{variableName}} syntax"""


CreateChatCompletionRouterRequestType = Literal["exact_match",]


class CacheTypedDict(TypedDict):
    r"""Cache configuration for the request."""

    type: CreateChatCompletionRouterRequestType
    ttl: NotRequired[float]
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""


class Cache(BaseModel):
    r"""Cache configuration for the request."""

    type: CreateChatCompletionRouterRequestType

    ttl: Optional[float] = 1800
    r"""Time to live for cached responses in seconds. Maximum 259200 seconds (3 days)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["ttl"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionSearchType = Literal[
    "vector_search",
    "keyword_search",
    "hybrid_search",
]
r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""


class CreateChatCompletionOrExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class CreateChatCompletionOrExists(BaseModel):
    r"""Exists"""

    exists: bool


CreateChatCompletionOrRouterNinTypedDict = TypeAliasType(
    "CreateChatCompletionOrRouterNinTypedDict", Union[str, float, bool]
)


CreateChatCompletionOrRouterNin = TypeAliasType(
    "CreateChatCompletionOrRouterNin", Union[str, float, bool]
)


class CreateChatCompletionOrNinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[CreateChatCompletionOrRouterNinTypedDict]


class CreateChatCompletionOrNin(BaseModel):
    r"""Not in"""

    nin: List[CreateChatCompletionOrRouterNin]


CreateChatCompletionOrRouterInTypedDict = TypeAliasType(
    "CreateChatCompletionOrRouterInTypedDict", Union[str, float, bool]
)


CreateChatCompletionOrRouterIn = TypeAliasType(
    "CreateChatCompletionOrRouterIn", Union[str, float, bool]
)


class CreateChatCompletionOrInTypedDict(TypedDict):
    r"""In"""

    in_: List[CreateChatCompletionOrRouterInTypedDict]


class CreateChatCompletionOrIn(BaseModel):
    r"""In"""

    in_: Annotated[List[CreateChatCompletionOrRouterIn], pydantic.Field(alias="in")]


class CreateChatCompletionOrLteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletionOrLte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletionOrLtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class CreateChatCompletionOrLt(BaseModel):
    r"""Less than"""

    lt: float


class CreateChatCompletionOrGteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletionOrGte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletionOrGtTypedDict(TypedDict):
    r"""Greater than"""

    gt: float


class CreateChatCompletionOrGt(BaseModel):
    r"""Greater than"""

    gt: float


CreateChatCompletionOrRouterNeTypedDict = TypeAliasType(
    "CreateChatCompletionOrRouterNeTypedDict", Union[str, float, bool]
)


CreateChatCompletionOrRouterNe = TypeAliasType(
    "CreateChatCompletionOrRouterNe", Union[str, float, bool]
)


class CreateChatCompletionOrNeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: CreateChatCompletionOrRouterNeTypedDict


class CreateChatCompletionOrNe(BaseModel):
    r"""Not equal to"""

    ne: CreateChatCompletionOrRouterNe


CreateChatCompletionOrRouterEqTypedDict = TypeAliasType(
    "CreateChatCompletionOrRouterEqTypedDict", Union[str, float, bool]
)


CreateChatCompletionOrRouterEq = TypeAliasType(
    "CreateChatCompletionOrRouterEq", Union[str, float, bool]
)


class CreateChatCompletionOrEqTypedDict(TypedDict):
    r"""Equal to"""

    eq: CreateChatCompletionOrRouterEqTypedDict


class CreateChatCompletionOrEq(BaseModel):
    r"""Equal to"""

    eq: CreateChatCompletionOrRouterEq


CreateChatCompletionFilterByRouterOrTypedDict = TypeAliasType(
    "CreateChatCompletionFilterByRouterOrTypedDict",
    Union[
        CreateChatCompletionOrEqTypedDict,
        CreateChatCompletionOrNeTypedDict,
        CreateChatCompletionOrGtTypedDict,
        CreateChatCompletionOrGteTypedDict,
        CreateChatCompletionOrLtTypedDict,
        CreateChatCompletionOrLteTypedDict,
        CreateChatCompletionOrInTypedDict,
        CreateChatCompletionOrNinTypedDict,
        CreateChatCompletionOrExistsTypedDict,
    ],
)


CreateChatCompletionFilterByRouterOr = TypeAliasType(
    "CreateChatCompletionFilterByRouterOr",
    Union[
        CreateChatCompletionOrEq,
        CreateChatCompletionOrNe,
        CreateChatCompletionOrGt,
        CreateChatCompletionOrGte,
        CreateChatCompletionOrLt,
        CreateChatCompletionOrLte,
        CreateChatCompletionOrIn,
        CreateChatCompletionOrNin,
        CreateChatCompletionOrExists,
    ],
)


class CreateChatCompletionFilterByOrTypedDict(TypedDict):
    r"""Or"""

    or_: List[Dict[str, CreateChatCompletionFilterByRouterOrTypedDict]]


class CreateChatCompletionFilterByOr(BaseModel):
    r"""Or"""

    or_: Annotated[
        List[Dict[str, CreateChatCompletionFilterByRouterOr]],
        pydantic.Field(alias="or"),
    ]


class CreateChatCompletionAndExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class CreateChatCompletionAndExists(BaseModel):
    r"""Exists"""

    exists: bool


CreateChatCompletionAndRouterNinTypedDict = TypeAliasType(
    "CreateChatCompletionAndRouterNinTypedDict", Union[str, float, bool]
)


CreateChatCompletionAndRouterNin = TypeAliasType(
    "CreateChatCompletionAndRouterNin", Union[str, float, bool]
)


class CreateChatCompletionAndNinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[CreateChatCompletionAndRouterNinTypedDict]


class CreateChatCompletionAndNin(BaseModel):
    r"""Not in"""

    nin: List[CreateChatCompletionAndRouterNin]


CreateChatCompletionAndRouterInTypedDict = TypeAliasType(
    "CreateChatCompletionAndRouterInTypedDict", Union[str, float, bool]
)


CreateChatCompletionAndRouterIn = TypeAliasType(
    "CreateChatCompletionAndRouterIn", Union[str, float, bool]
)


class CreateChatCompletionAndInTypedDict(TypedDict):
    r"""In"""

    in_: List[CreateChatCompletionAndRouterInTypedDict]


class CreateChatCompletionAndIn(BaseModel):
    r"""In"""

    in_: Annotated[List[CreateChatCompletionAndRouterIn], pydantic.Field(alias="in")]


class CreateChatCompletionAndLteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletionAndLte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletionAndLtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class CreateChatCompletionAndLt(BaseModel):
    r"""Less than"""

    lt: float


class CreateChatCompletionAndGteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletionAndGte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletionAndGtTypedDict(TypedDict):
    r"""Greater than"""

    gt: float


class CreateChatCompletionAndGt(BaseModel):
    r"""Greater than"""

    gt: float


CreateChatCompletionAndRouterNeTypedDict = TypeAliasType(
    "CreateChatCompletionAndRouterNeTypedDict", Union[str, float, bool]
)


CreateChatCompletionAndRouterNe = TypeAliasType(
    "CreateChatCompletionAndRouterNe", Union[str, float, bool]
)


class CreateChatCompletionAndNeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: CreateChatCompletionAndRouterNeTypedDict


class CreateChatCompletionAndNe(BaseModel):
    r"""Not equal to"""

    ne: CreateChatCompletionAndRouterNe


CreateChatCompletionAndRouterEqTypedDict = TypeAliasType(
    "CreateChatCompletionAndRouterEqTypedDict", Union[str, float, bool]
)


CreateChatCompletionAndRouterEq = TypeAliasType(
    "CreateChatCompletionAndRouterEq", Union[str, float, bool]
)


class CreateChatCompletionAndEqTypedDict(TypedDict):
    r"""Equal to"""

    eq: CreateChatCompletionAndRouterEqTypedDict


class CreateChatCompletionAndEq(BaseModel):
    r"""Equal to"""

    eq: CreateChatCompletionAndRouterEq


CreateChatCompletionFilterByRouterAndTypedDict = TypeAliasType(
    "CreateChatCompletionFilterByRouterAndTypedDict",
    Union[
        CreateChatCompletionAndEqTypedDict,
        CreateChatCompletionAndNeTypedDict,
        CreateChatCompletionAndGtTypedDict,
        CreateChatCompletionAndGteTypedDict,
        CreateChatCompletionAndLtTypedDict,
        CreateChatCompletionAndLteTypedDict,
        CreateChatCompletionAndInTypedDict,
        CreateChatCompletionAndNinTypedDict,
        CreateChatCompletionAndExistsTypedDict,
    ],
)


CreateChatCompletionFilterByRouterAnd = TypeAliasType(
    "CreateChatCompletionFilterByRouterAnd",
    Union[
        CreateChatCompletionAndEq,
        CreateChatCompletionAndNe,
        CreateChatCompletionAndGt,
        CreateChatCompletionAndGte,
        CreateChatCompletionAndLt,
        CreateChatCompletionAndLte,
        CreateChatCompletionAndIn,
        CreateChatCompletionAndNin,
        CreateChatCompletionAndExists,
    ],
)


class CreateChatCompletionFilterByAndTypedDict(TypedDict):
    r"""And"""

    and_: List[Dict[str, CreateChatCompletionFilterByRouterAndTypedDict]]


class CreateChatCompletionFilterByAnd(BaseModel):
    r"""And"""

    and_: Annotated[
        List[Dict[str, CreateChatCompletionFilterByRouterAnd]],
        pydantic.Field(alias="and"),
    ]


class CreateChatCompletion1ExistsTypedDict(TypedDict):
    r"""Exists"""

    exists: bool


class CreateChatCompletion1Exists(BaseModel):
    r"""Exists"""

    exists: bool


CreateChatCompletion1RouterNinTypedDict = TypeAliasType(
    "CreateChatCompletion1RouterNinTypedDict", Union[str, float, bool]
)


CreateChatCompletion1RouterNin = TypeAliasType(
    "CreateChatCompletion1RouterNin", Union[str, float, bool]
)


class CreateChatCompletion1NinTypedDict(TypedDict):
    r"""Not in"""

    nin: List[CreateChatCompletion1RouterNinTypedDict]


class CreateChatCompletion1Nin(BaseModel):
    r"""Not in"""

    nin: List[CreateChatCompletion1RouterNin]


CreateChatCompletion1RouterInTypedDict = TypeAliasType(
    "CreateChatCompletion1RouterInTypedDict", Union[str, float, bool]
)


CreateChatCompletion1RouterIn = TypeAliasType(
    "CreateChatCompletion1RouterIn", Union[str, float, bool]
)


class CreateChatCompletion1InTypedDict(TypedDict):
    r"""In"""

    in_: List[CreateChatCompletion1RouterInTypedDict]


class CreateChatCompletion1In(BaseModel):
    r"""In"""

    in_: Annotated[List[CreateChatCompletion1RouterIn], pydantic.Field(alias="in")]


class CreateChatCompletion1LteTypedDict(TypedDict):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletion1Lte(BaseModel):
    r"""Less than or equal to"""

    lte: float


class CreateChatCompletion1LtTypedDict(TypedDict):
    r"""Less than"""

    lt: float


class CreateChatCompletion1Lt(BaseModel):
    r"""Less than"""

    lt: float


class CreateChatCompletion1GteTypedDict(TypedDict):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletion1Gte(BaseModel):
    r"""Greater than or equal to"""

    gte: float


class CreateChatCompletion1GtTypedDict(TypedDict):
    r"""Greater than"""

    gt: float


class CreateChatCompletion1Gt(BaseModel):
    r"""Greater than"""

    gt: float


CreateChatCompletion1RouterNeTypedDict = TypeAliasType(
    "CreateChatCompletion1RouterNeTypedDict", Union[str, float, bool]
)


CreateChatCompletion1RouterNe = TypeAliasType(
    "CreateChatCompletion1RouterNe", Union[str, float, bool]
)


class CreateChatCompletion1NeTypedDict(TypedDict):
    r"""Not equal to"""

    ne: CreateChatCompletion1RouterNeTypedDict


class CreateChatCompletion1Ne(BaseModel):
    r"""Not equal to"""

    ne: CreateChatCompletion1RouterNe


CreateChatCompletion1RouterEqTypedDict = TypeAliasType(
    "CreateChatCompletion1RouterEqTypedDict", Union[str, float, bool]
)


CreateChatCompletion1RouterEq = TypeAliasType(
    "CreateChatCompletion1RouterEq", Union[str, float, bool]
)


class CreateChatCompletion1EqTypedDict(TypedDict):
    r"""Equal to"""

    eq: CreateChatCompletion1RouterEqTypedDict


class CreateChatCompletion1Eq(BaseModel):
    r"""Equal to"""

    eq: CreateChatCompletion1RouterEq


CreateChatCompletionFilterBy1TypedDict = TypeAliasType(
    "CreateChatCompletionFilterBy1TypedDict",
    Union[
        CreateChatCompletion1EqTypedDict,
        CreateChatCompletion1NeTypedDict,
        CreateChatCompletion1GtTypedDict,
        CreateChatCompletion1GteTypedDict,
        CreateChatCompletion1LtTypedDict,
        CreateChatCompletion1LteTypedDict,
        CreateChatCompletion1InTypedDict,
        CreateChatCompletion1NinTypedDict,
        CreateChatCompletion1ExistsTypedDict,
    ],
)


CreateChatCompletionFilterBy1 = TypeAliasType(
    "CreateChatCompletionFilterBy1",
    Union[
        CreateChatCompletion1Eq,
        CreateChatCompletion1Ne,
        CreateChatCompletion1Gt,
        CreateChatCompletion1Gte,
        CreateChatCompletion1Lt,
        CreateChatCompletion1Lte,
        CreateChatCompletion1In,
        CreateChatCompletion1Nin,
        CreateChatCompletion1Exists,
    ],
)


CreateChatCompletionFilterByTypedDict = TypeAliasType(
    "CreateChatCompletionFilterByTypedDict",
    Union[
        CreateChatCompletionFilterByAndTypedDict,
        CreateChatCompletionFilterByOrTypedDict,
        Dict[str, CreateChatCompletionFilterBy1TypedDict],
    ],
)
r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""


CreateChatCompletionFilterBy = TypeAliasType(
    "CreateChatCompletionFilterBy",
    Union[
        CreateChatCompletionFilterByAnd,
        CreateChatCompletionFilterByOr,
        Dict[str, CreateChatCompletionFilterBy1],
    ],
)
r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""


class CreateChatCompletionSearchOptionsTypedDict(TypedDict):
    r"""Additional search options"""

    include_vectors: NotRequired[bool]
    r"""Whether to include the vector in the chunk"""
    include_metadata: NotRequired[bool]
    r"""Whether to include the metadata in the chunk"""
    include_scores: NotRequired[bool]
    r"""Whether to include the scores in the chunk"""


class CreateChatCompletionSearchOptions(BaseModel):
    r"""Additional search options"""

    include_vectors: Optional[bool] = None
    r"""Whether to include the vector in the chunk"""

    include_metadata: Optional[bool] = None
    r"""Whether to include the metadata in the chunk"""

    include_scores: Optional[bool] = None
    r"""Whether to include the scores in the chunk"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["include_vectors", "include_metadata", "include_scores"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionRerankConfigTypedDict(TypedDict):
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    model: str
    r"""The name of the rerank model to use. Refer to the [model list](https://docs.orq.ai/docs/proxy#/rerank-models)."""
    threshold: NotRequired[float]
    r"""The threshold value used to filter the rerank results, only documents with a relevance score greater than the threshold will be returned"""
    top_k: NotRequired[int]
    r"""The number of top results to return after reranking. If not provided, will default to the knowledge base configured `top_k`."""


class CreateChatCompletionRerankConfig(BaseModel):
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    model: str
    r"""The name of the rerank model to use. Refer to the [model list](https://docs.orq.ai/docs/proxy#/rerank-models)."""

    threshold: Optional[float] = 0
    r"""The threshold value used to filter the rerank results, only documents with a relevance score greater than the threshold will be returned"""

    top_k: Optional[int] = 10
    r"""The number of top results to return after reranking. If not provided, will default to the knowledge base configured `top_k`."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["threshold", "top_k"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionAgenticRagConfigTypedDict(TypedDict):
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    model: str
    r"""The name of the model for the Agent to use. Refer to the [model list](https://docs.orq.ai/docs/proxy#/chat-models)."""


class CreateChatCompletionAgenticRagConfig(BaseModel):
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    model: str
    r"""The name of the model for the Agent to use. Refer to the [model list](https://docs.orq.ai/docs/proxy#/chat-models)."""


class CreateChatCompletionKnowledgeBasesTypedDict(TypedDict):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""
    top_k: NotRequired[int]
    r"""The number of results to return. If not provided, will default to the knowledge base configured `top_k`."""
    threshold: NotRequired[float]
    r"""The threshold to apply to the search. If not provided, will default to the knowledge base configured `threshold`"""
    search_type: NotRequired[CreateChatCompletionSearchType]
    r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""
    filter_by: NotRequired[CreateChatCompletionFilterByTypedDict]
    r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""
    search_options: NotRequired[CreateChatCompletionSearchOptionsTypedDict]
    r"""Additional search options"""
    rerank_config: NotRequired[CreateChatCompletionRerankConfigTypedDict]
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""
    agentic_rag_config: NotRequired[CreateChatCompletionAgenticRagConfigTypedDict]
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""
    query: NotRequired[str]
    r"""The query to use to search the knowledge base. If not provided we will use the last user message from the messages of the requests"""


class CreateChatCompletionKnowledgeBases(BaseModel):
    knowledge_id: str
    r"""Unique identifier of the knowledge base to search"""

    top_k: Optional[int] = None
    r"""The number of results to return. If not provided, will default to the knowledge base configured `top_k`."""

    threshold: Optional[float] = None
    r"""The threshold to apply to the search. If not provided, will default to the knowledge base configured `threshold`"""

    search_type: Optional[CreateChatCompletionSearchType] = "hybrid_search"
    r"""The type of search to perform. If not provided, will default to the knowledge base configured `retrieval_type`"""

    filter_by: Optional[CreateChatCompletionFilterBy] = None
    r"""The metadata filter to apply to the search. Check the [Searching a Knowledge Base](https://dash.readme.com/project/orqai/v2.0/docs/searching-a-knowledge-base) for more information."""

    search_options: Optional[CreateChatCompletionSearchOptions] = None
    r"""Additional search options"""

    rerank_config: Optional[CreateChatCompletionRerankConfig] = None
    r"""Override the rerank configuration for this search. If not provided, will use the knowledge base configured rerank settings."""

    agentic_rag_config: Optional[CreateChatCompletionAgenticRagConfig] = None
    r"""Override the agentic RAG configuration for this search. If not provided, will use the knowledge base configured agentic RAG settings."""

    query: Optional[str] = None
    r"""The query to use to search the knowledge base. If not provided we will use the last user message from the messages of the requests"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "top_k",
                "threshold",
                "search_type",
                "filter_by",
                "search_options",
                "rerank_config",
                "agentic_rag_config",
                "query",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


LoadBalancerType = Literal["weight_based",]


class LoadBalancer1TypedDict(TypedDict):
    type: LoadBalancerType
    model: str
    r"""Model identifier for load balancing"""
    weight: NotRequired[float]
    r"""Weight assigned to this model for load balancing"""


class LoadBalancer1(BaseModel):
    type: LoadBalancerType

    model: str
    r"""Model identifier for load balancing"""

    weight: Optional[float] = 0.5
    r"""Weight assigned to this model for load balancing"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["weight"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


LoadBalancerTypedDict = LoadBalancer1TypedDict


LoadBalancer = LoadBalancer1


class TimeoutTypedDict(TypedDict):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class Timeout(BaseModel):
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    call_timeout: float
    r"""Timeout value in milliseconds"""


class CreateChatCompletionOrqTypedDict(TypedDict):
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    name: NotRequired[str]
    r"""The name to display on the trace. If not specified, the default system name will be used."""
    retry: NotRequired[CreateChatCompletionRetryTypedDict]
    r"""Retry configuration for the request"""
    fallbacks: NotRequired[List[FallbacksTypedDict]]
    r"""Array of fallback models to use if primary model fails"""
    prompt: NotRequired[PromptTypedDict]
    r"""Prompt configuration for the request"""
    contact: NotRequired[PublicContactTypedDict]
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""
    thread: NotRequired[CreateChatCompletionThreadTypedDict]
    r"""Thread information to group related requests"""
    inputs: NotRequired[InputsTypedDict]
    r"""Values to replace in the prompt messages using {{variableName}} syntax"""
    cache: NotRequired[CacheTypedDict]
    r"""Cache configuration for the request."""
    knowledge_bases: NotRequired[List[CreateChatCompletionKnowledgeBasesTypedDict]]
    load_balancer: NotRequired[List[LoadBalancerTypedDict]]
    r"""Array of models with weights for load balancing requests"""
    timeout: NotRequired[TimeoutTypedDict]
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""


class CreateChatCompletionOrq(BaseModel):
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    name: Optional[str] = None
    r"""The name to display on the trace. If not specified, the default system name will be used."""

    retry: Optional[CreateChatCompletionRetry] = None
    r"""Retry configuration for the request"""

    fallbacks: Optional[List[Fallbacks]] = None
    r"""Array of fallback models to use if primary model fails"""

    prompt: Optional[Prompt] = None
    r"""Prompt configuration for the request"""

    contact: Optional[PublicContact] = None
    r"""Information about the contact making the request. If the contact does not exist, it will be created automatically."""

    thread: Optional[CreateChatCompletionThread] = None
    r"""Thread information to group related requests"""

    inputs: Optional[Inputs] = None
    r"""Values to replace in the prompt messages using {{variableName}} syntax"""

    cache: Optional[Cache] = None
    r"""Cache configuration for the request."""

    knowledge_bases: Optional[List[CreateChatCompletionKnowledgeBases]] = None

    load_balancer: Optional[List[LoadBalancer]] = None
    r"""Array of models with weights for load balancing requests"""

    timeout: Optional[Timeout] = None
    r"""Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "name",
                "retry",
                "fallbacks",
                "prompt",
                "contact",
                "thread",
                "inputs",
                "cache",
                "knowledge_bases",
                "load_balancer",
                "timeout",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionRequestBodyTypedDict(TypedDict):
    messages: List[CreateChatCompletionMessagesTypedDict]
    r"""A list of messages comprising the conversation so far."""
    model: str
    r"""Model ID used to generate the response, like `openai/gpt-4o` or `anthropic/claude-haiku-4-5-20251001`. The AI Gateway offers a wide range of models with different capabilities, performance characteristics, and price points. Refer to the (Supported models)[/docs/proxy/supported-models] to browse available models."""
    metadata: NotRequired[Dict[str, str]]
    r"""Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can have a maximum length of 64 characters and values can have a maximum length of 512 characters."""
    audio: NotRequired[Nullable[CreateChatCompletionAudioTypedDict]]
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""
    max_tokens: NotRequired[Nullable[int]]
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""
    logprobs: NotRequired[Nullable[bool]]
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""
    response_format: NotRequired[CreateChatCompletionResponseFormatTypedDict]
    r"""An object specifying the format that the model must output"""
    reasoning_effort: NotRequired[CreateChatCompletionReasoningEffort]
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """
    verbosity: NotRequired[str]
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""
    seed: NotRequired[Nullable[float]]
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""
    stop: NotRequired[Nullable[CreateChatCompletionStopTypedDict]]
    r"""Up to 4 sequences where the API will stop generating further tokens."""
    stream_options: NotRequired[Nullable[CreateChatCompletionStreamOptionsTypedDict]]
    r"""Options for streaming response. Only set this when you set stream: true."""
    thinking: NotRequired[CreateChatCompletionThinkingTypedDict]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    top_k: NotRequired[Nullable[float]]
    r"""Limits the model to consider only the top k most likely tokens at each step."""
    tools: NotRequired[List[CreateChatCompletionToolsTypedDict]]
    r"""A list of tools the model may call."""
    tool_choice: NotRequired[CreateChatCompletionToolChoiceTypedDict]
    r"""Controls which (if any) tool is called by the model."""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use."""
    modalities: NotRequired[Nullable[List[CreateChatCompletionModalities]]]
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""
    guardrails: NotRequired[List[CreateChatCompletionGuardrailsTypedDict]]
    r"""A list of guardrails to apply to the request."""
    orq: NotRequired[CreateChatCompletionOrqTypedDict]
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""
    stream: NotRequired[bool]


class CreateChatCompletionRequestBody(BaseModel):
    messages: List[CreateChatCompletionMessages]
    r"""A list of messages comprising the conversation so far."""

    model: str
    r"""Model ID used to generate the response, like `openai/gpt-4o` or `anthropic/claude-haiku-4-5-20251001`. The AI Gateway offers a wide range of models with different capabilities, performance characteristics, and price points. Refer to the (Supported models)[/docs/proxy/supported-models] to browse available models."""

    metadata: Optional[Dict[str, str]] = None
    r"""Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can have a maximum length of 64 characters and values can have a maximum length of 512 characters."""

    audio: OptionalNullable[CreateChatCompletionAudio] = UNSET
    r"""Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more."""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""`[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

    This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens"""

    logprobs: OptionalNullable[bool] = UNSET
    r"""Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."""

    response_format: Optional[CreateChatCompletionResponseFormat] = None
    r"""An object specifying the format that the model must output"""

    reasoning_effort: Optional[CreateChatCompletionReasoningEffort] = None
    r"""Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

    - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
    - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
    - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
    - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

    Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
    """

    verbosity: Optional[str] = None
    r"""Adjusts response verbosity. Lower levels yield shorter answers."""

    seed: OptionalNullable[float] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."""

    stop: OptionalNullable[CreateChatCompletionStop] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens."""

    stream_options: OptionalNullable[CreateChatCompletionStreamOptions] = UNSET
    r"""Options for streaming response. Only set this when you set stream: true."""

    thinking: Optional[CreateChatCompletionThinking] = None

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""

    top_k: OptionalNullable[float] = UNSET
    r"""Limits the model to consider only the top k most likely tokens at each step."""

    tools: Optional[List[CreateChatCompletionTools]] = None
    r"""A list of tools the model may call."""

    tool_choice: Optional[CreateChatCompletionToolChoice] = None
    r"""Controls which (if any) tool is called by the model."""

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use."""

    modalities: OptionalNullable[List[CreateChatCompletionModalities]] = UNSET
    r"""Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"]."""

    guardrails: Optional[List[CreateChatCompletionGuardrails]] = None
    r"""A list of guardrails to apply to the request."""

    orq: Optional[CreateChatCompletionOrq] = None
    r"""Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, contact-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution."""

    stream: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "metadata",
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "response_format",
                "reasoning_effort",
                "verbosity",
                "seed",
                "stop",
                "stream_options",
                "thinking",
                "temperature",
                "top_p",
                "top_k",
                "tools",
                "tool_choice",
                "parallel_tool_calls",
                "modalities",
                "guardrails",
                "orq",
                "stream",
            ]
        )
        nullable_fields = set(
            [
                "audio",
                "frequency_penalty",
                "max_tokens",
                "max_completion_tokens",
                "logprobs",
                "top_logprobs",
                "n",
                "presence_penalty",
                "seed",
                "stop",
                "stream_options",
                "temperature",
                "top_p",
                "top_k",
                "modalities",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateChatCompletionRouterFinishReason = Literal[
    "stop",
    "length",
    "tool_calls",
    "content_filter",
    "function_call",
]
r"""The reason the model stopped generating tokens."""


class CreateChatCompletionRouterResponseTopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class CreateChatCompletionRouterResponseTopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateChatCompletionRouterContentTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[CreateChatCompletionRouterResponseTopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class CreateChatCompletionRouterContent(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[CreateChatCompletionRouterResponseTopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateChatCompletionRouterResponse200TopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class CreateChatCompletionRouterResponse200TopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateChatCompletionRouterRefusalTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[CreateChatCompletionRouterResponse200TopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class CreateChatCompletionRouterRefusal(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[CreateChatCompletionRouterResponse200TopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateChatCompletionRouterLogprobsTypedDict(TypedDict):
    r"""Log probability information for the choice."""

    content: Nullable[List[CreateChatCompletionRouterContentTypedDict]]
    r"""A list of message content tokens with log probability information."""
    refusal: Nullable[List[CreateChatCompletionRouterRefusalTypedDict]]
    r"""A list of message refusal tokens with log probability information."""


class CreateChatCompletionRouterLogprobs(BaseModel):
    r"""Log probability information for the choice."""

    content: Nullable[List[CreateChatCompletionRouterContent]]
    r"""A list of message content tokens with log probability information."""

    refusal: Nullable[List[CreateChatCompletionRouterRefusal]]
    r"""A list of message refusal tokens with log probability information."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


CreateChatCompletionRouterResponse200Type = Literal["function",]
r"""The type of the tool. Currently, only `function` is supported."""


class CreateChatCompletionRouterResponseFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreateChatCompletionRouterResponseFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name", "arguments"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionRouterToolCallsTypedDict(TypedDict):
    index: NotRequired[float]
    r"""The index of the tool call."""
    id: NotRequired[str]
    r"""The ID of the tool call."""
    type: NotRequired[CreateChatCompletionRouterResponse200Type]
    r"""The type of the tool. Currently, only `function` is supported."""
    function: NotRequired[CreateChatCompletionRouterResponseFunctionTypedDict]
    thought_signature: NotRequired[str]
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models."""


class CreateChatCompletionRouterToolCalls(BaseModel):
    index: Optional[float] = None
    r"""The index of the tool call."""

    id: Optional[str] = None
    r"""The ID of the tool call."""

    type: Optional[CreateChatCompletionRouterResponse200Type] = None
    r"""The type of the tool. Currently, only `function` is supported."""

    function: Optional[CreateChatCompletionRouterResponseFunction] = None

    thought_signature: Optional[str] = None
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["index", "id", "type", "function", "thought_signature"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionRouterRole = Literal["assistant",]


class CreateChatCompletionRouterResponseAudioTypedDict(TypedDict):
    r"""Audio response data in streaming mode."""

    id: NotRequired[str]
    transcript: NotRequired[str]
    data: NotRequired[str]
    expires_at: NotRequired[int]


class CreateChatCompletionRouterResponseAudio(BaseModel):
    r"""Audio response data in streaming mode."""

    id: Optional[str] = None

    transcript: Optional[str] = None

    data: Optional[str] = None

    expires_at: Optional[int] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["id", "transcript", "data", "expires_at"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class DeltaTypedDict(TypedDict):
    r"""A chat completion delta generated by streamed model responses."""

    content: NotRequired[Nullable[str]]
    r"""The contents of the chunk message."""
    refusal: NotRequired[Nullable[str]]
    tool_calls: NotRequired[List[CreateChatCompletionRouterToolCallsTypedDict]]
    role: NotRequired[CreateChatCompletionRouterRole]
    reasoning: NotRequired[str]
    r"""Internal thought process of the model"""
    reasoning_signature: NotRequired[str]
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""
    redacted_reasoning: NotRequired[str]
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""
    audio: NotRequired[Nullable[CreateChatCompletionRouterResponseAudioTypedDict]]
    r"""Audio response data in streaming mode."""


class Delta(BaseModel):
    r"""A chat completion delta generated by streamed model responses."""

    content: OptionalNullable[str] = UNSET
    r"""The contents of the chunk message."""

    refusal: OptionalNullable[str] = UNSET

    tool_calls: Optional[List[CreateChatCompletionRouterToolCalls]] = None

    role: Optional[CreateChatCompletionRouterRole] = None

    reasoning: Optional[str] = None
    r"""Internal thought process of the model"""

    reasoning_signature: Optional[str] = None
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""

    redacted_reasoning: Optional[str] = None
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""

    audio: OptionalNullable[CreateChatCompletionRouterResponseAudio] = UNSET
    r"""Audio response data in streaming mode."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "content",
                "refusal",
                "tool_calls",
                "role",
                "reasoning",
                "reasoning_signature",
                "redacted_reasoning",
                "audio",
            ]
        )
        nullable_fields = set(["content", "refusal", "audio"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateChatCompletionRouterChoicesTypedDict(TypedDict):
    finish_reason: Nullable[CreateChatCompletionRouterFinishReason]
    r"""The reason the model stopped generating tokens."""
    delta: DeltaTypedDict
    r"""A chat completion delta generated by streamed model responses."""
    index: NotRequired[float]
    r"""The index of the choice in the list of choices."""
    logprobs: NotRequired[Nullable[CreateChatCompletionRouterLogprobsTypedDict]]
    r"""Log probability information for the choice."""


class CreateChatCompletionRouterChoices(BaseModel):
    finish_reason: Nullable[CreateChatCompletionRouterFinishReason]
    r"""The reason the model stopped generating tokens."""

    delta: Delta
    r"""A chat completion delta generated by streamed model responses."""

    index: Optional[float] = 0
    r"""The index of the choice in the list of choices."""

    logprobs: OptionalNullable[CreateChatCompletionRouterLogprobs] = UNSET
    r"""Log probability information for the choice."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["index", "logprobs"])
        nullable_fields = set(["finish_reason", "logprobs"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateChatCompletionRouterPromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[int]]
    cache_creation_tokens: NotRequired[Nullable[int]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio input tokens consumed by the request."""


class CreateChatCompletionRouterPromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[int] = UNSET

    cache_creation_tokens: OptionalNullable[int] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio input tokens consumed by the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["cached_tokens", "cache_creation_tokens", "audio_tokens"]
        )
        nullable_fields = set(
            ["cached_tokens", "cache_creation_tokens", "audio_tokens"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateChatCompletionRouterCompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]
    accepted_prediction_tokens: NotRequired[Nullable[float]]
    rejected_prediction_tokens: NotRequired[Nullable[float]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio output tokens produced by the response."""


class CreateChatCompletionRouterCompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    accepted_prediction_tokens: OptionalNullable[float] = UNSET

    rejected_prediction_tokens: OptionalNullable[float] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio output tokens produced by the response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "reasoning_tokens",
                "accepted_prediction_tokens",
                "rejected_prediction_tokens",
                "audio_tokens",
            ]
        )
        nullable_fields = set(
            [
                "reasoning_tokens",
                "accepted_prediction_tokens",
                "rejected_prediction_tokens",
                "audio_tokens",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateChatCompletionRouterUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""
    prompt_tokens_details: NotRequired[
        Nullable[CreateChatCompletionRouterPromptTokensDetailsTypedDict]
    ]
    completion_tokens_details: NotRequired[
        Nullable[CreateChatCompletionRouterCompletionTokensDetailsTypedDict]
    ]


class CreateChatCompletionRouterUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""

    prompt_tokens_details: OptionalNullable[
        CreateChatCompletionRouterPromptTokensDetails
    ] = UNSET

    completion_tokens_details: OptionalNullable[
        CreateChatCompletionRouterCompletionTokensDetails
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "completion_tokens",
                "prompt_tokens",
                "total_tokens",
                "prompt_tokens_details",
                "completion_tokens_details",
            ]
        )
        nullable_fields = set(["prompt_tokens_details", "completion_tokens_details"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateChatCompletionRouterObject = Literal["chat.completion.chunk",]


class CreateChatCompletionDataTypedDict(TypedDict):
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    id: str
    r"""A unique identifier for the chat completion."""
    choices: List[CreateChatCompletionRouterChoicesTypedDict]
    r"""A list of chat completion choices. Can contain more than one elements if n is greater than 1. Can also be empty for the last chunk if you set stream_options: {\"include_usage\": true}."""
    created: float
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    model: str
    r"""The model used for the chat completion."""
    object: CreateChatCompletionRouterObject
    system_fingerprint: NotRequired[Nullable[str]]
    r"""This fingerprint represents the backend configuration that the model runs with."""
    usage: NotRequired[Nullable[CreateChatCompletionRouterUsageTypedDict]]
    r"""Usage statistics for the completion request."""


class CreateChatCompletionData(BaseModel):
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    id: str
    r"""A unique identifier for the chat completion."""

    choices: List[CreateChatCompletionRouterChoices]
    r"""A list of chat completion choices. Can contain more than one elements if n is greater than 1. Can also be empty for the last chunk if you set stream_options: {\"include_usage\": true}."""

    created: float
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    model: str
    r"""The model used for the chat completion."""

    object: CreateChatCompletionRouterObject

    system_fingerprint: OptionalNullable[str] = UNSET
    r"""This fingerprint represents the backend configuration that the model runs with."""

    usage: OptionalNullable[CreateChatCompletionRouterUsage] = UNSET
    r"""Usage statistics for the completion request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["system_fingerprint", "usage"])
        nullable_fields = set(["system_fingerprint", "usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateChatCompletionRouterResponseBodyTypedDict(TypedDict):
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    data: NotRequired[CreateChatCompletionDataTypedDict]
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""


class CreateChatCompletionRouterResponseBody(BaseModel):
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    data: Optional[CreateChatCompletionData] = None
    r"""Represents a streamed chunk of a chat completion response returned by model, based on the provided input."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["data"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionFinishReason = Literal[
    "stop",
    "length",
    "tool_calls",
    "content_filter",
    "function_call",
]
r"""The reason the model stopped generating tokens."""


CreateChatCompletionRouterResponseType = Literal["function",]


class CreateChatCompletionRouterFunctionTypedDict(TypedDict):
    name: NotRequired[str]
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    arguments: NotRequired[str]
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""


class CreateChatCompletionRouterFunction(BaseModel):
    name: Optional[str] = None
    r"""The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""

    arguments: Optional[str] = None
    r"""The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name", "arguments"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateChatCompletionToolCallsTypedDict(TypedDict):
    index: NotRequired[float]
    id: NotRequired[str]
    type: NotRequired[CreateChatCompletionRouterResponseType]
    function: NotRequired[CreateChatCompletionRouterFunctionTypedDict]
    thought_signature: NotRequired[str]
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models when continuing a conversation after a tool call."""


class CreateChatCompletionToolCalls(BaseModel):
    index: Optional[float] = None

    id: Optional[str] = None

    type: Optional[CreateChatCompletionRouterResponseType] = None

    function: Optional[CreateChatCompletionRouterFunction] = None

    thought_signature: Optional[str] = None
    r"""Encrypted representation of the model internal reasoning state during function calling. Required by Gemini 3 models when continuing a conversation after a tool call."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["index", "id", "type", "function", "thought_signature"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateChatCompletionRole = Literal["assistant",]


class CreateChatCompletionRouterAudioTypedDict(TypedDict):
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""

    id: str
    expires_at: int
    data: str
    transcript: str


class CreateChatCompletionRouterAudio(BaseModel):
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""

    id: str

    expires_at: int

    data: str

    transcript: str


class CreateChatCompletionMessageTypedDict(TypedDict):
    r"""A chat completion message generated by the model."""

    content: NotRequired[Nullable[str]]
    refusal: NotRequired[Nullable[str]]
    tool_calls: NotRequired[List[CreateChatCompletionToolCallsTypedDict]]
    role: NotRequired[CreateChatCompletionRole]
    reasoning: NotRequired[Nullable[str]]
    r"""Internal thought process of the model"""
    reasoning_signature: NotRequired[Nullable[str]]
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""
    redacted_reasoning: NotRequired[str]
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""
    audio: NotRequired[Nullable[CreateChatCompletionRouterAudioTypedDict]]
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""


class CreateChatCompletionMessage(BaseModel):
    r"""A chat completion message generated by the model."""

    content: OptionalNullable[str] = UNSET

    refusal: OptionalNullable[str] = UNSET

    tool_calls: Optional[List[CreateChatCompletionToolCalls]] = None

    role: Optional[CreateChatCompletionRole] = None

    reasoning: OptionalNullable[str] = UNSET
    r"""Internal thought process of the model"""

    reasoning_signature: OptionalNullable[str] = UNSET
    r"""The signature holds a cryptographic token which verifies that the thinking block was generated by the model, and is verified when thinking is part of a multiturn conversation. This value should not be modified and should always be sent to the API when the reasoning is redacted. Currently only supported by `Anthropic`."""

    redacted_reasoning: Optional[str] = None
    r"""Occasionally the model's internal reasoning will be flagged by the safety systems of the provider. When this occurs, the provider will encrypt the reasoning. These redacted reasoning is decrypted when passed back to the API, allowing the model to continue its response without losing context."""

    audio: OptionalNullable[CreateChatCompletionRouterAudio] = UNSET
    r"""If the audio output modality is requested, this object contains data about the audio response from the model."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "content",
                "refusal",
                "tool_calls",
                "role",
                "reasoning",
                "reasoning_signature",
                "redacted_reasoning",
                "audio",
            ]
        )
        nullable_fields = set(
            ["content", "refusal", "reasoning", "reasoning_signature", "audio"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateChatCompletionTopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class CreateChatCompletionTopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateChatCompletionContentTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[CreateChatCompletionTopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class CreateChatCompletionContent(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[CreateChatCompletionTopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateChatCompletionRouterTopLogprobsTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""


class CreateChatCompletionRouterTopLogprobs(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateChatCompletionRefusalTypedDict(TypedDict):
    token: str
    r"""The token."""
    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""
    bytes_: Nullable[List[float]]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""
    top_logprobs: List[CreateChatCompletionRouterTopLogprobsTypedDict]
    r"""List of the most likely tokens and their log probability, at this token position."""


class CreateChatCompletionRefusal(BaseModel):
    token: str
    r"""The token."""

    logprob: float
    r"""The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."""

    bytes_: Annotated[Nullable[List[float]], pydantic.Field(alias="bytes")]
    r"""A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: List[CreateChatCompletionRouterTopLogprobs]
    r"""List of the most likely tokens and their log probability, at this token position."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateChatCompletionLogprobsTypedDict(TypedDict):
    r"""Log probability information for the choice."""

    content: Nullable[List[CreateChatCompletionContentTypedDict]]
    r"""A list of message content tokens with log probability information."""
    refusal: Nullable[List[CreateChatCompletionRefusalTypedDict]]
    r"""A list of message refusal tokens with log probability information."""


class CreateChatCompletionLogprobs(BaseModel):
    r"""Log probability information for the choice."""

    content: Nullable[List[CreateChatCompletionContent]]
    r"""A list of message content tokens with log probability information."""

    refusal: Nullable[List[CreateChatCompletionRefusal]]
    r"""A list of message refusal tokens with log probability information."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                m[k] = val

        return m


class CreateChatCompletionChoicesTypedDict(TypedDict):
    finish_reason: Nullable[CreateChatCompletionFinishReason]
    r"""The reason the model stopped generating tokens."""
    message: CreateChatCompletionMessageTypedDict
    r"""A chat completion message generated by the model."""
    index: NotRequired[float]
    r"""The index of the choice in the list of choices."""
    logprobs: NotRequired[Nullable[CreateChatCompletionLogprobsTypedDict]]
    r"""Log probability information for the choice."""


class CreateChatCompletionChoices(BaseModel):
    finish_reason: Nullable[CreateChatCompletionFinishReason]
    r"""The reason the model stopped generating tokens."""

    message: CreateChatCompletionMessage
    r"""A chat completion message generated by the model."""

    index: Optional[float] = 0
    r"""The index of the choice in the list of choices."""

    logprobs: OptionalNullable[CreateChatCompletionLogprobs] = UNSET
    r"""Log probability information for the choice."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["index", "logprobs"])
        nullable_fields = set(["finish_reason", "logprobs"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateChatCompletionPromptTokensDetailsTypedDict(TypedDict):
    cached_tokens: NotRequired[Nullable[int]]
    cache_creation_tokens: NotRequired[Nullable[int]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio input tokens consumed by the request."""


class CreateChatCompletionPromptTokensDetails(BaseModel):
    cached_tokens: OptionalNullable[int] = UNSET

    cache_creation_tokens: OptionalNullable[int] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio input tokens consumed by the request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["cached_tokens", "cache_creation_tokens", "audio_tokens"]
        )
        nullable_fields = set(
            ["cached_tokens", "cache_creation_tokens", "audio_tokens"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateChatCompletionCompletionTokensDetailsTypedDict(TypedDict):
    reasoning_tokens: NotRequired[Nullable[float]]
    accepted_prediction_tokens: NotRequired[Nullable[float]]
    rejected_prediction_tokens: NotRequired[Nullable[float]]
    audio_tokens: NotRequired[Nullable[int]]
    r"""The number of audio output tokens produced by the response."""


class CreateChatCompletionCompletionTokensDetails(BaseModel):
    reasoning_tokens: OptionalNullable[float] = UNSET

    accepted_prediction_tokens: OptionalNullable[float] = UNSET

    rejected_prediction_tokens: OptionalNullable[float] = UNSET

    audio_tokens: OptionalNullable[int] = UNSET
    r"""The number of audio output tokens produced by the response."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "reasoning_tokens",
                "accepted_prediction_tokens",
                "rejected_prediction_tokens",
                "audio_tokens",
            ]
        )
        nullable_fields = set(
            [
                "reasoning_tokens",
                "accepted_prediction_tokens",
                "rejected_prediction_tokens",
                "audio_tokens",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


class CreateChatCompletionUsageTypedDict(TypedDict):
    r"""Usage statistics for the completion request."""

    completion_tokens: NotRequired[float]
    r"""Number of tokens in the generated completion."""
    prompt_tokens: NotRequired[float]
    r"""Number of tokens in the prompt."""
    total_tokens: NotRequired[float]
    r"""Total number of tokens used in the request (prompt + completion)."""
    prompt_tokens_details: NotRequired[
        Nullable[CreateChatCompletionPromptTokensDetailsTypedDict]
    ]
    completion_tokens_details: NotRequired[
        Nullable[CreateChatCompletionCompletionTokensDetailsTypedDict]
    ]


class CreateChatCompletionUsage(BaseModel):
    r"""Usage statistics for the completion request."""

    completion_tokens: Optional[float] = None
    r"""Number of tokens in the generated completion."""

    prompt_tokens: Optional[float] = None
    r"""Number of tokens in the prompt."""

    total_tokens: Optional[float] = None
    r"""Total number of tokens used in the request (prompt + completion)."""

    prompt_tokens_details: OptionalNullable[CreateChatCompletionPromptTokensDetails] = (
        UNSET
    )

    completion_tokens_details: OptionalNullable[
        CreateChatCompletionCompletionTokensDetails
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "completion_tokens",
                "prompt_tokens",
                "total_tokens",
                "prompt_tokens_details",
                "completion_tokens_details",
            ]
        )
        nullable_fields = set(["prompt_tokens_details", "completion_tokens_details"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateChatCompletionObject = Literal["chat.completion",]


class CreateChatCompletionResponseBodyTypedDict(TypedDict):
    r"""Represents a chat completion response returned by model, based on the provided input."""

    id: str
    r"""A unique identifier for the chat completion."""
    choices: List[CreateChatCompletionChoicesTypedDict]
    r"""A list of chat completion choices. Can be more than one if n is greater than 1."""
    created: float
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""
    model: str
    r"""The model used for the chat completion."""
    object: CreateChatCompletionObject
    system_fingerprint: NotRequired[Nullable[str]]
    r"""This fingerprint represents the backend configuration that the model runs with."""
    usage: NotRequired[Nullable[CreateChatCompletionUsageTypedDict]]
    r"""Usage statistics for the completion request."""


class CreateChatCompletionResponseBody(BaseModel):
    r"""Represents a chat completion response returned by model, based on the provided input."""

    id: str
    r"""A unique identifier for the chat completion."""

    choices: List[CreateChatCompletionChoices]
    r"""A list of chat completion choices. Can be more than one if n is greater than 1."""

    created: float
    r"""The Unix timestamp (in seconds) of when the chat completion was created."""

    model: str
    r"""The model used for the chat completion."""

    object: CreateChatCompletionObject

    system_fingerprint: OptionalNullable[str] = UNSET
    r"""This fingerprint represents the backend configuration that the model runs with."""

    usage: OptionalNullable[CreateChatCompletionUsage] = UNSET
    r"""Usage statistics for the completion request."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["system_fingerprint", "usage"])
        nullable_fields = set(["system_fingerprint", "usage"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            is_nullable_and_explicitly_set = (
                k in nullable_fields
                and (self.__pydantic_fields_set__.intersection({n}))  # pylint: disable=no-member
            )

            if val != UNSET_SENTINEL:
                if (
                    val is not None
                    or k not in optional_fields
                    or is_nullable_and_explicitly_set
                ):
                    m[k] = val

        return m


CreateChatCompletionResponseTypedDict = TypeAliasType(
    "CreateChatCompletionResponseTypedDict",
    Union[
        CreateChatCompletionResponseBodyTypedDict,
        Union[
            eventstreaming.EventStream[CreateChatCompletionRouterResponseBodyTypedDict],
            eventstreaming.EventStreamAsync[
                CreateChatCompletionRouterResponseBodyTypedDict
            ],
        ],
    ],
)


CreateChatCompletionResponse = TypeAliasType(
    "CreateChatCompletionResponse",
    Union[
        CreateChatCompletionResponseBody,
        Union[
            eventstreaming.EventStream[CreateChatCompletionRouterResponseBody],
            eventstreaming.EventStreamAsync[CreateChatCompletionRouterResponseBody],
        ],
    ],
)
