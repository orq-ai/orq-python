"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from enum import Enum
from orq_ai_sdk import models, utils
from orq_ai_sdk._hooks import HookContext
from orq_ai_sdk.models import createchatcompletionop as models_createchatcompletionop
from orq_ai_sdk.types import OptionalNullable, UNSET
from orq_ai_sdk.utils import eventstreaming, get_security_from_env
from orq_ai_sdk.utils.unmarshal_json_response import unmarshal_json_response
from typing import Dict, List, Mapping, Optional, Union


class CreateAcceptEnum(str, Enum):
    APPLICATION_JSON = "application/json"
    TEXT_EVENT_STREAM = "text/event-stream"


class OrqCompletions(BaseSDK):
    def create(
        self,
        *,
        messages: Union[
            List[models_createchatcompletionop.CreateChatCompletionMessages],
            List[models_createchatcompletionop.CreateChatCompletionMessagesTypedDict],
        ],
        model: str,
        metadata: Optional[Dict[str, str]] = None,
        name: Optional[str] = None,
        audio: OptionalNullable[
            Union[
                models_createchatcompletionop.CreateChatCompletionAudio,
                models_createchatcompletionop.CreateChatCompletionAudioTypedDict,
            ]
        ] = UNSET,
        frequency_penalty: OptionalNullable[float] = UNSET,
        max_tokens: OptionalNullable[int] = UNSET,
        max_completion_tokens: OptionalNullable[int] = UNSET,
        logprobs: OptionalNullable[bool] = UNSET,
        top_logprobs: OptionalNullable[int] = UNSET,
        n: OptionalNullable[int] = UNSET,
        presence_penalty: OptionalNullable[float] = UNSET,
        response_format: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionResponseFormat,
                models_createchatcompletionop.CreateChatCompletionResponseFormatTypedDict,
            ]
        ] = None,
        reasoning_effort: Optional[
            models_createchatcompletionop.CreateChatCompletionReasoningEffort
        ] = None,
        verbosity: Optional[str] = None,
        seed: OptionalNullable[float] = UNSET,
        stop: OptionalNullable[
            Union[
                models_createchatcompletionop.CreateChatCompletionStop,
                models_createchatcompletionop.CreateChatCompletionStopTypedDict,
            ]
        ] = UNSET,
        stream_options: OptionalNullable[
            Union[
                models_createchatcompletionop.CreateChatCompletionStreamOptions,
                models_createchatcompletionop.CreateChatCompletionStreamOptionsTypedDict,
            ]
        ] = UNSET,
        thinking: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionThinking,
                models_createchatcompletionop.CreateChatCompletionThinkingTypedDict,
            ]
        ] = None,
        temperature: OptionalNullable[float] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        top_k: OptionalNullable[float] = UNSET,
        tools: Optional[
            Union[
                List[models_createchatcompletionop.CreateChatCompletionTools],
                List[models_createchatcompletionop.CreateChatCompletionToolsTypedDict],
            ]
        ] = None,
        tool_choice: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionToolChoice,
                models_createchatcompletionop.CreateChatCompletionToolChoiceTypedDict,
            ]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        modalities: OptionalNullable[
            List[models_createchatcompletionop.CreateChatCompletionModalities]
        ] = UNSET,
        guardrails: Optional[
            Union[
                List[models_createchatcompletionop.CreateChatCompletionGuardrails],
                List[
                    models_createchatcompletionop.CreateChatCompletionGuardrailsTypedDict
                ],
            ]
        ] = None,
        fallbacks: Optional[
            Union[
                List[models_createchatcompletionop.CreateChatCompletionFallbacks],
                List[
                    models_createchatcompletionop.CreateChatCompletionFallbacksTypedDict
                ],
            ]
        ] = None,
        retry: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionRetry,
                models_createchatcompletionop.CreateChatCompletionRetryTypedDict,
            ]
        ] = None,
        cache: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionCache,
                models_createchatcompletionop.CreateChatCompletionCacheTypedDict,
            ]
        ] = None,
        load_balancer: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionLoadBalancer,
                models_createchatcompletionop.CreateChatCompletionLoadBalancerTypedDict,
            ]
        ] = None,
        timeout: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionTimeout,
                models_createchatcompletionop.CreateChatCompletionTimeoutTypedDict,
            ]
        ] = None,
        orq: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionOrq,
                models_createchatcompletionop.CreateChatCompletionOrqTypedDict,
            ]
        ] = None,
        stream: Optional[bool] = False,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        accept_header_override: Optional[CreateAcceptEnum] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.CreateChatCompletionResponse:
        r"""Create chat completion

        Creates a model response for the given chat conversation with support for retries, fallbacks, prompts, and variables.

        :param messages: A list of messages comprising the conversation so far.
        :param model: Model ID used to generate the response, like `openai/gpt-4o` or `anthropic/claude-haiku-4-5-20251001`. The AI Gateway offers a wide range of models with different capabilities, performance characteristics, and price points. Refer to the (Supported models)[/docs/proxy/supported-models] to browse available models.
        :param metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can have a maximum length of 64 characters and values can have a maximum length of 512 characters.
        :param name: The name to display on the trace. If not specified, the default system name will be used.
        :param audio: Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more.
        :param frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        :param max_tokens: `[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

            This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
        :param max_completion_tokens: An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens
        :param logprobs: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
        :param top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
        :param n: How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
        :param presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        :param response_format: An object specifying the format that the model must output
        :param reasoning_effort: Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

            - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
            - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
            - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
            - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

            Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
        :param verbosity: Adjusts response verbosity. Lower levels yield shorter answers.
        :param seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
        :param stop: Up to 4 sequences where the API will stop generating further tokens.
        :param stream_options: Options for streaming response. Only set this when you set stream: true.
        :param thinking:
        :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
        :param top_k: Limits the model to consider only the top k most likely tokens at each step.
        :param tools: A list of tools the model may call.
        :param tool_choice: Controls which (if any) tool is called by the model.
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use.
        :param modalities: Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"].
        :param guardrails: A list of guardrails to apply to the request.
        :param fallbacks: Array of fallback models to use if primary model fails
        :param retry: Retry configuration for the request
        :param cache: Cache configuration for the request.
        :param load_balancer: Load balancer configuration for the request.
        :param timeout: Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured.
        :param orq: Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, identity-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution.
        :param stream:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.CreateChatCompletionRequestBody(
            messages=utils.get_pydantic_model(
                messages, List[models.CreateChatCompletionMessages]
            ),
            model=model,
            metadata=metadata,
            name=name,
            audio=utils.get_pydantic_model(
                audio, OptionalNullable[models.CreateChatCompletionAudio]
            ),
            frequency_penalty=frequency_penalty,
            max_tokens=max_tokens,
            max_completion_tokens=max_completion_tokens,
            logprobs=logprobs,
            top_logprobs=top_logprobs,
            n=n,
            presence_penalty=presence_penalty,
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.CreateChatCompletionResponseFormat]
            ),
            reasoning_effort=reasoning_effort,
            verbosity=verbosity,
            seed=seed,
            stop=stop,
            stream_options=utils.get_pydantic_model(
                stream_options,
                OptionalNullable[models.CreateChatCompletionStreamOptions],
            ),
            thinking=utils.get_pydantic_model(
                thinking, Optional[models.CreateChatCompletionThinking]
            ),
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            tools=utils.get_pydantic_model(
                tools, Optional[List[models.CreateChatCompletionTools]]
            ),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.CreateChatCompletionToolChoice]
            ),
            parallel_tool_calls=parallel_tool_calls,
            modalities=modalities,
            guardrails=utils.get_pydantic_model(
                guardrails, Optional[List[models.CreateChatCompletionGuardrails]]
            ),
            fallbacks=utils.get_pydantic_model(
                fallbacks, Optional[List[models.CreateChatCompletionFallbacks]]
            ),
            retry=utils.get_pydantic_model(
                retry, Optional[models.CreateChatCompletionRetry]
            ),
            cache=utils.get_pydantic_model(
                cache, Optional[models.CreateChatCompletionCache]
            ),
            load_balancer=utils.get_pydantic_model(
                load_balancer, Optional[models.CreateChatCompletionLoadBalancer]
            ),
            timeout=utils.get_pydantic_model(
                timeout, Optional[models.CreateChatCompletionTimeout]
            ),
            orq=utils.get_pydantic_model(orq, Optional[models.CreateChatCompletionOrq]),
            stream=stream,
        )

        req = self._build_request(
            method="POST",
            path="/v2/router/chat/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value=accept_header_override.value
            if accept_header_override is not None
            else "application/json;q=1, text/event-stream;q=0",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.CreateChatCompletionRequestBody
            ),
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="createChatCompletion",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            return unmarshal_json_response(
                models.CreateChatCompletionResponseBody, http_res, http_res_text
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.CreateChatCompletionRouterChatCompletionsResponseBody
                ),
                sentinel="[DONE]",
                client_ref=self,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        http_res_text = utils.stream_to_text(http_res)
        raise models.APIError("Unexpected response received", http_res, http_res_text)

    async def create_async(
        self,
        *,
        messages: Union[
            List[models_createchatcompletionop.CreateChatCompletionMessages],
            List[models_createchatcompletionop.CreateChatCompletionMessagesTypedDict],
        ],
        model: str,
        metadata: Optional[Dict[str, str]] = None,
        name: Optional[str] = None,
        audio: OptionalNullable[
            Union[
                models_createchatcompletionop.CreateChatCompletionAudio,
                models_createchatcompletionop.CreateChatCompletionAudioTypedDict,
            ]
        ] = UNSET,
        frequency_penalty: OptionalNullable[float] = UNSET,
        max_tokens: OptionalNullable[int] = UNSET,
        max_completion_tokens: OptionalNullable[int] = UNSET,
        logprobs: OptionalNullable[bool] = UNSET,
        top_logprobs: OptionalNullable[int] = UNSET,
        n: OptionalNullable[int] = UNSET,
        presence_penalty: OptionalNullable[float] = UNSET,
        response_format: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionResponseFormat,
                models_createchatcompletionop.CreateChatCompletionResponseFormatTypedDict,
            ]
        ] = None,
        reasoning_effort: Optional[
            models_createchatcompletionop.CreateChatCompletionReasoningEffort
        ] = None,
        verbosity: Optional[str] = None,
        seed: OptionalNullable[float] = UNSET,
        stop: OptionalNullable[
            Union[
                models_createchatcompletionop.CreateChatCompletionStop,
                models_createchatcompletionop.CreateChatCompletionStopTypedDict,
            ]
        ] = UNSET,
        stream_options: OptionalNullable[
            Union[
                models_createchatcompletionop.CreateChatCompletionStreamOptions,
                models_createchatcompletionop.CreateChatCompletionStreamOptionsTypedDict,
            ]
        ] = UNSET,
        thinking: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionThinking,
                models_createchatcompletionop.CreateChatCompletionThinkingTypedDict,
            ]
        ] = None,
        temperature: OptionalNullable[float] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        top_k: OptionalNullable[float] = UNSET,
        tools: Optional[
            Union[
                List[models_createchatcompletionop.CreateChatCompletionTools],
                List[models_createchatcompletionop.CreateChatCompletionToolsTypedDict],
            ]
        ] = None,
        tool_choice: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionToolChoice,
                models_createchatcompletionop.CreateChatCompletionToolChoiceTypedDict,
            ]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        modalities: OptionalNullable[
            List[models_createchatcompletionop.CreateChatCompletionModalities]
        ] = UNSET,
        guardrails: Optional[
            Union[
                List[models_createchatcompletionop.CreateChatCompletionGuardrails],
                List[
                    models_createchatcompletionop.CreateChatCompletionGuardrailsTypedDict
                ],
            ]
        ] = None,
        fallbacks: Optional[
            Union[
                List[models_createchatcompletionop.CreateChatCompletionFallbacks],
                List[
                    models_createchatcompletionop.CreateChatCompletionFallbacksTypedDict
                ],
            ]
        ] = None,
        retry: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionRetry,
                models_createchatcompletionop.CreateChatCompletionRetryTypedDict,
            ]
        ] = None,
        cache: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionCache,
                models_createchatcompletionop.CreateChatCompletionCacheTypedDict,
            ]
        ] = None,
        load_balancer: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionLoadBalancer,
                models_createchatcompletionop.CreateChatCompletionLoadBalancerTypedDict,
            ]
        ] = None,
        timeout: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionTimeout,
                models_createchatcompletionop.CreateChatCompletionTimeoutTypedDict,
            ]
        ] = None,
        orq: Optional[
            Union[
                models_createchatcompletionop.CreateChatCompletionOrq,
                models_createchatcompletionop.CreateChatCompletionOrqTypedDict,
            ]
        ] = None,
        stream: Optional[bool] = False,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        accept_header_override: Optional[CreateAcceptEnum] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.CreateChatCompletionResponse:
        r"""Create chat completion

        Creates a model response for the given chat conversation with support for retries, fallbacks, prompts, and variables.

        :param messages: A list of messages comprising the conversation so far.
        :param model: Model ID used to generate the response, like `openai/gpt-4o` or `anthropic/claude-haiku-4-5-20251001`. The AI Gateway offers a wide range of models with different capabilities, performance characteristics, and price points. Refer to the (Supported models)[/docs/proxy/supported-models] to browse available models.
        :param metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can have a maximum length of 64 characters and values can have a maximum length of 512 characters.
        :param name: The name to display on the trace. If not specified, the default system name will be used.
        :param audio: Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more.
        :param frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        :param max_tokens: `[Deprecated]`. The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.

            This value is now `deprecated` in favor of `max_completion_tokens`, and is not compatible with o1 series models.
        :param max_completion_tokens: An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens
        :param logprobs: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
        :param top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
        :param n: How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
        :param presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        :param response_format: An object specifying the format that the model must output
        :param reasoning_effort: Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

            - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool calls are supported for all reasoning values in gpt-5.1.
            - All models before `gpt-5.1` default to `medium` reasoning effort, and do not support `none`.
            - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.
            - `xhigh` is currently only supported for `gpt-5.1-codex-max`.

            Any of \"none\", \"minimal\", \"low\", \"medium\", \"high\", \"xhigh\".
        :param verbosity: Adjusts response verbosity. Lower levels yield shorter answers.
        :param seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
        :param stop: Up to 4 sequences where the API will stop generating further tokens.
        :param stream_options: Options for streaming response. Only set this when you set stream: true.
        :param thinking:
        :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
        :param top_k: Limits the model to consider only the top k most likely tokens at each step.
        :param tools: A list of tools the model may call.
        :param tool_choice: Controls which (if any) tool is called by the model.
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use.
        :param modalities: Output types that you would like the model to generate. Most models are capable of generating text, which is the default: [\"text\"]. The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use: [\"text\", \"audio\"].
        :param guardrails: A list of guardrails to apply to the request.
        :param fallbacks: Array of fallback models to use if primary model fails
        :param retry: Retry configuration for the request
        :param cache: Cache configuration for the request.
        :param load_balancer: Load balancer configuration for the request.
        :param timeout: Timeout configuration to apply to the request. If the request exceeds the timeout, it will be retried or fallback to the next model if configured.
        :param orq: Leverage Orq's intelligent routing capabilities to enhance your AI application with enterprise-grade reliability and observability. Orq provides automatic request management including retries on failures, model fallbacks for high availability, identity-level analytics tracking, conversation threading, and dynamic prompt templating with variable substitution.
        :param stream:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if timeout_ms is None:
            timeout_ms = 600000

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.CreateChatCompletionRequestBody(
            messages=utils.get_pydantic_model(
                messages, List[models.CreateChatCompletionMessages]
            ),
            model=model,
            metadata=metadata,
            name=name,
            audio=utils.get_pydantic_model(
                audio, OptionalNullable[models.CreateChatCompletionAudio]
            ),
            frequency_penalty=frequency_penalty,
            max_tokens=max_tokens,
            max_completion_tokens=max_completion_tokens,
            logprobs=logprobs,
            top_logprobs=top_logprobs,
            n=n,
            presence_penalty=presence_penalty,
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.CreateChatCompletionResponseFormat]
            ),
            reasoning_effort=reasoning_effort,
            verbosity=verbosity,
            seed=seed,
            stop=stop,
            stream_options=utils.get_pydantic_model(
                stream_options,
                OptionalNullable[models.CreateChatCompletionStreamOptions],
            ),
            thinking=utils.get_pydantic_model(
                thinking, Optional[models.CreateChatCompletionThinking]
            ),
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            tools=utils.get_pydantic_model(
                tools, Optional[List[models.CreateChatCompletionTools]]
            ),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.CreateChatCompletionToolChoice]
            ),
            parallel_tool_calls=parallel_tool_calls,
            modalities=modalities,
            guardrails=utils.get_pydantic_model(
                guardrails, Optional[List[models.CreateChatCompletionGuardrails]]
            ),
            fallbacks=utils.get_pydantic_model(
                fallbacks, Optional[List[models.CreateChatCompletionFallbacks]]
            ),
            retry=utils.get_pydantic_model(
                retry, Optional[models.CreateChatCompletionRetry]
            ),
            cache=utils.get_pydantic_model(
                cache, Optional[models.CreateChatCompletionCache]
            ),
            load_balancer=utils.get_pydantic_model(
                load_balancer, Optional[models.CreateChatCompletionLoadBalancer]
            ),
            timeout=utils.get_pydantic_model(
                timeout, Optional[models.CreateChatCompletionTimeout]
            ),
            orq=utils.get_pydantic_model(orq, Optional[models.CreateChatCompletionOrq]),
            stream=stream,
        )

        req = self._build_request_async(
            method="POST",
            path="/v2/router/chat/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value=accept_header_override.value
            if accept_header_override is not None
            else "application/json;q=1, text/event-stream;q=0",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.CreateChatCompletionRequestBody
            ),
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="createChatCompletion",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            return unmarshal_json_response(
                models.CreateChatCompletionResponseBody, http_res, http_res_text
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.CreateChatCompletionRouterChatCompletionsResponseBody
                ),
                sentinel="[DONE]",
                client_ref=self,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.APIError("API error occurred", http_res, http_res_text)

        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.APIError("Unexpected response received", http_res, http_res_text)
